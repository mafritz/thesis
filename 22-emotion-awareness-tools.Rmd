---
bibliography: references.bib
---

# Emotional Awareness Tools in Computer-Mediated Learning Environments {#eat-general-chapter}

```{r rationale-eat-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

After a general overview about the relationship between affect and learning, this chapter focuses on emotional awareness and, by extension, an Emotion Awareness Tool (EAT) more specifically. First, the concept of emotional awareness is defined within the context of computer-mediated learning environments. Second, three main assumptions are derived by this definition. The theoretical underpinnings of each assumption are set forth, complemented by the overview of related empirical works. In the second part of the chapter, an abstract model of an EAT is laid out in an attempt to frame the different interactions between the learners and an EAT, for emotional awareness to be instrumental. The model also allows to highlight how the instrumentality of an EAT is far from being trivial and rather requires a complex interplay between learners expertise and willingness in taking emotional awareness into account on the one hand, and features of the EAT that can sustain or facilitate the process.

## Definition of Emotional Awareness in Computer-Mediated Learning Environments

Emotional awareness in computer-mediated learning environments is often proposed in the literature as self-explaining or defined somehow implicitly. For instance, as stated in the previous chapter, Feidakis and colleagues [-@feidakisReviewEmotionAwareSystems2016, p. 217] define emotion awareness as "the *implicit* or *explicit collection of emotion data* and the *recognition* of *emotion patterns*" (italics in the text). Cernea and colleagues [-@cerneaSurveyTechnologiesRise2015], in their overview of rising technologies in emotion-enhanced interaction, identify the category of affective-aware systems as "designed to improve affective self-awareness of a user (i.e.,internal affective awareness) or awareness of the emotions of other users (i.e.,external affective awareness)" (*ibid.*, p.78). More precise definitions are provided by Lavoué and colleagues [-@lavoueEmotionAwarenessTools2020], who partially derived them from clinical psychology. The first definition provided by Lavoué and colleagues is based upon @bodenFacetsEmotionalAwareness2015 and posits that emotional awarenss is "the ability to perceive, identify, and understand emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. Later in their article, Lavoué and colleagues (*ibid.*) propose another definition, based upon Rieffe and colleagues [-@rieffeEmotionAwarenessInternalising2008], referring to emotional awareness as "the attentional process by which individuals identify, explain and differentiate between their own emotions as well as the others' emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. These two definitions resonate with socio-emotional competences briefly illustrated in the previous chapter. Finally, later in the same article, Lavoué and colleagues (*ibid.*) provide a more technical definition of emotion awareness tools as "tools that display information on own' own [sic] or partners' emotions, circumstances and antecedents" [@lavoueEmotionAwarenessTools2020, p. 284]. Compared to the more abstract definition in Feidakis and colleagues or in Cernea and colleagues (*ibid.*), the definition in Lavoué and colleagues (*ibid.*) implies that the tool shall also provide information about circumstances and antecedents that concurred in eliciting the emotional episode.

From these definitions it is possible to extrapolate three fundamental assumptions about emotional awareness and, by extension, the instrumentality of an EAT in a computer-mediated learning environment:

1.  Learners may benefit from intra-personal emotional awareness by using information about their own emotions as valuable information for their own learning processes (self-regulation).
2.  Learners may benefit from inter-personal emotional awareness by using information about their partners' emotions and/or by communicating their own emotions to their partners and/or by a combination of the two. One or all of these circumstances may contribute to learners' own learning processes (self-regulation), the learning processes of their partners individually (co-regulation), or the learning processes of the group as a whole (socially shared regulation).
3.  Learners may benefit from emotional awareness conveyed by a dedicated tool, which *encodes* emotional information into the computer-mediated learning environment, and *decodes* that information, for learners to extrapolate emotional meaning-making instrumental to learning.

Each assumption is discussed in the following three sections. Every section proposes an overview of related theories, as well a selection of empirical works, which are of particular interest for the present contribution in terms of objectives, methodologies, or findings.

## Emotional Awareness at the Intra-Personal Level {#ea-intra-personal}

The first assumption about the instrumentality of emotional awareness implies that learners can benefit from it at the individual, intra-personal level.

### Theoretical Underpinnings

The role of emotion at the intra-personal level has received over the last few decades extensive consideration from different perspectives and using different methods of investigation. A widespread consensus has emerged over the years about the fact that emotion, rather then disruptive of behavior and in antithesis with cognition, plays a prominent role in helping the organism to cope with a complex environment [@adolphsNeuroscienceEmotionNew2018; @armonyCambridgeHandbookHuman2013; @damasioDescartesError2006; @damasioStrangeOrderThings2018; @schererEmotionTheoriesConcepts2009; @immordino-yangEmotionsLearningBrain2016; @levensonAutonomicNervousSystem2014; @levensonIntrapersonalFunctionsEmotion1999; @leventhalRelationshipEmotionCognition1987; @pessoaCognitiveemotionalBrainInteractions2013; @sanderModelsEmotionAffective2013].

For instance, emotion is known to influence high-level cognitive functions such as attention, perception, memory, and decision-making [@broschImpactEmotionPerception2013], which are all implicated in learning processes. For instance, it is posited that an emotional attention system may complement and interact with the exogenous and endogenous systems [@broschAdditiveEffectsEmotional2011]. Emotionally charged stimuli are also known to often bestow precedence in perception over non emotionally-charged stimuli [@broschPerceptionCategorisationEmotional2010]. Many links between emotion and memory have been established both in the encoding and retrieving of information [@sharotHowArousalModulates2004; @kensingerRetrievalEmotionalEvents2020], which may have important consequences for learning [@beegeMoodaffectCongruencyExploring2018; @tyngInfluencesEmotionLearning2017]. Finally, it has been determined that when emotion is not taken into account in decision-making, consequences can be highly disruptive for the person [@becharaRoleEmotionDecisionmaking2004; @rollsEmotionDecisionmakingExplained2014].

Emotional awareness, though, goes a step forward in considering the importance of emotion at the intra-personal level, because it presupposes that learners can benefit from being aware of their own emotions [@lavoueEmotionAwarenessTools2020]. Whether consciousness is a necessary condition for emotion to *exist* [@ledouxHigherorderTheoryEmotional2017; @liebermanBooConsciousnessProblem2019] is a debate outside the scope of the present contribution (see also next chapter): the use of voluntary self-report requires consciousness for emotional awareness to emerge in the first place and therefore *dodges* the issue. Conscious processing derived from emotional awareness has been related more specifically to three overlapping factors in learning processes [@lavoueEmotionAwarenessTools2020]: (1) emotion as useful information to direct or redirect cognitive resources; (2) emotion as useful information to assess and guide learners' interest and motivation; and (3) emotion as useful information to regulate one's own behavior, including one's own emotions.

Learners may use their achievement, epistemic and topic emotions as useful cues to evaluate and direct cognitive processes [@pekrunControlValueTheoryAchievement2014]. The work of D'Mello and colleagues [-@dmelloConfusionCanBe2014], for instance, highlights how confusion is a signal of cognitive disequilibrium, which learners are therefore incited to re-balance. The work of Vogl and colleagues [-@voglSurpriseCuriosityConfusion2019] about the role of epistemic emotions suggests that experience of curiosity, pride and shame may inform learners about different reasons for knowledge exploration.

Awareness of one's emotions may prompt learners to assess their motivation to keep engaged or disengage from a learning activity [@linnenbrink-garciaAdaptiveMotivationEmotion2016]. For instance, Baker and colleagues' work [-@bakerBetterBeFrustrated2010] draws attention on the importance of recognizing boredom and frustration as warnings of inefficient and potentially misleading efforts.

Finally, emotion may be used as useful information to regulate one's own behavior, including emotion regulation itself [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]. When performed explicitly rather than implicitly [@torrePuttingFeelingsWords2018], emotion regulation requires the person to be aware of the emotion to be regulated [@lavoueEmotionAwarenessTools2020]. According to Gross, emotion regulation "refers to the processes by which we influence which emotions we have, when we have them, and how we experience and express them" [@grossEmotionRegulationAffective2002, p. 282]. The process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015] posits that regulation can happen at five successive stages:

1.  *situation selection*, which consists in avoiding or experiencing events according to their probability of eliciting unwanted or sought after emotions;
2.  *situation modification*, consisting in attuning the situation once it has been selected or it is forced upon the person;
3.  *attentional deployment*, by which some characteristics of the situation are considered more relevant than others;
4.  *cognitive change*, through which the focal points of the situation can be reappraised in an attempt to shift, endure or increase an emotional experience;
5.  *response modulation*, which occurs once an emotion has already been elicited and the person attempts, for instance, to suppress its manifestations (*e.g.*, trying not to shake during and oral exam or suppressing a laughter during a lesson).

The fourth and fifth passages have been often considered as two alternative strategies for emotion regulation, with potentially different impact on person's cognition and behavior [@bonannoImportanceBeingFlexible2004; @grossEmotionRegulationAffective2002; @richardsEmotionRegulationMemory2000]. In some circumstances, cognitive change can be more beneficial, since it reassesses the situation in what shall be more favorable terms for the person, freeing resources that would otherwise be monopolized by the undesired emotion [@richardsEmotionRegulationMemory2000]. In other circumstances, a cognitive re-evaluation of the situation may be too demanding, and the person may have more benefits in trying to suppress the response modulation [@bonannoImportanceBeingFlexible2004].

Torre and Lieberman [-@torrePuttingFeelingsWords2018], on the other hand, hypothesize that emotion regulation may occur even implicitly through *affect labeling* [@liebermanAffectLabelingAge2019; @liebermanPuttingFeelingsWords2007; @liebermanSubjectiveResponsesEmotional2011], that is, when people assign a word to the their emotional experience (*e.g.* "I feel angry"). In their article, the authors provide an overview of research about affect labeling and state that it "has demonstrated a modulation of emotional output effects in the same experiential, autonomic, neural, and behavioral domains as found in other forms of emotion regulation" [@torrePuttingFeelingsWords2018, p. 117]. The authors also highlight, though, that the mechanisms by which affect labeling intervene as implicit emotion regulation are still unclear. In this regard, Torre and Lieberman (*ibid*.) propose four possible mechanisms:

1.  *distraction*, for resorting to language shift the attention from the situation itself and therefore attenuates full processing of the eliciting event;
2.  *self-reflection*, as a means to initiate an introspection process fostering self-distancing from the emotion;
3.  *reduction of uncertainty*, which results from categorizing an intense and often nuanced experience using known and community-shared words;
4.  *symbolic conversion*, consisting in events assuming symbolic status through the associated word, which may induce more abstract thinking about the eliciting event.

Emotional awareness may therefore be useful either as explicit [@grossEmotionRegulationCurrent2015] or implicit [@torrePuttingFeelingsWords2018] emotion regulation. It may also help learners in evaluating whether a regulatory strategy may be more or less appropriate given the situation at hand [@lavoueEmotionAwarenessTools2020].

To sum up, there is a consistent body of research that highlights how emotion play a prominent role at the intra-individual level in various processes, which are also implicated in learning. Learners may therefore benefit from self-emotional awareness as valuable information upon which deciding how to steer their learning processes.

### Related Works

Molinari and colleagues [-@molinariEMORELOutilReporting2016] developed a self-reporting system from an experience sampling method [@csikszentmihalyiValidityReliabilityExperienceSampling2014] perspective, the EMORE-L (EMOtion REport for E-Learning), which was used in an ecological setting by 16 university students who voluntarily adopted the system during 15 days of distance learning in a blended bachelor program. The system consists in a short online questionnaire that students were reminded to fill once per day through an email that they received at a time previously concerted with the investigators. The EMORE-L consists in nine short questions, which are meant to reduce the amount of time needed to fill-in the questionnaire, organized in 4 parts: a first part about the situation the students were facing, with information about the activity students were conducting; a second part about the cognitive evaluation of the situation on three dimensions (Control, Value, Activation); a third part about the emotional experience, with the possibility of choosing between eight discrete emotions (*pleasure*, *anxiety*, *curiosity*, *boredom*, *engagement*, *confusion*, *surprise*, and *frustration*), for which students could also provide the intensity; and a last part about the social sharing of emotion, with items related to the wish for students to share their emotions with their colleagues, as well as the mutual knowledge of emotions between the student and their colleagues.

Through the 169 questionnaire that were filled throughout the 15 days, Molinari and colleagues (*ibid.*) were able to assess three main exploratory subjects. For each element, the authors used results of the study to discuss consequences on the implementation of emotion self-reporting tool in a context of emotional awareness.

The first subject under scrutiny concerned what emotions were most frequently associated with the different situations a student may encounter during distance learning. In this regard, results highlight four main activities, ordered from the most frequent: reading resources, synthesis of the same resources, individual work, and group work. The three most likely emotion to be experience are *pleasure*, *anxiety*, and *surprise*, whereas students made a scant use of the other five emotions proposed by the list. The authors also evinced from their data that the frequency of emotion changed as a function of the activity. On this ground, Molinari and colleagues suggest that self-report tool should take into account the specific activity that they aim to sustain, avoiding thus generic list of emotions.

The second phenomenon explored in the study investigated to what extent students wished to share their emotions with their colleagues, and in what situation. According to their results, Molinari and colleagues posit that students prefer to share their emotions during individual and group works. Furthermore, students were also more inclined to share their emotions when they experienced *anxiety* or *pleasure*. Based on these results, the authors suggest that self-reporting tools shall not be provided at all time, but only when the activity is likely to elicit emotions learners wish to share.

Finally, Molinari and colleagues assessed the contribution of the dimensional approach of the cognitive evaluation items and of the eight discrete emotions in allowing students to express what they felt. The dimensional vs. discrete emotion approach to emotion self-reporting is a longstanding debate [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005], with both approaches presenting advantages and shortcomings in terms of user experience and quality of data (see below in the chapter). Results observed by Molinari and colleagues, augmented by feedbacks from students regarding the use of the system, confirm this trend. For instance, the authors highlight the scant use of the different discrete emotions, with only three out of eight being adopted frequently. At the same time, the cognitive evaluation provided through the *control*, *value* and *activation* dimensions also presented shortcomings: *pleasure*, *anxiety* and *surprise*, in spite of their diversity, were in fact associated to similar appraisal profiles. On this ground, the authors point out how important it is that the self-reporting system helps students in identifying their emotional experience.

Molinari and colleagues (*ibid*) exploratory work provide useful elements with respect to emotional awareness in a voluntary self-reporting setting. On the technical standpoint, the EMORE-L combines antecedents of emotions, in the form of cognitive evaluation on dimensions, with emotional experience expressed as discrete natural language words, even though the two blocks of items are not intertwined in the user experience. Also, even if relatively short, the use of 9 questions to express one's emotions is more suitable for a *scripting* rather than a moment-to-moment emotional awareness.

At a conceptual level, Molinari and colleagues introduced emotional awareness in individual settings, as a means to provide students with self-awareness of their own emotions. At the same time, the authors also measured students' wish to dispose of collective emotional awareness. To this extent, Molinari and colleagues highlight how students wished to have full control over when and which emotional information to disclose. In addition, they also manifested the wish of being aware of their colleagues emotions, that is, bringing emotional awareness at the inter-personal level, which is the subject of the next section.

## Emotional Awareness at the Inter-Personal Level {#ea-inter-personal}

The second assumption about the instrumentality of emotional awareness posits that learners can benefit from emotional information available at a collective level, either for their own learning processes, that of their partners, or both at the same time.

### Theoretical Underpinnings

Whereas the intra-personal function of emotion has received extensive attention in the last few decades, several researchers have more recently advocated the need for investigating the inter-personal function of emotion [@parkinsonEmotionSocialRelations2005; @rimePartageSocialEmotions2005; @vankleefInterpersonalDynamicsEmotion2018; @fischerWhereHaveAll2010]. Fisher and Van Kleef [-@fischerWhereHaveAll2010], for instance, posit that

> It is an indisputable fact that emotions are mostly reactions to other people, that emotions take place in settings where other people are present, that emotions are expressed towards other people and regulated because of other people. It is hardly possible to imagine the elicitation of anger, shame, sadness, happiness, envy, guilt, contempt, love, or hatred without imagining other people as cause, target, or third-party observer of these emotions. In other words, the social constitution of emotions is beyond doubt.\
> -- @fischerWhereHaveAll2010, p. 208.

Keltner and Haidt [-@keltnerSocialFunctionsEmotions1999] identify four level of analysis in which emotion play a social function: (1) the individual level whenever the source of the emotion is of a social nature; (2) the dyadic level, such as in direct dialogue or collaboration; (3) the group-level, in which people share common identities and goals to attain; and (4) the cultural level, where the analysis focus on macro-elements such as history and tradition. An overarching synthesis on the social functions of emotions across all levels is proposed by Fischer and Manstead [-@fischerSocialFunctionsEmotion2016], who identify two complementary, but distinct, functions performed by emotions: affiliation and distancing. The affiliation function serves to form and maintain positive social relationship with others, whereas the distancing function helps in establishing and maintaining a social position relative to others (*ibid.*).

Other social functions of emotion comprise: the social sharing of emotion [@rimeEmotionElicitsSocial2009; @rimePartageSocialEmotions2005] as a means to initiate and reinforce social-bonding; harnessing emotional information to better causes and consequences of behaviors in others [@parkinsonEmotionsInterpersonalInteractions2010; @vankleefHowEmotionsRegulate2009]; the use of emotional information as a reference for normative conduct or for inferring personality traits [@hareliEmotionsSignalsNormative2013; @hareliWhatEmotionalReactions2010]; and the manifestation of emotion as a form of involuntary contagion [@barsadeRippleEffectsEmotional2002; @barsadeGroupAffect2015] or deliberate influence on others [@vankleefEmotionInfluence2011; @vankleefEmotionalInfluenceGroups2017]. In this regard, Van Kleef [-@vankleefEmergingViewEmotion2010; -@vankleefHowEmotionsRegulate2009; -@vankleefInterpersonalDynamicsEmotion2018] proposes an overarching framework, the Emotion As Social Information (EASI) model, which attempts at integrating the many inter-personal functions of emotion in terms of two classes of mutually influential, but conceptually distinct and empirically separable mechanisms:

1.  *Emotional expressions trigger affective reactions in observers*. The first mechanism implies that the emotion of a person may be the eliciting stimulus of an emotion in the observer. The affective reaction in the observer may be the same as the one expressed by the person, as in the case in emotional contagion [@barsadeRippleEffectsEmotional2002] or empathy [@bloomEmpathyItsDiscontents2016; @wondraAppraisalTheoryEmpathy2015]. For instance, boredom may propagate between learners as the result of the first person pausing and puffing. At the same time, the emotion in one person can trigger a different affective reaction in the observer, but independently of inferential processes about the situation. For instance, the amusement of a colleague during a collaborative task may trigger anger in the observer who wants to stay focused, independently of the reasons why the colleague is amused. Whether the observer affective reaction mirrors or complements that of the person who has expressed it, the reaction will have consequences on the observer's behavior (*e.g.*, deciding to ignore the amused colleague) and/or both the observer and the person who expressed the *original* emotion (*e.g.*, both learners decide to take a break).
2.  *Emotional expressions elicit inferential processes in observers*. The second mechanism implies a more complex and deliberate act of information-processing by which the observer attributes to the expressed emotion potential causes of and consequences on behavior. Especially in appraisal theories of emotion [@moorsAppraisalTheoriesEmotion2013; @moorsFlavorsAppraisalTheories2014; @rosemanAppraisalTheoryOverview2001], which will be more thoroughly depicted in the next chapter, the evaluation a person makes of the situation is pivotal in determining what emotion she may feel. By a form of *reverse engineering* [@hareliWhatEmotionalReactions2010; @schererFacialExpressionsAllow2007], the specific emotion a person is feeling may be used to infer how that emotion came to be elicited.

The EASI model (*ibid.*) also specify two preconditions and two moderator factors that determine the inter-personal dynamics of emotion. The preconditions concern (1) the ability of the person experiencing the emotion to *encode* it in a form that can be communicated to the observer, which broadly refers to the concept of *emotion expressivity* [@kringIndividualDifferencesDispositional1994; @scarantinoHowThingsEmotional2017]; and (2) the observer's ability to *decode* the emotional information and make sense from it using emotional knowledge and understanding, which closely relates to the principle of socio-emotional competences depicted in Section \@ref(socio-affective-competences) [@brackettRULERTheoryDrivenSystemic2019; @chernissEmotionalIntelligenceClarification2010; @matthewsScienceEmotionalIntelligence2007]. The two preconditions reflect the displaying and the monitoring functions of awareness tools depicted in Section \@ref(displaying-monitoring-functions) [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. As stated by Van Kleef [-@vankleefInterpersonalDynamicsEmotion2018]:

> No matter how informative emotions may be, and however critical their social-regulatory functions, if emotions are not expressed and/or fail to be perceived by others, their signaling function is evidently lost and social interaction may be jeopardized.\
> -- @vankleefInterpersonalDynamicsEmotion2018, p.52

The EASI model also proposes two classes of moderator factors that may influence to what extent an expressed emotion may result in an affective reaction and/or an inferential process by the observer (*ibid*.). For instance, it may be the case that the *confusion* expressed by a colleague may trigger an affective reaction of *anger* in the observer, because this slows down the collaboration. On the other hand, the same expression may also push the observer to understand the causes of the colleague's *confusion*, for then proposing some form of co-regulation. According to the EASI model, the engagement of the observer in deliberate and more cognitively demanding inferential processes may be facilitated by (1) the observer's motivation and capacity to allocate resources to make inferences about causes and consequences on the sender's behavior; and (2) the perceived *appropriateness* of the expressed emotions according to the observer's criteria of evaluation, for instance in relation to social norms [@fischerSocialFunctionsEmotion2016; @hareliWhatEmotionalReactions2010]. In other words, the observer may try to understand the colleague's confusion if (a) the observer has some *free* resources to dedicate, which may be difficult if the learning context is demanding, and (b) the observer evaluates confusion as an appropriate reaction to the situation, for instance by reckoning that the learning task is difficult.

The EASI theory (*ibid.*) provides an integrative framework of the social-function of emotion, which is founded on the assumption that one's own emotions may guide thoughts, actions and feelings in another person. Using emotion as social information may, for instance, be a first step in the social regulation of emotion, that is, the attempt to voluntary act upon other people's emotion [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013]. According to Reecks and colleagues [-@reeckSocialRegulationEmotion2016, p.48] , "[t]he goal-driven nature of social regulation distinguishes it from related phenomena, such as social sharing, empathy, or emotional contagion, where one person's actions are not strategically directed towards influencing another's emotions". The authors propose a cross-disciplinary model that implements the process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], illustrated in the intra-personal section, from an inter-personal perspective. In what they call the Social Regulatory Cycle, depicted in Figure @ref(fig:tf-social-regulation-model), the authors identify the regulator and the target person upon which the emotion regulation is intended. The regulator must proceed, first, by identifying the target emotion. Second, the regulator must assess whether the identified emotion corresponds to a suitable emotion or not. Third, in case of a discrepancy between the two, the regulator must plan a strategy aiming at producing in the target the suitable emotion. Last, this strategy must be implemented. At this point, the strategy may target a different stage of the process in the target (see the description of the individual model above for more details): the selection or modification of the situation; the orientation of the target's attention; the possibility of changing the way the target appraises the situation; or the modulation of the behavioral, physiological and experiential manifestation of the emotion. The cycle may start again either from an individual standpoint (*e.g.*, the target is dissatisfied with the new emotion induced by the regulator) or inter-individual perspective (*e.g.*, the regulator did not obtain the suitable emotion or identify an even more adapted emotion for the target).

(ref:tf-social-regulation-model-caption) The Social Regulatory Cycle at the inter-personal level, fromm the original Figure 2 in @reeckSocialRegulationEmotion2016, p. 51.

```{r tf-social-regulation-model, fig.align="center", out.width="80%", fig.cap="(ref:tf-social-regulation-model-caption)"}
knitr::include_graphics(here("./figure/theory/social-regulation-emotion.png"))
```

To sum up, there is extensive work in emotion theory advancing reasons why emotions play a pivotal role at the inter-personal level. Learners may therefore benefit from socially conveyed emotional awareness both at the individual and collective level, assuming *appropriate* communicating and inferential processes are at play.

### Related Works

Eligio, Ainsworth and Crook [-@eligioEmotionUnderstandingPerformance2012] carried out two intertwined experiments aiming at exploring "what collaborators understand about each other's emotions and the implications of sharing information about them" (*ibid*, p. 2046). In the first experiment, the authors asked pairs of same-sex, unacquainted participants to play a collaborative game in a co-located environment, where they shared the same computer equipment. At the end of the collaboration, participants were asked to fill in two questionnaires where they had to rate the intensity of 15 emotions: *happy*, *angry*, *sad*, *fearful*, *angry*, *bored*, *challenged*, *interested*, *hopeful*, *frustrated*, *contempt*, *disgusted*, *surprised*, *proud*, *ashamed* and *guilty*. In the *own* version of the questionnaire, they rated the intensity of each emotion as they have perceived it during the task. In the *partner* version of the questionnaire, participants had to project the intensity by which their partner in the dyad had experienced each of the listed emotion. The administration of the questionnaire was made individually, so that participants could not discuss the matter with their partner, and up until that moment, participants were not informed of the rating, so that they had no particular reason to pay attention neither to their own, nor to their partners' emotions. By comparing the responses to the two questionnaires, the authors concluded that, despite collaborating side-by-side, participants had little understanding of their partner's emotions. On the contrary, consistently with previous findings in the literature [@kruegerTrulyFalseConsensus1994; @tomaAnticipatedCooperationVs2010], participants tended to project their own emotional experience onto their partners. Eligio and colleagues (*ibid*) provides two possible explanations for these results. On the one hand, participants could simply not care about the emotions of their partner. On the other hand, participants could genuinely care about them, but lack the means to focus on them, for instance because the task was too demanding, and they decided to prioritize other mental states perceived as *more* instrumental to the task at hand. In both cases, the attribution of their own emotional experiences to that of the partner is caused by a lack of information available, but the reasons for this shortcoming are not the same depending on the intentions.

In the second experiment, Eligio and colleagues (*ibid*) therefore decided to intervene by providing (or not) participants with explicit awareness of their partner's emotions using a *scripting* strategy, that is, by interrupting the collaboration at specific moments for participants to express their emotions and projecting that of their partner. After filling the *own* and *partner* questionnaire, if participants disposed of awareness, they could look at the emotions of their partner's before resuming the task, otherwise they just continued with the collaboration. Furthermore, dyads were also assigned either in a co-located or a remote collaborative setting, resulting in a 2x2 factorial design with awareness vs. no-awareness, and co-located vs. remote conditions. Using experimental settings similar to the first experiment (but with a slightly different collaborative task and with only women as participants), the authors report evidence suggesting that participants benefited of emotional awareness in terms of performance both in the co-located and remote conditions. Furthermore, participants in the remote condition were more accurate in understanding emotions of their partner and also experienced more *positive affect*, computed by averaging the intensity of *happy*, *interested*, *hopeful*, *excited* and *challenged*.

From the two experiments, Eligio and colleagues (*ibid.*) concluded that participants do not have an accurate understanding of the emotion of their partner if their attention is not explicitly driven to it. Providing emotional awareness seems thus a promising way to increase mutual understanding, since participants with emotional awareness showed higher accuracy in estimating their partner's emotions, as well as better performance to the task at hand.

Following Eligio and colleagues (*ibid.*) findings, Molinari and colleagues [@molinariEmotionFeedbackComputermediated2013] conducted a study in which 30 dyads of same-sex participants (16 dyads of women, 14 dyads of men) performed a collaborative task in remote conditions, with the possibility of audio but not video connection. The aim of the collaborative task was to conceive a slogan against violence in schools using an argument graphic tool [@lundHowArgumentationDiagrams2007]. The task therefore implied some form of negotiation for deciding the final outcome of the collaboration, which was intended to solicit socio-cognitive tensions [@andriessenSociocognitiveTensionCollaborative2011] in participants as a means to elicit emotions. Half of the dyads were randomly assigned to a control condition, in which participants did not dispose of emotional awareness. The other half of the dyads were provided with emotional awareness through the use of an EAT persistently available on screen alongside the collaborative tool to build the slogan. (The tool will be described in more detail below, see Figure \@ref(fig:tf-eatmint-eat).) Contrary to Eligio and colleagues (*ibid*.), in which participants shared emotions at specific moments during the task following a *scripting* strategy, in this case the setting was congruent with an *awareness* strategy, since participants could express and have access to their partner's emotions at any time during the task. A message also showed up after 5 minutes from the last expressed emotion to remind participants to express how they were feeling.

At the end of the collaborative task, participants individually filled-in a questionnaire aimed at gathering information about (1) the kind and intensity of the emotions of the participant at the end of the task, as well as the emotions the participant attributed to the partner, particularly with respect to the 20 discrete emotions available as buttons on the EAT; and (2) the quality of the perceived interaction based on the frequency by which the participant and the partner provided/imposed their own points of view, defended and argued their ideas, understood their partner's points of view, built up on their partner's ideas, as well as managed emotion during interaction. As for Eligio and colleagues (*ibid.*), also Molinari and colleagues (*ibid*.) report that their result support the hypothesis of beneficial effect from the presence of an EAT, but only in dyads of women and with the presence of mixed results with respect to their initial hypothesis.

The tests upon which Eligio and colleagues (*ibid*.) and Molinari and colleagues' (*ibid*.) results are based, though, do consider the hierarchical structure of the dyads, which may inflate the rate of type I error since the non-independence of observations is not taken into account [@brownIntroductionLinearMixedEffects2021; @singmannIntroductionMixedModels2020; @westLinearMixedModels2015]. As a consequence, the evidence provided by the two studies should be taken with caution. The experimental settings, on the other hand, are interesting and directly compare a *scripting* and an *awareness* perspective, with the latter presenting the advantage of persistent, moment-to-moment emotional awareness. The two studies also highlight the many methodological challenges in determining the effect of an EAT in computer-mediated learning environments, which concern how emotional awareness is provided, but also how the potential benefit are measured and analyzed.

A different approach was taken by Avry and colleagues [-@avryAchievementAppraisalsEmotions2020], who, rather than providing awareness through discrete emotions, took a dimensional approach adopting Pekrun's Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006] introduced in Section \@ref(learning-on-affect). In their study, the authors asked 28 dyads of same-sex participants to play a remote collaborative game. Beside the game window, participants disposed of smaller window on the screen which reported two feedbacks: (1) a feedback on how well the dyds was mastering the game, representing the Control dimension; and (2) how well they were performing in a standing compared to other dyads, representing the Value dimension. The two feedbacks were manipulated by the authors as to create a 2x2 factorial design combining high vs. low Control on the one hand, and high vs. low Value on the other. After the collaborative task, participants filled in a questionnaire, on which they evaluated the overall collaboration in terms of (a) affective dimensions (Valence, Dominance, and Activation) and 16 discrete achievement emotions, and (b) six computer-supported collaborative exchanges (sustaining mutual understanding, information pooling, transactivity, reaching consensus, task management, and time management). For both category of measures, the rating was performed for the participant herself, as well as what the participant thought her partner would have experienced. The results obtained by Avry and colleagues (*ibid.*), who took into account the hierarchical structure of dyds by checking the intraclass correlation of dyads, corroborate an effect of the manipulation of the Control-Value appraisals on both category of measures (affective and socio-cognitive). These results are particularly interesting considering that in this study a form of emotional awareness was (1) manipulated, and (2) conveyed through a feedback aimed at eliciting a certain kind of appraisal of the situation, which influenced the kind of discrete achievement emotions experienced.

The three studies outlined in this section provide very different approaches to the study of emotional awareness in computer-mediated learning environments. Since this is a recent and cross-disciplinary field of inquiry, a fragmented stage of research is inevitable [@fiedlerCycleTheoryFormation2004]. On the other hand, efforts should also be dedicated to the possibility of direct comparison and incremental building of knowledge. In this regard, the adoption of a prototypical EAT could be beneficial, for similarities and differences in conveying emotional awareness can be more easily defined through objective parameters,

## Emotion in Computer-Mediated Learning Environments {#ea-in-computer-mediated-environment}

The third assumption underlying emotion awareness tools concerns the possibility to create emotional awareness in computer-mediated learning environments.

### Theoretical Underpinnings

For emotional awareness to emerge and be available, it is necessary that emotion may be fruitfully *encoded* and *decoded* in a computer-mediated learning environment. In this regard, the computer-mediated environment is somehow ambivalent with respect to affect-related phenomena. On the one hand, the combined presence of a learning task and a technological device captures learner's attention and thus, even in the case of co-located interaction or seamless audio/video connection in remote settings, diminishes the possibility to pay attention to efferent cues of affective experiences -- such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009] -- that are more readily available in face-to-face interaction [@baltesComputerMediatedCommunicationGroup2002; @lundHowArgumentationDiagrams2007]. On the other hand, the same presence of a technological device provides alternative or complementary means to create emotional awareness and even extend it over time [@derksRoleEmotionComputermediated2008; @gliksonDarkSideSmiley2018; @hegartyCognitiveScienceVisualspatial2011; @leonyProvisionAwarenessLearners2013; @derickEvaluatingEmotionVisualizations2017].

The presence of emotion in computer-mediated learningn environments even without seamless audio/video connection can be sustained by a corollary to the EASI model, illustrated in the previous section, which Van Kleef [-@vankleefSocialEffectsEmotions2017] identifies as the *functional equivalence hypothesis*. The hypothesis posits that the role emotion play at the inter-personal level is equivalent regardless of the specific way it is communicated, as long as the emotional information passes from the sender to the receiver. In the words of the author:

> If one accepts the notion that emotional expressions can influence social interactions by providing information about what is on the expresser's mind, it follows that emotions can have such effects regardless of how they are expressed, as long as the expressions convey the relevant information. Consequently, EASI theory posits that expressions of the same emotion that are emitted via different expressive modalities (i.e., in the face, through the voice, by means of bodily postures, with words, or via symbols such as emoticons) have comparable effects, provided that the emotional expressions can be perceived by others\
> --- @vankleefSocialEffectsEmotions2017, p. 213

On the other hand, the fact that various forms of emotional expression are equivalent with respect to their social function does not mean that each way of expressing an emotion is equivalent [@vankleefInterpersonalDynamicsEmotion2018]. A colleague's anger expressed by shouting on your face how badly your part of a collaborative task has been handled has not the same intensity of *I am very angry with you* written in an email. A diminished *veracity*, though, may be compensated by more favorable conditions to engage in inferential processes rather than affective reactions as described by the EASI model (*ibid.*). As stated in Section \@ref(awareness-tools), even in the case of emotional awareness, face-to-face interaction may not be the golden standard to aim for [@buderGroupAwarenessTools2011; @boehnerHowEmotionMade2007]. An EAT whose aim is to make learners' aware of their own and/or their colleague's emotions must therefore face the challenges of *encoding* and *decoding* emotional information, so that it maximized the preservation of its *functional* meaning.

In this regard, the *encoding* of emotional information has received so far greater attention than *decoding*, since it is tightly related to the action of detecting, representing or measuring emotions [@fuentesSystematicLiteratureReview2017; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @silvaComparativeStudyUsers2020]. In their review of technologies for emotion-enhanced interaction already cited in Section \@ref(affect-aware-systems), Cernea and Kern [-@cerneaSurveyTechnologiesRise2015] distinguishes between three types of techniques commonly adopted to estimate emotions: (1) perception-based estimation, derived from efferent manifestations of emotion such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009; @martinezContributionsFacialExpressions2016]; (2) physiological estimation, based on the detection of physiological patterns such as heart rate, blood pressure, or skin conductance [@ragotEmotionRecognitionUsing2018; @shuReviewEmotionRecognition2018]; and (3) subjective feelings, based on the person's self-report of her own emotional experience [@lavoueEmotionalDataCollection2017; @ritchieEvolutionSelfreportingMethods2016; @silvaComparativeStudyUsers2020]. As stated in the introduction, the thesis focuses on this last category. In fact, physiological estimation requires dedicated hardware and software, which would be difficult to provide at scale. Perception-based estimation may use more widely available hardware and software, for instance through a webcam or keyboard stroke, but the accuracy and usefulness of this kind of measure is still lively debated in the literature [@barrettEmotionalExpressionsReconsidered2019; @bahreiniMultimodalEmotionRecognition2016; @nahinIdentifyingEmotionKeystroke2014]. Barrett and colleagues [-@barrettEmotionalExpressionsReconsidered2019], for instance, argue that automatic recognition from facial expression are still limited in reliability, lack in specificity, and does not take sufficiently into account effects of context and culture. Voluntary self-report is therefore retained as the most parsimonious, portable and reliable way to provide emotional awareness both at the intra-personal and inter-personal levels, especially when the aim is to stimulate voluntary emotional introspection and/or inferential processes [@boehnerHowEmotionMade2007; @vankleefInterpersonalDynamicsEmotion2018].

Emotional self-report is tightly related to the underlying concept of *what an emotion is*, which will be discussed in Chapter \@ref(defining-emotion-unit). From a technical standpoint, emotion self-report is traditionally characterized either by the dimensional or the discrete emotion approaches, as well as by a combination of the two [@bradleyMeasuringEmotionSelfAssessment1994; @cowenSelfreportCaptures272017; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005].

In a dimensional approach, an emotion is conceptualized as a rating on a number of continuous criteria, among which the most frequently adopted are *valence* (also called *pleasure*) and *arousal* (also called *activation*) [*e.g.*, @russellCircumplexModelAffect1980; @stanleyTwodimensionalAffectiveSpace2009], but may also comprise different or additional dimensions. It is the case, for instance, of the Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] or the AffectButton [@broekensAffectButtonMethodReliable2013], which both use the *valence*, *arousal*, and *dominance* dimensions, even if in different formats. The Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] uses three rows of figures (*i.e.*, the *manikin*), where each rows represent one of the three dimensions. For each row, 5 figures, progressively modified in some features, represent the increasing or decreasing value on the specific dimension. The AffectButton [@broekensAffectButtonMethodReliable2013], on the other hand, uses a single iconic facial expression that changes according to the user's coordinates of the mouse on the surface of the icon: the horizontal movement determines the pleasure dimensions; the vertical movement the dominance dimension; and the arousal dimension is calculated according to the distance of the mouse from the central point of the image.

In the discrete approach, emotion is conceptualized as a phenomenon with distinctive features compared to other emotions, which may consist in different facial expressions when emotion is represented graphically, or semantic meaning when emotion is represented by natural language words or idioms [@torrePuttingFeelingsWords2018; @desmet2003measuring; @fontaineComponentsEmotionalMeaning2013]. The number, kind and organization of the representations varies depending on several aspects. In the case of verbal representations that adopts natural language nouns or adjectives, for example, the list may be determined according to emotion theories -- as in the case of basic emotion theory [@ekmanArgumentBasicEmotions1992] -- or determined empirically according to previous (or pilot) studies, often with the aim of retaining a list of the most frequently experienced or expressed items. For instance, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented in a self-report interface 20 buttons, 10 labeled with *negative* and 10 with *positive* emotion adjectives retrieved among the most frequently expressed during a computer-mediated collaborative task in a pilot study. When emotion is conceptualized graphically, representations usually attempt at maintaining some degree of analogy with *reality*, for instance with respect to facial expressions [@desmet2003measuring; @lauransNewDirectionsNonVerbal2012].

The dimensional and discrete approaches can be combined by providing discrete emotions a precise collocation on the dimensions. The result is often referred to as an *affective space* [@shumanConceptsStructuresEmotions2014; @schererWhatDeterminesFeeling2006; @gilliozMappingEmotionTerms2016]. Recent work on the meaning of emotional terms seems to suggest that four dimensions are needed to account for emotion differentiation : *valence*, *power*, *arousal*, and *novelty* [@fontaineGlobalMeaningStructure2013; @fontaineWorldEmotionsNot2007; @gilliozMappingEmotionTerms2016]. Gillioz and colleagues [@gilliozMappingEmotionTerms2016], for instance, empirically mapped through a principal component analysis 80 french emotions terms on the four-dimensional affective space. (More on this in Section \@ref(subjective-feeling).)

Emotional *decoding* in a computer-mediated environment is tightly linked on how emotion is *encoded* and has received so far limited attention [@bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Furthermore, visualization of emotion in this context is usually derived from affective information available in students' productions or communication exchanges, for instance through sentiment analysis rather than a dedicated tool [@leonyProvisionAwarenessLearners2013]. In the absence of specific representations for emotional awareness, more general guidelines about the visuo-spatial representation of data should be applied [@hegartyCognitiveScienceVisualspatial2011; @hehmanDoingBetterData2021]. Emotional *decoding* is also influenced by the objective of the visualization. Learners' emotion may be represented individually or collectively, as a single unit in time or grouped through short or long time spans. They may also be partially or fully available to all other students or only to a selection of colleagues (e.g. in a group work). In other words, if the aim of emotional *encoding* is rather straightforward, emotional *decoding* depends on a larger number of factors.

### Related Works

Henritius and collegues conducted a systematic review of empirical research conducted on universities' students emotions in virtual learning based on 91 articles published between 2002 and 2017 in four journals [@henritius2019]. Among the conclusions stated from this review, the authors state that "on a more critical note, we observed that only a few of the studies actually pay attention to the fluctuation of emotions in the context and in the flow of events" (*ibid.*, p. 97). According to the authors, most studies analyzed emotions retrospectively and conceptualize them as traits rather than transitory states or processes (*ibid.*).

The lack of studies investigating affect or emotion in a dynamic perspective is also due to the fact that there is scant work that has been dedicated to instruments that combine both emotion *encoding* and *decoding* in a computer-mediated learning environment, especially in real-time [@lavoueEmotionalDataCollection2017; @ez-zaouiaEmodashDashboardSupporting2020]. As pointed out by Lavoué and colleagues [@lavoueEmotionalDataCollection2017], the few attempts that have been made are mostly ad-hoc solutions that do not aim at a general application. As a result, in learning settings, emotion self-report is often provided through questionnaires [@pekrunMeasuringEmotionsEpistemic2016; @pekrun2011] or adaptation of experience sampling methods [@scollonExperienceSamplingPromises2003; @csikszentmihalyiValidityReliabilityExperienceSampling2014], as in Molinari and colleagues discussed above [@molinariEMORELOutilReporting2016]

Emotion representation, on the other hand, is even less frequent and therefore less developed, especially in real-time [@ez-zaouiaEmodashDashboardSupporting2020; @bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Ez-zaouia and colleagues [-@ez-zaouiaEmodashDashboardSupporting2020], for instance, developed EMODASH, a dashboard for visualizing retrospective emotions for tutors in an online learning environment. @leonyProvisionAwarenessLearners2013 and @derickEvaluatingEmotionVisualizations2017 also propose dashboard inserted into a computer-mediated learning environment, but limited to a handful of emotions (e.g. *frustrated*, *confused*, *bored*, *happy*, and *motivated),* which are automatically derived from learners' activities. @leonyProvisionAwarenessLearners2013, in particular, propose a series of visualizations organized as time-based*,* context-based*,* visualizations of change in emotions*, and* visualization of accumulated information.

To the best of my knowledge, there is only a handful of tools reported in the literature that come close to the purpose of an EAT as considered in the present contribution. I will focus on three of them in the reminder of this section.

Feidakis and colleagues [@feidakisProvidingEmotionAwareness2014; @feidakis2013] developed the *emot-control* emotion aware system, depicted as a pop-up window in Figure \@ref(fig:emot-control-image). The tool combines the dimensional and discrete approaches by placing 12 icons of facial expressions *(inspired, excited, interested, relaxed, curious, confused, anxious, indifferent, bored, tired, angry, desperate*) in Russel's [-@russellCircumplexModelAffect1980] Valence x Arousal/Activation circumplex, with a neutral face in the middle. The tool also presents a text input, which originally shows the noun associated to the icon on which users have clicked, but that can also be switched to another semantic word/expression. Users can also provide more context to their feeling by typing a description through a second text-area. 5 other icons on the right-hand of the interface represent each a different mood (*sad*, *unhappy*, *neutral*, *happy*, and *very happy*). Users can access their own last affective episode reported directly on the interface, whereas they have to click on a button to open a new window to access their colleagues' emotions. The window shows a vertical timeline of affective episodes organized by group members, with the date, the time, the emotion (combination of icon and noun/expression), and the mood. In an experiment, for instance, Feidakis and colleagues [@feidakis2013] adapted the tool to the Moodle learning environment and, in accordance to the Affect-Aware perspective (see previous chapter), endowed the environment also with an Affective Virtual Assistant, which responded to learners affective episodes. Congruently with the approach advocated in this contribution, learners where free to use the tool as they wished, and they could therefore dispose of emotional awareness at any time in the learning environments. In another, similar, study [@feidakisProvidingEmotionAwareness2014], the authors also collected the System Usability Score @brookeSUSQuickDirty1996 -- a widely adopted usability scale -- based on the rating of $N = 29$ participants, obtaining a rating of $M = 67.91$ (no SD provided) out of 100. (More about this in Chapter \@ref(study-comparaison).)

(ref:emot-control-image-caption) Image of the overall interface in which the *emot-control* is inserted. Retrieved from @feidakis2013, Figure 7 in the original article. p. 1653.

```{r emot-control-image, out.width="100%", fig.cap="(ref:emot-control-image-caption)"}
knitr::include_graphics(here::here("figure/theory/emot-control-feidakis.png"))
```

The tool provided by Feidakis and colleagues comply with all the three main features of an EAT as advocated by the present contribution: it is based on voluntary-self report, it incorporates an emotional structure based on Russel's (1980) circumplex, and allows moment-to-moment emotional awareness. On the other hand, the tool separated the *displaying/expressing* and the *monitoring/perceiving* function of an awareness tools, which are provided in two separates windows. Furthermore, the use of icons representing synthetic facial expressions present some limitations. From a spatial perspective, augmenting the number of options would reduce the size of the icons and make identification more difficult. Depicting emotions with icons is also more prone to misunderstanding or difficulty to provide a sufficient number of images, and different enough to discriminate between similar feelings. For instance the emotion placed from noon to 1 (*inspired*) in the circumplex is very similar to that placed from 2 to 3 o'clock (*interested*). It is useful to note in this regard that a previous version of the tool combined the icon and the label, for the label being dropped in the latest version to overcome language barriers [@feidakis2013]. Finally, the tool combine emotions and moods on the same observation, which may be problematic given a growing consensus in considering the two as distinct phenomena: we may have moods and emotions at the same time, but also only moods or only emotions [@linnenbrink-garciaAdaptiveMotivationEmotion2016; @schererWhatAreEmotions2005; @scherer2022].

The Mood Meter Mobile Application[^1], depicted in Figure \@ref(fig:mood-meter-image), is a digital extension of the physical Mood Meter dashboard of the RULER approach introduced in Section \@ref(socio-affective-competences). The physical Mood Meter consists in four colored quadrants organized according to two axes: valence and activation/arousal. Learners can therefore place themselves in one of the four quadrants according to (1) how pleasant or unpleasant they feel, and (2) how high or low is their energy. The top-right yellow quadrant corresponds to emotions which are pleasant and high on energy, such as *excitement*, *joy* and *elation*. The bottom-right green quadrant corresponds to emotions which are pleasant and low on energy, as *tranquility*, *serenity*, and *satisfaction*. The bottom-left blue quadrant corresponds to emotions which are unpleasant and low in energy, as *boredom*, *sadness* and *despair*. And finally, in the top-left red quadrant corresponds to emotions which are unpleasant and high in energy, such as *anger*, *frustration* or *anxiety*. In the digital app, the overall *dashboard* is divided in 100 points (25 per quadrant). Users can tap one of the point in the quadrant and obtain 9 emotion words, that is the word that is associated with the very point they have clicked, plus 8 alternatives, which corresponds to the 8 adjacent points on the dashboard. Once identified the emotion term that corresponds to how the learner is feeling, the person can: (1) describe the feeling by typing in more information (as in Feidakis etl al); and (2) select a strategy between quotes, images or practical tips that are meant to help the person shift to another feeling if desired. Further features of the app include the possibility to track the feelings entered in the app, for instance with an overview of the percentage per quadrant; set a reminder to use the app at specific periods, congruently with an experience sampling perspective [@csikszentmihalyiValidityReliabilityExperienceSampling2014]; and finally share their feelings via Facebook or Twitter.

```{r mood-meter-image, out.width="60%", fig.cap="Image of the four quadrants of the Mood Meter, retrieved from Brackett et al. (2019), Figure 1 in the original article. For the Mood Meter app, see the official site in the footnote."}
knitr::include_graphics(here::here("figure/theory/mood-meter-image.png"))
```

As the *emot-control*, also the Mood Meter app comply with the three main features of an EAT advocated in the present contribution, for they both are self-report tools, based on the same underlying emotion structure, and can be applied in a moment-to-moment perspective. On the other hand, the Mood Meter app seems to be more oriented towards intra-individual emotional awareness: the possibility to share the emotion via a social platform is a very limiting feature to provide group-specific and ongoing inter-personal awareness. Furthermore, to the best of my knowledge, the app is provided only in a mobile version, which would force learners to switch back-and-forth from the computer-mediated learning environment to their phone.

A third and final tool cited in this overview is the one adopted in the contribution of @molinariEmotionFeedbackComputermediated2013, already discussesd in the related works of Section \@ref(ea-inter-personal) about inter-personal awareness. As a reminder, half of the dyads in a computer-mediated collaborative task disposed of a persistent EAT on the right-hand side of the screen. As shown in Figure \@ref(fig:tf-eatmint-eat), the EAT is vertically divided in two areas. On top, the monitoring-perceiving area consists in the last three discrete emotions expressed by the participant (green boxes) and the partner (blue boxes). The box on top of each pile, representing the very last emotion expressed, is highlighted with a paler color. The lighter green box is also editable by the participant, who can type-in an emotion not available through the buttons in the lower part of the screen. These buttons represent the displaying-expressing function and are organized in two columns: the right one with 10 *positive* emotions, and the left side with 10 *negative* emotions. The list of discrete emotions were defined mixing a previous study in the field [@dmelloDynamicsAffectiveStates2012] and 2 pre-experiments that aimed to identify the most frequent and intense emotions felt during a situation of collaboration, real or imagined.

(ref:tf-eatmint-eat-caption) The argument graphic tool and the EAT adopted in @molinariEmotionFeedbackComputermediated2013, from Figure 1 in the original article.

```{r tf-eatmint-eat, fig.align="center", fig.cap="(ref:tf-eatmint-eat-caption)", out.width="80%"}
knitr::include_graphics(here("figure/theory/eatmint-eat-and-task.png"))
```

The EAT adopted by Molinari and colleagues (*ibid.*) is based on self-report and provide moment-to-moment emotional awareness, being persistently on screen. On the other hand, though, it adopts a purely discrete approach to emotion, without implementing any explicit emotion structure into the tool beside the dichotomous organization of emotion terms according to the *positive* vs. *negative* macro-valence discussed in Section \@ref(affect-on-learning) [@erbasRoleValenceFocus2015; @shumanLevelsValence2013; @colombettiAppraisingValence2009]. In this regard, as a follow-up of the experiment, the authors reckoned that the tool could be improved on a number of points, which became the input for my Master thesis [@fritzReinventingWheelEmotional2015] detailed in Chapter \@ref(dew-chapter).

The three EATs illustrated in this section highlight some of the many choices in *encoding* and *decoding* emotional information that can be made in providing emotional awareness through a dedicated self-report tool. These choices can influence emotional awareness both at the intra- and inter-personal levels in ways that are put into perspective in the next section, which provides an abstract model of the functions of an EAT.

## Abstract Model of the Functions of an Emotion Awareness Tool {#abstract-model-of-ea}

The information illustrated above, as well as in the previous chapters, can be integrated in an abstract model that depicts the mechanisms allegedly carried out by an EAT based on the three underlying assumptions, namely that emotional awareness is usefull at the individual level, the collective level and that it can be fruitfully implemented in a computer-mediated learning environment. The proposed abstract model, though, does not have the pretension to be exhaustive. On the contrary, it aims at providing an organized overview of some of the many different affective, learning or human-computer interaction processes that are (or may be) assumed in the instrumentality of an EAT, as well as how design choices can sustain these processes. The model takes into account elements that are specific or particularly important from the perspective of an Emotion Awareness Tool, assuming that the general functions of an Awareness Tool illustrated in Section \@ref(awareness-tools) are also at stake.

The model, depicted in Figure @ref(fig:intro-thesis-scm-model-figure), take the perspective of a learner that disposes of an EAT in its computer-mediated learning environment and comprises boxes for four conceptual elements: (a) the learning activity, which may broadly refer to any computer-mediated environment implementing an instructional design; (b) intra-personal emotion, representing a single event or a set of events corresponding to the learners' expressed-displayed emotions; (c) inter-personal emotion, representing a single event or set of events corresponding to emotions expressed-displayed by other learners sharing the same environment and that the learner herself can perceive-monitor; and (d) an overarching *meaning-making* process, which encompasses learner's effort to extrapolate instrumental information from the emotional information available. The boxes are connected with directional arrows, numbered from 1 to 7, representing a series of processes that are (or may be) implicated in each passage. The numbers are used for identifying the passages, but do not imply a fixed order, even though some processes may be built upon previous passages. The next sections discuss each passage more in detail.

(ref:thesis-scm-caption) Abstract model of the functions of an EAT from the perspective of a single learner disposing of it in a computer-mediated learning environment. Numbers pinpoint passages or processes that may influence the instrumentality of an EAT in sustaining the function at stake.

```{r intro-thesis-scm-model-figure, fig.cap="(ref:thesis-scm-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/intro/thesis-scm-model.png"))
```

### From the Learning Activity to Intra-Personal Emotion

Passage #1 goes from the learning activity to an intra-personal emotion, implicating that the learning activity elicits emotions in oneself. An EAT may intervene in this passage at least in two ways, labelled here as *emotion alertness* and *emotion conceptualization*.

Emotion alertness broadly refers to the fact that the presence of the EAT represents a general reminder about paying attention to emotion during the learning activity, and therefore may increase the *sensitivity* about emotional experience. In other words, the presence of the EAT may push learners to bestow to emotional self-awareness more attention that they would normally do without the presence of the EAT [@brackettRULERTheoryDrivenSystemic2019; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Emotion alertness may be induced to different degrees, depending on, for instance, whether the EAT is always available or not, and whether explicit prompts are frequently sent to learners [@csikszentmihalyiValidityReliabilityExperienceSampling2014; @shiffmanEcologicalMomentaryAssessment2008].

Emotion alertness is not necessarily and automatically linked to concrete action of emotion expression-display. In fact, even in the case of an increased alertness, learners could genuinely not feel any emotion, or decide not to express their emotion through the EAT for dispositional [@kringIndividualDifferencesDispositional1994; @scherer2021] or contingent reasons (*e.g.*, bestow priority to the learning task at hand).

Emotion conceptualization, on the other hand, broadly refers to the way learners self-report their emotions, which may vary greatly according to a number of theoretical and technical aspects, such as the dimensional, discrete, or combined approach to emotion measurement/reporting. Depending on how emotion is conceptualized and, by extension, planned for self-report, the intra-personal awareness of emotion may be more or less guided, inducing learners to pay attention to some elements of their emotional experience that they will not necessarily consider in the same way, or with the same weight, depending on the specifics of the EAT at hand.

At the same time, emotion self-report also depends on the learners' ability to identify their current emotional episodes and provide an accurate representation of them [@erbasRoleValenceFocus2015; @smithPatternsCognitiveAppraisal1985; @siemerSameSituationdifferentEmotions2007; @schererDynamicArchitectureEmotion2009]. Erbas and colleagues [-@erbasRoleValenceFocus2015], for instance, found evidence that people who differentiate only between a limited set of discrete emotions tend to use valence alone as a discriminating criterion. On the contrary, people discriminating between a larger panel of discrete emotions focus more thoroughly on different features of the situation at hand (see also Chapter \@ref(defining-emotion-unit)). An EAT may therefore impinge learners to reflect on a greater variety of potential emotional experiences rather than reinforcing the *positive/pleasant* vs*. negative/unpleasant* loop.

Finally, the EAT may influence learners to focus only on the emotional experience *per se*, or also provide contextual information about the situation or potential antecedents of that experience at various level of details [@feidakisProvidingEmotionAwareness2014; @lavoueEmotionAwarenessTools2020]. The richer the details and the reflection to account for them, though, the greater the effort for learners to concentrate and report about the affective component.

To sum up, the presence and features of an EAT may not only contribute to determine whether learners will pay attention to their emotional experience, but also how they will be guided to think about it, planning the way to next passage.

### From Intra-Personal Emotion to Meaning-Making

Once an emotion or a set of emotions are self-reported, it is assumed that the learner may extrapolate meaning from the emotional information available, which herself has injected into the system. In this regard, information may be punctual, for example the last emotion or set of emotions displayed concurrently. Conversely, the system may also implement some form of data aggregation and persistence, which allows to observe the accumulation and the evolution of the emotional experience over a varying time span [@leonyProvisionAwarenessLearners2013; @bersetVisualisationDonneesRecherche2018]. Depending on the way emotion information is graphically represented, emotional meaning-making can be guided by the system, which may privilege some form of representation (*e.g.*, changes over time) over another (*e.g.*, cumulative frequency of the same emotion expressed).

Meaning-making is also tightly related to the way emotion is expressed through the system (passage #1), not only from a technical standpoint -- since the information available is determined by what information is inserted into the system -- but also conceptually. If emotion is considered from a dimensional standpoint, learners will extrapolate meaning from the criteria through which dimensions are rated. On the contrary, in a discrete emotion approach, learners extrapolate meaning from an *information unity* that is supposed to provide unique meaning compared to other possible choices. If the system combines dimensional and discrete approach, learners can extrapolate meaning from both. Moreover, if contextual information is provided, learners can situate emotion more easily, whereas inferences on causes and consequences of behavior must be drawn from representations of emotion alone [@vankleefInterpersonalDynamicsEmotion2018].

The form through which emotional information is rendered on screen also influences the cognitive effort necessary to process it [@hegartyCognitiveScienceVisualspatial2011]. According to complexity and richness of the graphical rendering, processing can span on a continuum with *almost effortless perception* on the one side, and *deliberate cognitive effort* on the other.

To sum up, the design of the EAT determines the way by which the learner extrapolates meaning from her own emotional information and therefore influence to what extent the learner has interest in monitoring what she has inserted into the system.

### From the Learning Activity to Inter-Personal Emotion

Passage #3 determines how the learner becomes aware of emotional information provided by others. It therefore starts a complementary path with respect to passages #1 and #2, but from an inter-personal perspective. This path may in fact be optional when the EAT is exclusively concerned with emotion self-awareness [*e.g.*, @lavoueEmotionAwarenessTools2020], whereas it becomes an integral part when the EAT is inspired from a Group Awareness Tool perspective [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @feidakisProvidingEmotionAwareness2014; @molinariEmotionFeedbackComputermediated2013].

As for the intra-personal passage #1, the presence of the EAT can have, in the first place, an alertness effect, prompting the learner to pay more attention to her colleague(s) emotions of what she would have had without the presence of an EAT, as suggested by the results of @eligioEmotionUnderstandingPerformance2012 and @molinariEmotionFeedbackComputermediated2013 outlined in the empirical works of Section \@ref(ea-inter-personal). In this regard, the design of the EAT may determine to what extent emotion information about others is readily available during the learning activity, as with the EAT adopted my Molinari and colleagues (*ibid.*), or must be voluntarily sought after in another screen, as in the case of the *emot-control* in Feidakis and colleagues [@feidakisProvidingEmotionAwareness2014; @feidakis2013]. The closer the emotional information is to the task, the greater may be the chances to frequently notice and pay attention to it. At the same time, considering that affective stimuli often are bestowed precedence [@poolAttentionalBiasPositive2015; @broschPerceptionCategorisationEmotional2010] in cognitive processing, the ongoing availability of emotional information may also be perceived as disruptive by the learner.

### From Inter-Personal Emotion to Meaning-Making

Passage #4 consists in extrapolating meaning from the emotion of other learners sharing the same computer-mediated learning environment and whose emotions have been monitored by the learner herself. This passage can be illustrated through a series of questions the learner is likely to ask herself.

The first question may be: do I understand the emotional experience of my colleagues? This can be considered in two ways. First, in absolute and non-contextual terms. For instance, what does it mean to be *confused*, *angry*, *ashamed*, etc. for someone else? Or what does it mean to be high vs. low on the *Valence*, the *Arousal,* etc. dimension for someone else? Is it the same as for me or not? Second, in relative and contextual terms. That is, what does it mean that my colleague is *confused*, *angry*, *ashamed*, etc. in this specific situation? Or what does it mean that my colleague is high vs. low on the *Valence*, the *Arousal*, etc. dimension in this specific situation?

A subsequent questions may consists in wondering how may the emotional experience of my colleagues impact their behavior? As seen in the inter-personal function of emotion in Section \@ref(ea-inter-personal), causes and consequences of behavior can be inferred from emotion in others. This passage does not only require the ability to discriminate between different emotional experiences in the abstract and in the relative terms, but also to *attach* potential effects on the learning task at hand.

The design of an EAT can intervene in this passage even more preeminently compared to passage #4 from the inter-personal perspective. In that case, the learner herself can be more confident in extrapolating meaning making for her own emotions, since she is the one who has both experienced and decided to inject them into the system. It is therefore safe to assume the learner can come closer to the *intended* meaning, since she disposes of background knowledge. On the contrary, inter-personal meaning making may vary greatly based on the amount of background knowledge, depending for instance on how acquainted the learner is with her colleagues. In the extreme case of no acquaintance, as is the case in empirical settings, the information available through the EAT may be the only *explicit* emotional information available. As suggested by @janssenCoordinatedComputerSupportedCollaborative2013 in Section \@ref(mutual-modeling), socio-affective cues can also be derived by the content/collaborative space of the learning task at hand (see Figure \@ref(fig:janssen-bodemer-framework)).

To sum up,

### From Intra-Personal Emotion to Inter-Personal Emotion

Expression of emotion: emotion is made available to others depending on learners willingness to disclose their emotional experience.

Emotion influence.

Emotion sharing.

### From Inter-Personal Emotion to Intra-Personal Emotion

Emotion comparison (e.g. affiliation or differentiation). Emotion contagion. Mutual modeling.

### From Meaning-Making to the Learning Activity

Emotion is integrated into the learning activity, it modifies behavior (self-regulation, co-regulation, socially shared regulation of learning and/or emotion).

Modification of learning activity can trigger another emotion, creating cycle.

With Emotion Awareness this passage is bestowed to learner, contrary to Affect-Aware where some form of intelligence may intervene and guide the process.

## Synthesis

[^1]: <https://moodmeterapp.com/>
