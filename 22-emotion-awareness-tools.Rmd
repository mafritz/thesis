# Emotional Awareness Tools in Computer-Mediated Learning Environments: Overview, Assumptions, Mechanisms, and Methodological Issues

```{r rationale-eat-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

After a general overview about the relationship between affect and learning in general, this chapter focuses on emotional awareness and, by extension, an Emotion Awareness Tool (EA) more specifically. First, the concept of emotional awareness is defined within the context of computer-mediated learning environments. Second, three main assumptions are derived by this definition. The theoretical underpinnings of each assumption are set forth, complemented by the overview of related works related to the thesis. In the second part of the chapter, an abstract model of an EAT is laid out in an attempt to frame the different interactions between the learners and an EAT, for emotional awareness to be instrumental. The model also allows to highlight a number of design choices that have both theoretical and technical consequences in maximizing the chances for the EAT to be useful to learners. In this regard, the chapter ends with the claim that an EAT may best serve its purpose if it is built upon and integrates a theory-driven emotional structure.

## Definition of Emotional Awareness in Computer-Mediated Learning Environments

Emotional awareness in computer-mediated learning environments is often proposed in the literature as self-explaining or defined somehow implicitly. For instance, as stated in the previous chapter, Feidakis and colleagues [-@feidakisReviewEmotionAwareSystems2016, p. 217] define emotion awareness as "the *implicit* or *explicit collection of emotion data* and the *recognition* of *emotion patterns*" (italics in the text). Cernea and colleagues [-@cerneaSurveyTechnologiesRise2015], in their overview of rising technologies in emotion-enhanced interaction, identify the category of affective-aware systems as "designed to improve affective self-awareness of a user (i.e.,internal affective awareness) or awareness of the emotions of other users (i.e.,external affective awareness)" (*ibid.*, p.78). More precise definitions are provided by Lavoué and colleagues [-@lavoueEmotionAwarenessTools2020], which are partially derived from clinical psychology. They first definition provided by Lavoué and colleagues is based upon @bodenFacetsEmotionalAwareness2015. Emotional awareness is defined as "the ability to perceive, identify, and understand emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. Later in their article, Lavoué and colleagues (*ibid.*) propose another definition, based upon Rieffe and colleagues [-@rieffeEmotionAwarenessInternalising2008], referring to emotional awareness as "the attentional process by which individuals identify, explain and differentiate between their own emotions as well as the others' emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. These two definitions resonate with socio-emotional competences briefly illustrated in the previous chapter. Finally, later in the same article, Lavoué and colleagues (*ibid.*) provide a more technical definition of emotion awareness tools: "[e]motion awareness tools can be defined as tools that display information on own' own [sic] or partners' emotions, circumstances and antecedents" [@lavoueEmotionAwarenessTools2020, p. 284]. Compared to the more abstract definition in Feidakis and colleagues or in Cernea and colleagues (*ibid.*), the definition in Lavoué and colleagues (*ibid.*) implies that the tool shall also provide information about circumstances and antecedents that concurred in eliciting the emotional episode.

From these definitions it is possible to extrapolate three fundamental assumptions about emotional awareness and, by extension, the instrumentality of an EAT in a computer-mediated learning environment:

1.  Learners may benefit from intra-personal emotional awareness by using information about their own emotions as valuable information for their own learning processes (self-regulation).
2.  Learners may benefit from inter-personal emotional awareness by using information about their partners' emotions and/or by communicating their own emotions to their partners and/or by a combination of the two. One or all of these circumstances may contribute to learners' own learning processes (self-regulation), the learning processes of their partners individually (co-regulation), or the learning processes of the group as a whole (socially shared regulation).
3.  Learners may benefit from emotional awareness conveyed by a dedicated tool, which *encodes* emotional information into the computer-mediated learning environment (expressing-displaying function), and *decodes* that information (perceiving-monitoring function), for learners to extrapolate emotional meaning-making instrumental to learning processes.

Each assumption is discussed in the following three sections. Every section proposes an overview of related theories, as well a selection of related works, which are of particular interest for the present contribution in terms of objectives, methodologies, or findings.

## Emotional Awareness at the Intra-Personal Level

The first assumption about the instrumentality of emotional awareness implies that learners can benefit from it at the individual, intra-personal level.

### Theoretical Underpinnings

The role of emotion at the individual and intra-personal level has received over the last few decades extensive consideration from different perspectives and using different methods of investigation. A widespread consensus has emerged over the years about the fact that emotion, rather then disruptive of behavior and in antithesis with cognition, plays a prominent role in helping the organism to cope with a complex environment [@adolphsNeuroscienceEmotionNew2018; @armonyCambridgeHandbookHuman2013; @damasioDescartesError2006; @damasioStrangeOrderThings2018; @schererEmotionTheoriesConcepts2009; @immordino-yangEmotionsLearningBrain2016; @levensonAutonomicNervousSystem2014; @levensonIntrapersonalFunctionsEmotion1999; @leventhalRelationshipEmotionCognition1987; @pessoaCognitiveemotionalBrainInteractions2013; @sanderModelsEmotionAffective2013].

For instance, emotion is known to influence high-level cognitive functions such as attention, perception, memory, and decision-making [@broschImpactEmotionPerception2013], which are all implicated in learning processes. A large body of research suggests that a dedicated system to emotional attention may complement and interact with the exogenous and endogenous systems [@broschAdditiveEffectsEmotional2011]. Emotionally charged stimuli are also known to often bestow precedence in perception over non emotionally-charged stimuli [@broschPerceptionCategorisationEmotional2010]. Many links between emotion and memory have been established both in the encoding and retrieving of information [@sharotHowArousalModulates2004; @kensingerRetrievalEmotionalEvents2020], which may have important consequences for learning [@beegeMoodaffectCongruencyExploring2018; @tyngInfluencesEmotionLearning2017]. Finally, it has been determined that when emotion is not taken into account in decision-making, consequences can be highly disruptive for the person [@becharaRoleEmotionDecisionmaking2004; @rollsEmotionDecisionmakingExplained2014].

Emotional awareness, though, goes a step forward in considering the importance of emotion at the intra-personal level, because it presupposes that learners can benefit from being aware of their emotions [@lavoueEmotionAwarenessTools2020]. Whether consciousness is a necessary condition for emotion to *exist* [@ledouxHigherorderTheoryEmotional2017; @liebermanBooConsciousnessProblem2019] is a debate outside the scope of the present contribution (see also next chapter): the use of voluntary self-report requires consciousness for emotional awareness to emerge in the first place and therefore *dodges* the issue. Conscious processing derived from emotional awareness has been related more specifically to three overlapping factors in learning processes [@lavoueEmotionAwarenessTools2020]: (1) emotion as useful information to direct or redirect cognitive resources; (2) emotion as useful information to assess and guide learners' interest and motivation; and (3) emotion as useful information to regulate one's own behavior, including one's own emotions.

Learners may use their achievement, epistemic and topic emotions as useful cues to evaluate and direct cognitive processes [@pekrunControlValueTheoryAchievement2014]. The work of D'Mello and colleagues [-@dmelloConfusionCanBe2014], for instance, highlights how confusion is a signal of cognitive disequilibrium, which learners are therefore incited to re-balance. The work of Vogl and colleagues [-@voglSurpriseCuriosityConfusion2019] about the role of epistemic emotions suggests that experience of curiosity, pride and shame may inform learners about different reasons for knowledge exploration.

Awareness of one's emotions may prompt learners to assess their motivation to keep engaged or disengage from a learning activity [@linnenbrink-garciaAdaptiveMotivationEmotion2016]. For instance, Baker and colleagues' work [-@bakerBetterBeFrustrated2010] draws attention on the importance of recognizing boredom and frustration as warnings of inefficient and potentially misleading efforts.

Finally, emotion may be used as useful information to regulate one's own behavior, including emotion regulation itself [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]. When performed explicitly rather than implicitly [@torrePuttingFeelingsWords2018], emotion regulation requires the person to be aware of the emotion to be regulated [@lavoueEmotionAwarenessTools2020]. According to Gross, "[e]motion regulation refers to the processes by which we influence which emotions we have, when we have them, and how we experience and express them" [@grossEmotionRegulationAffective2002, p. 282]. The process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015] posits that regulation can happen at five successive stages:

1.  *situation selection*, which consists in avoiding or experiencing events according to their probability of eliciting unwanted or sought after emotions;
2.  *situation modification*, consisting in attuning the situation once it has been selected or it is forced upon the person;
3.  *attentional deployment*, by which some characteristics of the situation are considered more relevant than others;
4.  *cognitive change*, through which the focal points of the situation can be reappraised in an attempt to shift, prolong or increase an emotional experience;
5.  *response modulation*, which occurs once an emotion has already been elicited and the person attempts, for instance, to suppress its manifestations (*e.g.*, trying not to shake during and oral exam or suppressing a laughter during a lesson).

The fourth and fifth passages have been often considered as two alternative strategies for emotion regulation, with potentially different impact on person's cognition and behavior [@bonannoImportanceBeingFlexible2004; @grossEmotionRegulationAffective2002; @richardsEmotionRegulationMemory2000]. In some circumstances, cognitive change can be more beneficial, since it reassesses the situation in what shall be more favorable terms for the person, freeing resources that would otherwise be monopolized by the undesired emotion [@richardsEmotionRegulationMemory2000]. In other circumstances, a cognitive re-evaluation of the situation may be too demanding, and the person may have more benefits in trying to suppress the response modulation [@bonannoImportanceBeingFlexible2004].

Torre and Lieberman [-@torrePuttingFeelingsWords2018], on the other hand, hypothesize that emotion regulation may occur even implicitly through *affect labeling* [@liebermanAffectLabelingAge2019; @liebermanPuttingFeelingsWords2007; @liebermanSubjectiveResponsesEmotional2011], that is, when people assign a word to the their emotional experience (*e.g.* "I feel angry"). In their article, the authors provide an overview of research about affect labeling and state that it "has demonstrated a modulation of emotional output effects in the same experiential, autonomic, neural, and behavioral domains as found in other forms of emotion regulation" [@torrePuttingFeelingsWords2018, p. 117]. The authors also highlight, though, that the mechanisms by which affect labeling intervene as implicit emotion regulation are still unclear. In this regard, Torre and Lieberman (*ibid*.) propose four possible mechanisms:

1.  *distraction*, for resorting to language shift the attention from the situation itself and therefore attenuates full processing of the eliciting event;
2.  *self-reflection*, as a means to initiate an introspection process fostering self-distancing from the emotion;
3.  *reduction of uncertainty*, which results from categorizing an intense and often nuanced experience using known and community-shared words;
4.  *symbolic conversion*, consisting in events assuming symbolic status through the associated word, which may induce more abstract thinking about the eliciting event.

Emotional awareness may therefore be useful either as explicit [@grossEmotionRegulationCurrent2015] or implicit [@torrePuttingFeelingsWords2018] emotion regulation. It may also help learners in evaluating whether a regulatory strategy may be more or less appropriate given the situation at hand [@lavoueEmotionAwarenessTools2020].

TO SUM UP

### Related Works

Molinari and colleagues [-@molinariEMORELOutilReporting2016] developed a self-reporting system from an experience sampling method [@csikszentmihalyiValidityReliabilityExperienceSampling2014] perspective, the EMORE-L (EMOtion REport for E-Learning), which was used in an ecological setting by 16 university students who voluntarily adopted the system during 15 days of distance learning in a blended bachelor program. The system consists in a short online questionnaire that students were reminded to fill once per day through an email that they received at a time previously concerted with the investigators. The EMORE-L consists in nine short questions, which are meant to reduce the amount of time needed to fill-in the questionnaire, organized in 4 parts: a first part about the situation the students were facing, with information about the activity students were conducting; a second part about the cognitive evaluation of the situation on three dimensions (Control, Value, Activation); a third part about the emotional experience, with the possibility of choosing between eight discrete emotions (*pleasure*, *anxiety*, *curiosity*, *boredom*, *engagement*, *confusion*, *surprise*, and *frustration*), for which students could also provide the intensity; and a last part about the social sharing of emotion, with items related to the wish for students to share their emotions with their colleagues, as well as the mutual knowledge of emotions between the student and their colleagues.

Through the 169 questionnaire that were filled throughout the 15 days, Molinari and colleagues (*ibid.*) were able to assess three main exploratory subjects. For each element, the authors used results of the study to discuss consequences on the implementation of emotion self-reporting tool in a context of emotional awareness.

The first subject under scrutiny concerned what emotions were most frequently associated with the different situations a student may encounter during distance learning. In this regard, results highlight four main activities, ordered from the most frequent: reading resources, synthesis of the same resources, individual work, and group work. The three most likely emotion to be experience are *pleasure*, *anxiety*, and *surprise*, whereas students made a scant use of the other five emotions proposed by the list. The authors also evinced from their data that the frequency of emotion changed as a function of the activity. On this ground, Molinari and colleagues suggest that self-report tool should take into account the specific activity that they aim to sustain, avoiding thus generic list of emotions.

The second phenomenon explored in the study investigated to what extent students wished to share their emotions with their colleagues, and in what situation. According to their results, Molinari and colleagues posit that students prefer to share their emotions during individual and group works. Furthermore, students were also more inclined to share their emotions when they experienced *anxiety* or *pleasure*. Based on these results, the author suggest that self-reporting tools shall not be provided at all time, but only when the activity is likely to elicit emotions learners wish to share.

Finally, Molinari and colleagues assessed the contribution of the dimensional approach of the cognitive evaluation items and of the eight discrete emotions in allowing students to express what they felt. The dimensional vs. discrete emotion approach to emotion self-reporting is a longstanding debate [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005], with both approaches presenting advantages and shortcomings in terms of user experience and quality of data (see below in the chapter). Results observed by Molinari and colleagues, augmented by feedbacks from students regarding the use of the system, confirm this trend. For instance, the authors highlight the scant use of the different discrete emotions, with only three out of eight being adopted frequently. At the same time, the cognitive evaluation provided through the *control*, *value* and *activation* dimensions also presented shortcomings: *pleasure*, *anxiety* and *surprise*, in spite of their diversity, were in fact associated to similar appraisal profiles. On this ground, the authors point out how important it is that the self-reporting system helps students in identifying their emotional experience.

Molinari and colleagues (*ibid*) exploratory work provide useful elements with respect to emotional awareness in a voluntary self-reporting setting. On the technical standpoint, the EMORE-L combines antecedents of emotions, in the form of cognitive evaluation on dimensions, with emotional experience expressed as discrete natural language words, even though the two blocks of items are not intertwined in the user experience. Also, even if relatively short, the use of 9 questions to express one's emotions is more suitable for a *scripting* rather than a moment-to-moment emotional awareness.

At a conceptual level, Molinari and colleagues introduced emotional awareness in individual settings, as a means to provide students with self-awareness of their own emotions. At the same time, the authors also measured, though, students' wish to dispose of collective emotional awareness. To this extent, Molinari and colleagues highlight how students expressed the wish to have full control over when and which emotional information to disclose. In addition, they also manifested the wish of being aware of their colleagues emotions, that is, bringing emotional awareness at the inter-personal level, which is the subject of the next section.

## Emotional Awareness at the Inter-Personal Level

The second assumption about the instrumentality of emotional awareness posits that learners can benefit from emotional information available at a collective level, either for their own learning processes, that of their partners, or both at the same time.

### Theoretical Underpinnings

Whereas the intra-personal function of emotion has received extensive attention in the last few decades, several researchers have more recently advocated the need for investigating the inter-personal function of emotion [@parkinsonEmotionSocialRelations2005; @rimePartageSocialEmotions2005; @vankleefInterpersonalDynamicsEmotion2018; @fischerWhereHaveAll2010]. Fisher and Van Kleef [-@fischerWhereHaveAll2010], for instance, posit that

> [i]t is an indisputable fact that emotions are mostly reactions to other people, that emotions take place in settings where other people are present, that emotions are expressed towards other people and regulated because of other people. It is hardly possible to imagine the elicitation of anger, shame, sadness, happiness, envy, guilt, contempt, love, or hatred without imagining other people as cause, target, or third-party observer of these emotions. In other words, the social constitution of emotions is beyond doubt -- @fischerWhereHaveAll2010, p. 208.

Keltner and Haidt [-@keltnerSocialFunctionsEmotions1999] identify four level of analysis in which emotion play a social function: (1) the individual level, whenever the source of the emotion is of a social nature; (2) the dyadic level, such as in direct dialogue or collaboration; (3) the group-level, in which people share common identities and goals to attain; and (4) the cultural level, where the analysis focus on macro-elements such as history and tradition. An overarching synthesis on the social functions of emotions across all levels is proposed by Fischer and Manstead [-@fischerSocialFunctionsEmotion2016], who identify two complementary, but distinct, functions performed by emotions: affiliation and distancing. The affiliation function serves to form and maintain positive social relationship with others, whereas the distancing function helps in establishing and maintaining a social position relative to others (*ibid.*). Other social functions of emotion comprise the social sharing of emotion [@rimeEmotionElicitsSocial2009; @rimePartageSocialEmotions2005], the use of emotional information to better understand behavior and appropriate conduct in social situations [@parkinsonEmotionsInterpersonalInteractions2010; @vankleefHowEmotionsRegulate2009; @hareliEmotionsSignalsNormative2013; @hareliWhatEmotionalReactions2010],

Van Kleef [-@vankleefEmergingViewEmotion2010; -@vankleefHowEmotionsRegulate2009; -@vankleefInterpersonalDynamicsEmotion2018] proposes on overarching framework, the Emotion As Social Information (EASI) model, which attempts at integrating the many inter-personal functions of emotion in terms of two classes of mutually influential, but conceptually distinct and empirically separable mechanisms:

1.  *Emotional expressions trigger affective reactions in observers*. The first mechanism implies that the emotion of a person may be the eliciting stimulus of an emotion in the observer. The affective reaction in the observer may be the same as the one expressed by the person, as in the case in emotional contagion [@barsadeRippleEffectsEmotional2002] or empathy [@bloomEmpathyItsDiscontents2016; @wondraAppraisalTheoryEmpathy2015]. For instance, boredom may propagate between learners as the result of the first person pausing and puffing. At the same time, the emotion in one person can trigger a different affective reaction in the observer, but independently of inferential processes about the situation. For instance, the amusement of a colleague during a collaborative task may trigger anger in the observer who wants to stay focused, independently of the reasons why the colleague is amused. Whether the observer affective reaction mirrors or complements that of the person who has expressed it, the reaction will have consequences on the observer's behavior (*e.g.*, deciding to ignore the amused colleague) and/or both the observer and the person who expressed the *original* emotion (*e.g.*, both learners decide to take a break).
2.  *Emotional expressions elicit inferential processes in observers*. The second mechanism implies a more complex and deliberate act of information-processing by which the observer attributes to the expressed emotion potential causes of and consequences on behavior. Especially in appraisal theories of emotion [@moorsAppraisalTheoriesEmotion2013; @moorsFlavorsAppraisalTheories2014; @rosemanAppraisalTheoryOverview2001], which will be more thoroughly depicted in the next chapter, the evaluation a person makes of the situation is pivotal in determining what emotion she may feel. By a form of *reverse engineering* [@hareliWhatEmotionalReactions2010; @schererFacialExpressionsAllow2007], the specific emotion a person is feeling may be used to infer how that emotion came to be elicited.

The EASI model (*ibid.*) also specify two preconditions and two moderator factors that determine the inter-personal dynamics of emotion. The preconditions concern (1) the ability of the person experiencing the emotion to *encode* it in a form that can be communicated to the observer, which broadly refers to the concept of *emotion expressivity* [@kringIndividualDifferencesDispositional1994; @scarantinoHowThingsEmotional2017]; and (2) the observer's ability to *decode* the emotional information and make sense from it using emotional knowledge and understanding, which closely relates to the principle of socio-emotional competences depicted in the previous chapter [@brackettRULERTheoryDrivenSystemic2019; @chernissEmotionalIntelligenceClarification2010; @matthewsScienceEmotionalIntelligence2007]. The two preconditions reflect the displaying and the monitoring functions of awareness tools depicted in the previous chapter [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. As stated by Van Kleef [-@vankleefInterpersonalDynamicsEmotion2018]:

> No matter how informative emotions may be, and however critical their social-regulatory functions, if emotions are not expressed and/or fail to be perceived by others, their signaling function is evidently lost and social interaction may be jeopardized -- @vankleefInterpersonalDynamicsEmotion2018, p.52

The EASI model also proposes two classes of moderator factors that may influence to what extent an expressed emotion may result in an affective reaction and/or an inferential process by the observer (*ibid*.). For instance, it may be the case that the *confusion* expressed by a colleague may trigger an affective reaction of *anger* in the observer, because this slows down the collaboration. On the other hand, the same expression may also push the observer to understand the causes of the colleague's *confusion*, for then proposing some form of learning co-regulation. According to the EASI model, the engagement of the observer in deliberate and more cognitively demanding inferential processes may be facilitated by (1) the observer's motivation and capacity to allocate resources to make inferences about causes and consequences on the sender's behavior; and (2) the perceived *appropriateness* of the expressed emotions according to the observer's criteria of evaluation, for instance in relation to social norms [@fischerSocialFunctionsEmotion2016; @hareliWhatEmotionalReactions2010]. In other words, the observer may try to understand the colleague's confusion if (a) the observer has some *free* resources to dedicate, which may be difficult if the learning context is demanding, and (b) the observer evaluates confusion as an appropriate reaction to the situation, for instance by reckoning that the learning task is difficult.

The EASI theory (*ibid.*) provides an integrative framework of the social-function of emotion, which is founded on the assumption that one's own emotions may guide thoughts, actions and feelings in another person. Using emotion as social information may, for instance, be a first step in the social regulation of emotion, that is, the attempt to voluntary act upon other people's emotion [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013]. According to Reecks and colleagues [-@reeckSocialRegulationEmotion2016, p.48] , "[t]he goal-driven nature of social regulation distinguishes it from related phenomena, such as social sharing, empathy, or emotional contagion, where one person's actions are not strategically directed towards influencing another's emotions". The authors propose a cross-disciplinary model that implements the process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], illustrated in the intra-personal section, from an inter-personal perspective. In what they call the Social Regulatory Cycle, depicted in Figure \@ref(fig:tf-social-regulation-model), the authors identify the regulator and the target person upon which the emotion regulation is intended. The regulator must proceed, first, by identifying the target emotion. Second, the regulator must assess whether the identified emotion corresponds to a suitable emotion or not. Third, in case of a discrepancy between the two, the regulator must plan a strategy aiming at producing in the target the suitable emotion. Last, this strategy must be implemented. At this point, the strategy may target a different stage of the process in the target (see the description of the individual model above for more details): the selection or modification of the situation; the orientation of the target's attention; the possibility of changing the way the target appraises the situation; or the modulation of the behavioral, physiological and experiential manifestation of the emotion. The cycle may start again either from an individual standpoint (*e.g.*, the target is dissatisfied with the new emotion induced by the regulator) or inter-individual perspective (*e.g.*, the regulator did not obtain the suitable emotion or identify an even more adapted emotion for the target).

(ref:tf-social-regulation-model-caption) The Social Regulatory Cycle at the inter-personal level, fromm the original Figure 2 in @reeckSocialRegulationEmotion2016, p. 51.

```{r tf-social-regulation-model, fig.align="center", out.width="80%", fig.cap="(ref:tf-social-regulation-model-caption)"}
knitr::include_graphics(here("./figure/theory/social-regulation-emotion.png"))
```

TO SUM UP

### Related Works

Eligio, Ainsworth and Crook [-@eligioEmotionUnderstandingPerformance2012] carried out two intertwined experiments aiming at exploring "what collaborators understand about each other's emotions and the implications of sharing information about them" (*ibid*, p. 2046). In the first experiment, the authors asked pairs of same-sex, unacquainted participants to play a collaborative game in a co-located environment, where they shared the same computer equipment. At the end of the collaboration, participants were asked to fill in two questionnaires where they had to rate the intensity of 15 emotions: *happy*, *angry*, *sad*, *fearful*, *angry*, *bored*, *challenged*, *interested*, *hopeful*, *frustrated*, *contempt*, *disgusted*, *surprised*, *proud*, *ashamed* and *guilty*. In the *own* version of the questionnaire, they rated the intensity of each emotion as they have perceived it during the task. In the *partner* version of the questionnaire, participants had to project the intensity by which their partner in the dyad had experienced each of the listed emotion. The administration of the questionnaire was made individually, so that participants could not discuss the matter with their partner, and up until that moment, participants were not informed of the rating, so that they had no particular reason to pay attention neither to their own, nor to their partners' emotions. By comparing the responses to the two questionnaires, the authors concluded that, despite collaborating side-by-side, participants had little understanding of their partner's emotions. On the contrary, consistently with previous findings in the literature [@kruegerTrulyFalseConsensus1994; @tomaAnticipatedCooperationVs2010], participants tended to project their own emotional experience onto their partners. Eligio and colleagues (*ibid*) provides two possible explanations of these results. On the one hand, participants could simply not care about the emotions of their partner. On the other hand, participants could genuinely care about them, but lack the means to focus on them, for instance because the task was too demanding, and they decided to prioritize other mental states perceived as *more* instrumental to the task at hand. In both cases, the attribution of their own emotional experiences to that of the partner is caused by a lack of information available, but the reasons for this shortcoming are not the same depending on the intentions.

In the second experiment, Eligio and colleagues (*ibid*) therefore decided to intervene by providing (or not) participants with explicit awareness of their partner's emotions using a *scripting* strategy, that is, by interrupting the collaboration at specific moments for participants to express their emotions and projecting that of their partner. After filling the *own* and *partner* questionnaire, if participants disposed of awareness, they could look at the emotions of their partner's before resuming the task, otherwise they just continued with the collaboration. Furthermore, dyads were also assigned either in a co-located or a remote collaborative setting, resulting in a 2x2 factorial design with awareness vs. no-awareness, and co-located vs. remote conditions. Using experimental settings similar to the first experiment (but with a slightly different collaborative task and with only women as participants), the authors report evidence suggesting that participants benefited of emotional awareness in terms of performance both in the co-located and remote conditions. Furthermore, participants in the remote condition were more accurate in understanding emotions of their partner and also experienced more *positive affect*, computed by averaging the intensity of *happy*, *interested*, *hopeful*, *excited* and *challenged*.

From the two experiments, Eligio and colleagues (*ibid.*) concluded that participants do not have an accurate understanding of the emotion of their partner if their attention is not explicitly driven to it. Providing emotional awareness seems thus a promising way to increase mutual understanding, since participants with emotional awareness showed higher accuracy in estimating their partner's emotions, as well as better performance to the task at hand.

Following Eligio and colleagues (*ibid.*) findings, Molinari and colleagues [@molinariEmotionFeedbackComputermediated2013] conducted a study in which 30 dyads of same-sex participants (16 dyads of women, 14 dyads of men) performed a collaborative task in remote conditions, with the possibility of audio but not video connection. The aim of the collaborative task was to conceive a slogan against violence in schools using an argument graphic tool [@lundHowArgumentationDiagrams2007]. The task therefore implied some form of negotiation for deciding the final outcome of the collaboration, which was intended to solicit socio-cognitive tensions [@andriessenSociocognitiveTensionCollaborative2011] in participants as a means to elicit emotions. Half of the dyads were randomly assigned to a control condition, in which participants did not dispose of emotional awareness. The other half of the dyads were provided with emotional awareness through the use of a persistent EAT available on the right-side of the screen, with the argument graphic tool occupying the left-hand side of the screen (see Figure \@ref(fig:tf-eatmint-eat)). Contrary to Eligio and colleagues (*ibid*.), in which participants shared emotions at specific moments during the task following a *scripting* strategy, in this case the setting was congruent with an *awareness* strategy, since participants could express and have access to their partner's emotions at any time during the task. A message also showed up after 5 minutes from the last expressed emotion to remind participants to express how they were feeling.

(ref:tf-eatmint-eat-caption) The argument graphic tool and the EAT adopted in @molinariEmotionFeedbackComputermediated2013, from Figure 1 in the original article.

```{r tf-eatmint-eat, fig.align="center", fig.cap="(ref:tf-eatmint-eat-caption)", out.width="80%"}
knitr::include_graphics(here("figure/theory/eatmint-eat-and-task.png"))
```

As shown in Figure \@ref(fig:tf-eatmint-eat), the EAT was horizontally divided in two areas. On top, the monitoring-perceiving area consisted in the last three discrete emotions expressed by the participant (green boxes) and the partner (blue boxes). The box on top of each pile, representing the very last emotion expressed, was highlighted with a paler color. The lighter green box was also editable by the participant, who could write in an emotion not available through the buttons in the lower part of the screen. These buttons represent the displaying-expressing function and are organized in two columns: the right one with 10 *positive* emotions, and the left side with 10 *negative* emotions. According to a private communication with the authors, the list of discrete emotions were defined mixing a previous study in the field [@dmelloDynamicsAffectiveStates2012] and 2 pre-experiments that aimed to identify the most frequent and intense emotions felt during a situation of collaboration, real or imagined. The EAT adopted by Molinari and colleagues (*ibid*.) will be further discussed in Chapter 4.

At the end of the collaborative task, participants individually filled-in a questionnaire aimed at gathering information about (1) the kind and intensity of the emotions of the participant at the end of the task, as well as the emotions the participant attributed to the partner, particularly with respect to the 20 discrete emotions available as buttons on the EAT; and (2) the quality of the perceived interaction based on the frequency by which the participant and the partner provided/imposed their own points of view, defended and argued their ideas, understood their partner's points of view, built up on their partner's ideas, as well as managed emotion during interaction. As for Eligio and colleagues (*ibid.*), also Molinari and colleagues (*ibid*.) report that their result support the hypothesis of beneficial effect from the presence of an EAT, but only in dyads of women and with the presence of mixed results with respect to their initial hypothesis.

The tests upon which Eligio and colleagues (*ibid*.) and Molinari and colleagues' (*ibid*.) result are based, though, do consider the hierarchical structure of the dyads, which may inflate the rate of type I error since the non-independence of observations is not taken into account [@brownIntroductionLinearMixedEffects2021; @singmannIntroductionMixedModels2020; @westLinearMixedModels2015]. As a consequence, the evidence provided by the two studies should be taken with caution. The experimental settings, on the other hand, are interesting and directly compare a *scripting* and an *awareness* perspective, with the latter presenting the advantage of persistent emotional awareness. The two studies also highlight the many methodological challenges in determining the effect of an EAT in computer-mediated learning environments, which concern how emotional awareness is provided, but also how the potential benefit are measured and analyzed.

A different approach was taken by Avry and colleagues [-@avryAchievementAppraisalsEmotions2020], who, rather than providing awareness through discrete emotions, took a dimensional approach adopting Pekrun's Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006] introduced in Chapter 1. In their study, the authors asked 28 dyads of same-sex participants to play a remote collaborative game. Beside the game window, participants disposed of smaller window on the screen which reported two feedbacks: (1) a feedback on how well the dyds was mastering the game, representing the Control dimension; and (2) how well they were performing in a standing compared to other dyads, representing the Value dimension. The two feedbacks were manipulated by the authors as to create a 2x2 factorial design combining high vs. low Control on the one hand, and high vs. low Value on the other. After the collaborative task, participants filled in a questionnaire on which they evaluated the overall collaboration in terms of (a) affective dimensions (Valence, Dominance, and Activation) and 16 discrete achievement emotions, and (b) six computer-supported collaborative exchanges (sustaining mutual understanding, information pooling, transactivity, reaching consensus, task management, and time management). For both category of measures, the rating was performed for the participant herself, as well as what the participant thought her partner would have experienced. The results obtained by Avry and colleagues (*ibid.*), who took into account the hierarchical structure of dyds by checking the intraclass correlation of dyads, corroborate an effect of the manipulation of the Control-Value appraisals on both category of measures (affective and socio-cognitive). These results are particularly interesting considering that in this study, a form of emotional awareness was (1) manipulated, and (2) conveyed through a feedback aimed at eliciting a certain kind of appraisal of the situation, which influenced the kind of discrete achievement emotions experienced.

The three studies outlined in this section provide very different approaches to the study of emotional awareness in computer-mediated learning environments. Since this is a recent and cross-disciplinary field of inquiry, a fragmented stage of research is inevitable [@fiedlerCycleTheoryFormation2004]. On the other hand, efforts should also be dedicated to the possibility of direct comparison and incremental building of knowledge. In this regard, the adoption of a prototypical EAT, whose eventual differences can be more easily defined through objective parameters, could be beneficial.

## Emotion in Computer-Mediated Learning Environments

The third assumption underlying emotion awareness tools concerns the possibility to create emotional awareness in computer-mediated learning environments.

### Theoretical Underpinnings

For emotional awareness to emerge and be available, it is necessary that emotion may be fruitfully *encoded* and *decoded* in a computer-mediated learning environment. In this regard, the computer-mediated environment is somehow ambivalent with respect to affect-related phenomena. On the one hand, the combined presence of a learning task and a technological device captures learner's attention and thus, even in the case of co-located interaction or seamless audio/video connection in remote settings, diminishes the possibility to pay attention to efferent cues of affective experiences -- such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009] -- that are more readily available in face-to-face interaction [@baltesComputerMediatedCommunicationGroup2002; @lundHowArgumentationDiagrams2007]. On the other hand, the same presence of a technological device provides alternative or complementary means to create emotional awareness and even extend it over time [@derksRoleEmotionComputermediated2008; @gliksonDarkSideSmiley2018; @hegartyCognitiveScienceVisualspatial2011; @leonyProvisionAwarenessLearners2013; @derickEvaluatingEmotionVisualizations2017].

Derks, Fisher and Bos [-@derksRoleEmotionComputermediated2008], for instance, conducted a review of studies from social psychology with the aim to investigate whether emotion is communicated differently in computer-mediated communication compared to face-to-face settings. The authors took into account a reduced social presence (see Chapter 1) in the expression of emotion by the sender -- even though they attributed it mostly to the potential anonymity of users -- and a reduced visibility of non-verbal cues that may impact emotion recognition in the receiver [@scarantinoHowThingsEmotional2017]. Even considering these factors, though, the conclusion of the review suggests that people feel the need to express their emotions in computer-mediated settings in a very similar way compared to face-to-face interactions, and they also tend to do so more frequently and more explicitly (*ibid.*). Even when the communication is only text-based, in fact, people tend to find ways to convey emotional information, for instance through smileys or emoticons [@blundenEmoticonAreThere2020; @gliksonDarkSideSmiley2018]. Cheshin, Rafaeli and Bos [-@cheshinAngerHappinessVirtual2011] found evidence in an experiment conducted in virtual teams suggesting that emotional contagion [@barsadeRippleEffectsEmotional2002; @parkinsonInterpersonalEmotionTransfer2011] from anger and happiness can happen also when communication is only text-based. 

This phenomenon may be explained by a corollary to the EASI model, illustrated in the previous section, which Van Kleef [-@vankleefSocialEffectsEmotions2017] identifies as the *functional equivalence hypothesis*. The hypothesis posits that the role emotion play at the inter-personal level is equivalent regardless of the specific way it is communicated, as long as the emotional information passes from the sender to the receiver. In the words of the author:

> If one accepts the notion that emotional expressions can influence social interactions by providing information about what is on the expresser's mind, it follows that emotions can have such effects regardless of how they are expressed, as long as the expressions convey the relevant information. Consequently, EASI theory posits that expressions of the same emotion that are emitted via different expressive modalities (i.e., in the face, through the voice, by means of bodily postures, with words, or via symbols such as emoticons) have comparable effects, provided that the emotional expressions can be perceived by others --- @vankleefSocialEffectsEmotions2017, p. 213

On the other hand, the fact that various forms of emotional expression are equivalent with respect to their social function does not mean that each way of expressing an emotion is equivalent [@vankleefInterpersonalDynamicsEmotion2018]. A colleague's anger expressed by shouting on your face how badly your part of a collaborative task has been handled has not the same intensity of *I am very angry with you* written in an email.

A diminished *veracity*, though, may be compensated by more favorable conditions to engage in inferential processes rather than affective reactions as described by the EASI model (*ibid.*). As stated in Chapter 1 for awareness tools more generally, even in the case of emotional awareness, face-to-face interaction may not be the golden standard to aim for [@buderGroupAwarenessTools2011; @boehnerHowEmotionMade2007]. An EAT whose aim is to make learners' aware of their own and/or their colleague's emotions must therefore face the challenges of *encoding* and *decoding* emotional information, so that it maximized the preservation of its *functional* meaning.

In this regard, the *encoding* of emotional information has received so far greater attention than *decoding*, since it is tightly related to the action of detecting, representing or measuring emotions [@fuentesSystematicLiteratureReview2017; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @silvaComparativeStudyUsers2020]. In their review of technologies for emotion-enhanced interaction already cited in Chapter 1, Cernea and Kern [-@cerneaSurveyTechnologiesRise2015] distinguishes between three types of techniques commonly adopted to estimate emotions: (1) perception-based estimation, derived from efferent manifestations of emotion such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009; @martinezContributionsFacialExpressions2016]; (2) physiological estimation, based on the detection of physiological patterns such as heart rate, blood pressure, or skin conductance [@ragotEmotionRecognitionUsing2018; @shuReviewEmotionRecognition2018]; and (3) subjective feelings, based on the person's self-report of her own emotional experience [@lavoueEmotionalDataCollection2017; @ritchieEvolutionSelfreportingMethods2016; @silvaComparativeStudyUsers2020]. As stated in the introduction, the thesis focuses on this last category. In fact, physiological estimation requires dedicated hardware and software, which would be difficult to provide at scale. Perception-based estimation may use more widely available hardware and software, for instance through a webcam or keyboard stroke, but the accuracy and usefulness of this kind of measure is still lively debated in the literature [@barrettEmotionalExpressionsReconsidered2019; @bahreiniMultimodalEmotionRecognition2016; @nahinIdentifyingEmotionKeystroke2014]. Barrett and colleagues [-@barrettEmotionalExpressionsReconsidered2019], for instance, argue that automatic recognition from facial expression are still limited in reliability, lack in specificity, and does not take sufficiently into account effects of context and culture. Voluntary self-report is therefore retained as the most parsimonious, portable and reliable way to provide emotional awareness both at the intra-personal and inter-personal levels, especially when the aim is to stimulate voluntary emotional introspection and/or inferential processes [@boehnerHowEmotionMade2007; @vankleefInterpersonalDynamicsEmotion2018].

Emotional self-report is tightly related to the underlying concept of *what an emotion is*, which will be discussed in Chapter 3. From a technical standpoint, though, emotion self-report is traditionally characterized by the dimensional or the discrete emotion approaches, and less frequently by a combination of the two [@bradleyMeasuringEmotionSelfAssessment1994; @cowenSelfreportCaptures272017; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005; ].

Emotional *decoding* in a computer-mediated environment is tightly linked on how emotion is *encoded* and has received so far limited attention [@bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Furthermore, visualization of emotion in this context is usually derived from affective information available in students' productions or communication exchanges, for instance through sentiment analysis rather than a dedicated tool [@leonyProvisionAwarenessLearners2013]. In the absence of specific representations for emotional awareness, more general guidelines about the visuo-spatial representation of data should be applied [@hegartyCognitiveScienceVisualspatial2011; @hehmanDoingBetterData2021]. Emotional *decoding* is also influenced by the objective of the visualization. Learners' emotion may be represented individually or collectively, as a single unit in time or grouped through short or long time spans. They may also be partially or fully available to all other students or only to a selection of colleagues (e.g. in a group work). In other words, if the aim of emotional *encoding* is rather straightforward, emotional *decoding* depends on a larger number of factors.

### Related Works

## Abstract Model of the Functions of an Emotion Awareness Tool

The information illustrated above, as well as in the previous chapter, can be integrated in an abstract model that depicts the mechanisms allegedly carried out by an EAT. The proposed abstract model does not have the pretension to establish an absolute reference; on the contrary, it aims at visually implementing the different passages that are (or may be) assumed in the instrumentality of an EAT as a support to learning activities in computer-mediated environments. The model, depicted in Figure \@ref(fig:intro-thesis-scm-model-figure), take the perspective of a learner that disposes of an EAT in its computer-mediated learning environment. The model comprises boxes for four conceptual elements: (a) the learning activity, which may refer broadly to any computer-mediated environment implementing an instructional design; (b) intra-personal emotion, representing a single event or a set of events corresponding to the learners' expressed-displayed emotions; (c) inter-personal emotion, representing a single event or set of events corresponding to emotions expressed-displayed by other learners sharing the same environment, that the learner herself can therefore perceive-monitor; and (d) an overarching *meaning-making* process, which encompasses learner's effort to extrapolate instrumental information from the emotional information available. The boxes are connected with directional arrows, numbered from 1 to 7, representing a series of processes that are (or may be) implicated in each passage. The numbers are used for identifying the passages, but do not imply a fixed order, even though some processes may be built upon previous passages. The next sections provides a discussion of each passage (*i.e.*, each numbered arrow) more specifically.

(ref:thesis-scm-caption) The EAT determines how an intra-personal emotion becomes inter-personal, and how the inter-personal emotions contribute to improve the shared understanding of the situation at hand. The situation may then trigger other intra-personal emotions.

```{r intro-thesis-scm-model-figure, fig.cap="(ref:thesis-scm-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/intro/thesis-scm-model.png"))
```

### From the Learning Activity to Intra-Personal Emotion

Passage #1 goes from the learning activity to an intra-personal emotion, implicating that the learning activity elicits emotions in oneself. An EAT may intervene in this passage in several ways, grouped here in a non exhaustive lists of X phenomena labeled for illustration purpose as *emotion alertness* and *emotion conceptualization*.

Emotion alertness broadly refers to the fact that the presence of the EAT represents a general reminder about paying attention to emotion during the learning activity, and therefore may increase the *sensitivity* about emotional experience. In other words, the presence of the EAT may push learners to bestow to emotional self-awareness more attention that they would normally do without the presence of the EAT [@brackettRULERTheoryDrivenSystemic2019; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Emotion alertness may be induced to different degrees, depending on, for instance, whether the EAT is always available or not, and whether explicit prompts are frequently sent to learners [@csikszentmihalyiValidityReliabilityExperienceSampling2014; @shiffmanEcologicalMomentaryAssessment2008]. As an example, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented an EAT into a computer-mediated collaborative task that reminded participants to self-report their emotions after five minutes since the last emotion expressed.

Emotion alertness is not necessarily and automatically linked to concrete action of emotion display, that is, self-reporting the emotion into the system. In fact, even in the case of an increased alertness, learners could genuinely not feel any emotion, or decide not to express their emotion through the EAT for dispositional (*e.g.*, emotional expressivity, see @kringIndividualDifferencesDispositional1994) or contingent reasons (*e.g.*, bestow priority to the learning task at hand).

Emotion conceptualization broadly refers to the mechanisms by which learners are planned to self-report their emotions, which may vary greatly according to a number of theoretical and technical aspects. For instance, emotion self-report tools or questionnaires are very often divided in two categories, depending on whether they adopt a dimensional or a discrete perspective [@broekensAffectButtonMethodReliable2013; @fuentesSystematicLiteratureReview2017; @ritchieEvolutionSelfreportingMethods2016; @schererWhatAreEmotions2005].

In a dimensional approach, an emotion is conceptualized as a rating on a number of continuous criteria, among which the most frequently adopted are *valence* (also called *pleasure*) and *arousal* (also called *activation*) [*e.g.*, @russellCircumplexModelAffect1980; @stanleyTwodimensionalAffectiveSpace2009], but may also comprise different or additional dimensions. It is the case, for instance, of the Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] or the AffectButton [@broekensAffectButtonMethodReliable2013], which both use the *valence*, *arousal*, and *dominance* dimensions, even if in different formats. The Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] uses three rows of figures (*i.e.*, the *manikin*), where each rows represent one of the three dimensions. For each row, 5 figures, progressively modified in some features, represent the increasing or decreasing value on the specific dimension. The AffectButton [@broekensAffectButtonMethodReliable2013], on the other hand, uses a single iconic facial expression that changes according to the user's coordinates of the mouse on the surface of the icon: the horizontal movement determines the pleasure dimensions; the vertical movement the dominance dimension; and the arousal dimension is calculated according to the distance of the mouse from the central point of the image.

In a discrete approach, an emotion is conceptualized as a distinctive experience that can be identified through a verbal and/or graphical representation [@mortillaroEmotionsMethodsAssessment2015]. The number, kind and organization of the representations varies depending on several aspects. In the case of verbal representations that adopts natural language nouns or adjectives, for example, the list may be determined according to emotion theories -- as in the case of basic emotion theory [@ekmanArgumentBasicEmotions1992] -- or determined empirically according to previous (or pilot) studies, often with the aim of retaining a list of the most frequently experienced or expressed items. For instance, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented in a self-report interface 20 buttons, 10 labeled with negative and 10 with positive emotion adjectives retrieved among the most frequently expressed during a computer-mediated collaborative task in a pilot study. The interface also provided -- as it is considered a good practice [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005] -- the possibility to provide another emotion not in the list. When emotion is conceptualized graphically, representations usually attempt at maintaining some degree of analogy with *reality*, for instance with respect to facial expressions [@desmet2003measuring; @lauransNewDirectionsNonVerbal2012].

Less frequently, self-report tools attempt to combine the two approaches, as it is the case in the Geneva Emotion Wheel [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005], depicted in more details in the next chapter, or the Mood Meter application associated with the RULER approach [@brackettRULERTheoryDrivenSystemic2019] mentioned in the previous chapter. When the dimensional and discrete approaches coalesces, they create an underlying structure, which is often referred to as an *affective space* [*e.g.*, @davidsonParsingAffectiveSpace19940101; @gilliozMappingEmotionTerms2016; @schererWhatDeterminesFeeling2006; @stanleyTwodimensionalAffectiveSpace2009]

The Mood Meter application ...

The emot-control system implemented by Feidakis and colleagues [-@feidakisProvidingEmotionAwareness2014] shows on the same interface (a) a circumplex of 14 discrete affective states, represented by stylized, and colored faces similar to smileys/emoticons, organized according to the valence x activation orthogonal axis; (b) 5 additional images of similar kind, but representing moods in a sort of 5-point Likert scale (*sad*, *unhappy*, *neutral*, *happy*, and *very happy*); (c) a free text input to type-in another emotion not listed; and (d) another free text input introduced by the field legend *Write more about your feelings*.

Depending on how emotion is conceptualized and, by extension, planned for self-report, the intra-personal awareness of emotion may be more or less guided, inducing learners to pay attention to some elements of their emotional experience that they will not necessarily consider in the same way, or with the same weight, depending on the specifics of the EAT at hand. At the same time, emotion self-report also depends on the learner herself, for instance in terms of her ability to discriminate between emotion ... ...

### From Intra-Personal Emotion to Meaning-Making

Once an emotion or a set of emotions are self-reported, it is assumed that learners may extrapolate meaning from the emotional information available. In this regard, information may be punctual, for example the last emotion or set of emotions displayed concurrently. Conversely, the system may also implement some form of data persistence, which allows to observe the accumulation and the evolution of the emotional experience over a varying time span. Depending on the way emotion information is also graphically represented, emotional meaning-making can be guided by the system, which may privilege some form of representation (*e.g.*, changes over time) over another (*e.g.*, cumulative frequency of the same emotion expressed).

Meaning-making is also tightly related to the way emotion is displayed through the system (passage #1), not only from a technical standpoint -- since the information available is determined by what information is inserted into the system -- but also conceptually. If emotion is considered from a dimensional standpoint, learners will extrapolate meaning from the extant criteria through which dimensions are rated. On the contrary, in a discrete emotion approach, learners extrapolate meaning from an *information unity* that is supposed to provide unique meaning compared to other possible choices. Finally, if the system combines dimensional and discrete approach, learners can extrapolate meaning from both.

The system may also influence meaning-making by adding contextual information to the way emotion is displayed. For instance,

### From the Learning Activity to Inter-Personal Emotion

Passage #3 starts a complementary path with respect to passages #1 and #2, but from an inter-personal perspective, that is, concerning emotion in others learners sharing the same computer-mediated learning environment with the learner herself. This path may in fact be optional when the EAT is exclusively concerned with emotion self-awareness [*e.g.*, @lavoueEmotionAwarenessTools2020], whereas it becomes an integral part when the EAT is inspired from a Group Awareness Tool perspective [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @feidakisProvidingEmotionAwareness2014; @molinariEmotionFeedbackComputermediated2013].

### From Inter-Personal Emotion to Meaning-Making

Passage #4 consists in extrapolating meaning from the emotion of other learners sharing the same computer-mediated learning environment and whose emotions have been monitored by the learner herself.

### From Intra-Personal Emotion to Inter-Personal Emotion

Expression of emotion: emotion is made available to others depending on learners willingness to disclose their emotional experience.

Emotion influence.

Emotion sharing.

### From Inter-Personal Emotion to Intra-Personal Emotion

Emotion comparison (e.g. affiliation or differentiation). Emotion contagion.

### From Meaning-Making to the Learning Activity

Emotion is integrated into the learning activity, it modifies behavior (self-regulation, co-regulation, socially shared regulation).

Modification of learning activity can trigger another emotion, creating cycle.

## Methodological Issues

For instance, Pekrun and Linnerbrink-Garcia [@pekrunIntroductionEmotionsEducation2014, p. 2] point out that "[i]n the broader education literature, affect is often used to denote a broad variety of noncognitive constructs including emotion but also including self-concept, beliefs, motivation, and so on [...]. In contrast, in emotion research, affect refers to emotions and moods more specifically".

## Synthesis
