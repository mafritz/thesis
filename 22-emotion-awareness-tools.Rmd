---
bibliography: references.bib
---

# Emotional Awarenss and Emotional Awareness Tools in Computer-Mediated Learning Environments {#eat-general-chapter}

```{r rationale-eat-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

After a general overview about the relationship between affect and learning, this chapter focuses on emotional awareness and on Emotion Awareness Tools (EAT) more specifically. First, the concept of emotional awareness is defined within the context of computer-mediated learning environments. Second, three main assumptions are derived by this definition. The theoretical underpinnings of each assumption are set forth, complemented by the overview of related empirical works. In the second part of the chapter, an abstract model of an EAT is laid out in an attempt to frame the different interactions between the learners and an EAT, for emotional awareness to be instrumental. The model also allows to highlight how the instrumentality of an EAT is far from being trivial and rather requires a complex interplay between learners expertise and willingness in taking emotional awareness into account on the one hand, and features of the EAT that can sustain or facilitate the process. Finally, it is suggested that the abstract model can be used as a way to direct research in emotional awareness, by pointing out which part of the model the research aims at investigating (*e.g.*, the whole or some specific parts).

## Definition of Emotional Awareness in Computer-Mediated Learning Environments

Emotional awareness in computer-mediated learning environments is often proposed in the literature as self-explaining or defined somehow implicitly. For instance, as stated in the previous chapter, Feidakis and colleagues [-@feidakisReviewEmotionAwareSystems2016, p. 217] define emotion awareness as "the *implicit* or *explicit collection of emotion data* and the *recognition* of *emotion patterns*" (italics in the text). Cernea and colleagues [-@cerneaSurveyTechnologiesRise2015], in their overview of rising technologies in emotion-enhanced interaction, identify the category of affective-aware systems as "designed to improve affective self-awareness of a user (i.e.,internal affective awareness) or awareness of the emotions of other users (i.e.,external affective awareness)" (*ibid.*, p.78). More precise definitions are provided by Lavoué and colleagues [-@lavoueEmotionAwarenessTools2020], who partially derive them from clinical psychology. The first definition provided by Lavoué and colleagues is based upon @bodenFacetsEmotionalAwareness2015 and posits that emotional awarenss is "the ability to perceive, identify, and understand emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. Later in their article, Lavoué and colleagues (*ibid.*) propose another definition, based upon Rieffe and colleagues [-@rieffeEmotionAwarenessInternalising2008], referring to emotional awareness as "the attentional process by which individuals identify, explain and differentiate between their own emotions as well as the others' emotions" [@lavoueEmotionAwarenessTools2020, p. 270]. These two definitions resonate with socio-emotional competences briefly illustrated in the previous chapter. Finally, later in the same article, Lavoué and colleagues (*ibid.*) provide a more technical definition of emotion awareness tools as "tools that display information on own' own [sic] or partners' emotions, circumstances and antecedents" [@lavoueEmotionAwarenessTools2020, p. 284]. Compared to the more abstract definition in Feidakis and colleagues or in Cernea and colleagues (*ibid.*), the definition in Lavoué and colleagues (*ibid.*) implies that the tool shall also provide information about circumstances and antecedents that concurred in eliciting the emotional episode.

From these definitions it is possible to extrapolate three fundamental assumptions about emotional awareness and, by extension, the instrumentality of an EAT in a computer-mediated learning environment:

1.  Learners may benefit from intra-personal emotional awareness by using information about their own emotions as valuable information for their own learning processes (self-regulation).
2.  Learners may benefit from inter-personal emotional awareness by using information about their partners' emotions and/or by communicating their own emotions to their partners and/or by a combination of the two. One or all of these circumstances may contribute to learners' own learning processes (self-regulation), the learning processes of their partners individually (co-regulation), or the learning processes of the group as a whole (socially shared regulation).
3.  Learners may benefit from emotional awareness conveyed by a dedicated tool, which *encodes* emotional information into the computer-mediated learning environment, and *decodes* that information, for learners to extrapolate emotional meaning-making instrumental to learning.

Each assumption is discussed in the following three sections. Every section proposes an overview of related theories, as well a selection of empirical works, which are of particular interest for the present contribution in terms of objectives, methodologies, or findings.

## Emotional Awareness at the Intra-Personal Level {#ea-intra-personal}

The first assumption about the instrumentality of emotional awareness implies that learners can benefit from it at the individual, intra-personal level.

### Theoretical Underpinnings

The role of emotion at the intra-personal level has received over the last few decades extensive consideration from different perspectives and using different methods of investigation. A widespread consensus has emerged over the years about the fact that emotion, rather then disruptive of behavior and in antithesis with cognition, plays a prominent role in helping the organism to cope with a complex environment [@adolphsNeuroscienceEmotionNew2018; @armonyCambridgeHandbookHuman2013; @damasioDescartesError2006; @damasioStrangeOrderThings2018; @schererEmotionTheoriesConcepts2009; @immordino-yangEmotionsLearningBrain2016; @levensonAutonomicNervousSystem2014; @levensonIntrapersonalFunctionsEmotion1999; @leventhalRelationshipEmotionCognition1987; @pessoaCognitiveemotionalBrainInteractions2013; @sanderModelsEmotionAffective2013].

For instance, emotion is known to influence high-level cognitive functions such as attention, perception, memory, and decision-making [@broschImpactEmotionPerception2013], which are all implicated in learning processes. For instance, it is posited that an emotional attention system may complement and interact with the exogenous and endogenous systems [@broschAdditiveEffectsEmotional2011]. Emotionally charged stimuli are also known to often bestow precedence in perception over non emotionally-charged stimuli [@broschPerceptionCategorisationEmotional2010]. Many links between emotion and memory have been established both in the encoding and retrieving of information [@sharotHowArousalModulates2004; @kensingerRetrievalEmotionalEvents2020], which may have important consequences for learning [@beegeMoodaffectCongruencyExploring2018; @tyngInfluencesEmotionLearning2017]. Finally, it has been determined that when emotion is not taken into account in decision-making, consequences can be highly disruptive for the person [@becharaRoleEmotionDecisionmaking2004; @rollsEmotionDecisionmakingExplained2014].

Emotional awareness, though, goes a step forward in considering the importance of emotion at the intra-personal level, because it presupposes that learners can benefit from being aware of their own emotions [@lavoueEmotionAwarenessTools2020]. Whether consciousness is a necessary condition for emotion to *exist* [@ledouxHigherorderTheoryEmotional2017; @liebermanBooConsciousnessProblem2019] is a debate outside the scope of the present contribution (see also next chapter): the use of voluntary self-report requires consciousness for emotional awareness to emerge in the first place and therefore *dodges* the issue. Conscious processing derived from emotional awareness has been related more specifically to three overlapping factors in learning processes [@lavoueEmotionAwarenessTools2020]: (1) emotion as useful information to direct or redirect cognitive resources; (2) emotion as useful information to assess and guide learners' interest and motivation; and (3) emotion as useful information to regulate one's own behavior, including one's own emotions.

Learners may use their achievement, epistemic and topic emotions as useful cues to evaluate and direct cognitive processes [@pekrunControlValueTheoryAchievement2014]. The work of D'Mello and colleagues [-@dmelloConfusionCanBe2014], for instance, highlights how confusion is a signal of cognitive disequilibrium, which learners are therefore incited to re-balance. The work of Vogl and colleagues [-@voglSurpriseCuriosityConfusion2019] about the role of epistemic emotions suggests that experience of curiosity, pride and shame may inform learners about different reasons for knowledge exploration.

Awareness of one's emotions may prompt learners to assess their motivation to keep engaged or disengage from a learning activity [@linnenbrink-garciaAdaptiveMotivationEmotion2016]. For instance, Baker and colleagues' work [-@bakerBetterBeFrustrated2010] draws attention on the importance of recognizing boredom and frustration as warnings of inefficient and potentially misleading efforts.

Finally, emotion may be used as useful information to regulate one's own behavior, including emotion regulation itself [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]. When performed explicitly rather than implicitly [@torrePuttingFeelingsWords2018], emotion regulation requires the person to be aware of the emotion to be regulated [@lavoueEmotionAwarenessTools2020]. According to Gross, emotion regulation "refers to the processes by which we influence which emotions we have, when we have them, and how we experience and express them" [@grossEmotionRegulationAffective2002, p. 282]. The process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015] posits that regulation can happen at five successive stages:

1.  *situation selection*, which consists in avoiding or experiencing events according to their probability of eliciting unwanted or sought after emotions (*e.g.*, *I won't use this Learning Management System, I get angry every time I use it!*);
2.  *situation modification*, consisting in attuning the situation once it has been selected or it is forced upon the person (*e.g., I will use it only to read the forum*);
3.  *attentional deployment*, by which some characteristics of the situation are considered more relevant than others (*e.g.*, *The individual resources are good, it is the overall system I don't like*);
4.  *cognitive change*, through which the focal points of the situation can be reappraised in an attempt to shift, endure or increase an emotional experience (*e.g.*, *Maybe with practice I will end up to find the system useful*);
5.  *response modulation*, which occurs once an emotion has already been elicited and the person attempts, for instance, to suppress its manifestations (*e.g.*, *Ok, stay calm, breath and don't shut the browser!*).

The fourth and fifth passages have been often considered as two alternative strategies for emotion regulation, with potentially different impact on person's cognition and behavior [@bonannoImportanceBeingFlexible2004; @grossEmotionRegulationAffective2002; @richardsEmotionRegulationMemory2000]. In some circumstances, cognitive change can be more beneficial, since it reassesses the situation in what shall be more favorable terms for the person, freeing resources that would otherwise be monopolized by the undesired emotion [@richardsEmotionRegulationMemory2000]. In other circumstances, a cognitive re-evaluation of the situation may be too demanding, and the person may have more benefits in trying to suppress the response modulation [@bonannoImportanceBeingFlexible2004].

Torre and Lieberman [-@torrePuttingFeelingsWords2018], on the other hand, hypothesize that emotion regulation may occur even implicitly through *affect labeling* [@liebermanAffectLabelingAge2019; @liebermanPuttingFeelingsWords2007; @liebermanSubjectiveResponsesEmotional2011], that is, when people assign a word to the their emotional experience (*e.g.* "I feel *angry*"). In their article, the authors provide an overview of research about affect labeling and state that it "has demonstrated a modulation of emotional output effects in the same experiential, autonomic, neural, and behavioral domains as found in other forms of emotion regulation" [@torrePuttingFeelingsWords2018, p. 117]. The authors also highlight, though, that the mechanisms by which affect labeling intervene as implicit emotion regulation are still unclear. In this regard, Torre and Lieberman (*ibid*.) propose four possible mechanisms:

1.  *distraction*, for resorting to language shift the attention from the situation itself and therefore attenuates full processing of the eliciting event;
2.  *self-reflection*, as a means to initiate an introspection process fostering self-distancing from the emotion;
3.  *reduction of uncertainty*, which results from categorizing an intense and often nuanced experience using known and community-shared words;
4.  *symbolic conversion*, consisting in events assuming symbolic status through the associated word, which may induce more abstract thinking about the eliciting event.

Emotional awareness may therefore be useful either as explicit [@grossEmotionRegulationCurrent2015] or implicit [@torrePuttingFeelingsWords2018] emotion regulation. It may also help learners in evaluating whether a regulatory strategy may be more or less appropriate given the situation at hand [@lavoueEmotionAwarenessTools2020].

To sum up, there is a consistent body of research that highlights how emotion play a prominent role at the intra-individual level in various processes, which are also implicated in learning. Learners may therefore benefit from self-emotional awareness as valuable information upon which deciding how to steer their learning processes.

### Related Works

Molinari and colleagues [-@molinariEMORELOutilReporting2016] developed a self-reporting system from an experience sampling method [@csikszentmihalyiValidityReliabilityExperienceSampling2014] perspective, the EMORE-L (EMOtion REport for E-Learning). The tool was used in an ecological setting by 16 university students who voluntarily adopted the system during 15 days of distance learning in a blended bachelor program. The system consists in a short online questionnaire that students were reminded to fill once per day through an email that they received at a time previously concerted with the investigators. The EMORE-L consists in nine short questions, which are meant to reduce the amount of time needed to fill-in the questionnaire, organized in 4 parts: a first part about the situation the students were facing, with information about the activity students were conducting; a second part about the cognitive evaluation of the situation on three dimensions (Control, Value, Activation); a third part about the emotional experience, with the possibility of choosing between eight discrete emotions (*pleasure*, *anxiety*, *curiosity*, *boredom*, *engagement*, *confusion*, *surprise*, and *frustration*), for which students could also provide the intensity; and a last part about the social sharing of emotion, with items related to the wish for students to share their emotions with their colleagues, as well as the mutual knowledge of emotions between the student and their colleagues.

Through the 169 questionnaire that were filled throughout the 15 days, Molinari and colleagues (*ibid.*) were able to assess three main exploratory subjects. For each element, the authors used results of the study to discuss consequences on the implementation of emotion self-reporting tool in a context of emotional awareness.

The first subject under scrutiny concerned what emotions were most frequently associated with the different situations a student may encounter during distance learning. In this regard, results highlight four main activities: reading resources, synthesis of the same resources, individual work, and group work. The three most likely emotion to be experience are *pleasure*, *anxiety*, and *surprise*, whereas students made a scant use of the other five emotions proposed by the list. The authors also evinced from their data that the frequency of emotion changed as a function of the activity. On this ground, Molinari and colleagues suggest that self-report tool should take into account the specific activity that they aim to sustain, avoiding thus generic list of emotions.

The second phenomenon explored in the study investigated to what extent students wished to share their emotions with their colleagues, and in what situation. According to their results, Molinari and colleagues posit that students prefer to share their emotions during individual and group works. Furthermore, students were also more inclined to share their emotions when they experienced *anxiety* or *pleasure*. Based on these results, the authors suggest that self-reporting tools shall not be provided at all time, but only when the activity is likely to elicit emotions learners wish to share.

Finally, Molinari and colleagues assessed the contribution of the dimensional approach of the cognitive evaluation items and of the eight discrete emotions in allowing students to express what they felt. The dimensional vs. discrete emotion approach to emotion self-reporting is a longstanding debate [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005], with both approaches presenting advantages and shortcomings in terms of user experience and quality of data (see below in the chapter). Results observed by Molinari and colleagues, augmented by feedbacks from students regarding the use of the system, confirm this trend. For instance, the authors highlight the scant use of the different discrete emotions, with only three out of eight being adopted frequently. At the same time, the cognitive evaluation provided through the *control*, *value* and *activation* dimensions also presented shortcomings: *pleasure*, *anxiety* and *surprise*, in spite of their diversity, were in fact associated to similar appraisal profiles. On this ground, the authors point out how important it is that the self-reporting system helps students in identifying their emotional experience.

Molinari and colleagues (*ibid*) exploratory work provide useful elements with respect to emotional awareness in a voluntary self-reporting setting. On the technical standpoint, the EMORE-L combines antecedents of emotions, in the form of cognitive evaluation on dimensions, with emotional experience expressed as discrete natural language words, even though the two blocks of items are not intertwined in the user experience. Also, even if relatively short, the use of 9 questions to express one's emotions is more suitable for a *scripting* rather than a moment-to-moment emotional awareness.

At a conceptual level, Molinari and colleagues introduced emotional awareness in individual settings, as a means to provide students with self-awareness of their own emotions. At the same time, the authors measured students' wish to dispose of collective emotional awareness. To this extent, Molinari and colleagues highlight how students wished to have full control over when and which emotional information to disclose. In addition, they also manifested the wish of being aware of their colleagues emotions, that is, bringing emotional awareness at the inter-personal level, which is the subject of the next section.

## Emotional Awareness at the Inter-Personal Level {#ea-inter-personal}

The second assumption about the instrumentality of emotional awareness posits that learners can benefit from emotional information available at a collective level, either for their own learning processes, that of their partners, or both at the same time.

### Theoretical Underpinnings

Whereas the intra-personal function of emotion has received extensive attention in the last few decades, several researchers have more recently advocated the need for investigating the inter-personal function of emotion [@parkinsonEmotionSocialRelations2005; @rimePartageSocialEmotions2005; @vankleefInterpersonalDynamicsEmotion2018; @fischerWhereHaveAll2010]. Fisher and Van Kleef [-@fischerWhereHaveAll2010], for instance, posit that

> It is an indisputable fact that emotions are mostly reactions to other people, that emotions take place in settings where other people are present, that emotions are expressed towards other people and regulated because of other people. It is hardly possible to imagine the elicitation of anger, shame, sadness, happiness, envy, guilt, contempt, love, or hatred without imagining other people as cause, target, or third-party observer of these emotions. In other words, the social constitution of emotions is beyond doubt.\
> -- @fischerWhereHaveAll2010, p. 208.

Keltner and Haidt [-@keltnerSocialFunctionsEmotions1999] identify four level of analysis in which emotion play a social function: (1) the individual level whenever the source of the emotion is of a social nature; (2) the dyadic level, such as in direct dialogue or collaboration; (3) the group-level, in which people share common identities and goals to attain; and (4) the cultural level, where the analysis focus on macro-elements such as history and tradition. An overarching synthesis on the social functions of emotions across all levels is proposed by Fischer and Manstead [-@fischerSocialFunctionsEmotion2016], who identify two complementary, but distinct, functions performed by emotions: affiliation and distancing. The affiliation function serves to form and maintain positive social relationship with others, whereas the distancing function helps in establishing and maintaining a social position relative to others (*ibid.*).

Other social functions of emotion comprise: the social sharing of emotion [@rimeEmotionElicitsSocial2009; @rimePartageSocialEmotions2005] as a means to initiate and reinforce social-bonding; harnessing emotional information to understand causes and consequences of behaviors in others [@parkinsonEmotionsInterpersonalInteractions2010; @vankleefHowEmotionsRegulate2009]; the use of emotional information as a reference for normative conduct or for inferring personality traits [@hareliEmotionsSignalsNormative2013; @hareliWhatEmotionalReactions2010]; and the manifestation of emotion as a form of involuntary contagion [@barsadeRippleEffectsEmotional2002; @barsadeGroupAffect2015] or deliberate influence on others [@vankleefEmotionInfluence2011; @vankleefEmotionalInfluenceGroups2017]. In this regard, Van Kleef [-@vankleefEmergingViewEmotion2010; -@vankleefHowEmotionsRegulate2009; -@vankleefInterpersonalDynamicsEmotion2018] proposes an overarching framework, the Emotion As Social Information (EASI) model, which attempts at integrating the many inter-personal functions of emotion in terms of two classes of mutually influential, but conceptually distinct and empirically separable mechanisms:

1.  *Emotional expressions trigger affective reactions in observers*. The first mechanism implies that the emotion of a person may be the eliciting stimulus of an emotion in the observer. The affective reaction in the observer may be the same as the one expressed by the person, as in the case in emotional contagion [@barsadeRippleEffectsEmotional2002] or empathy [@bloomEmpathyItsDiscontents2016; @wondraAppraisalTheoryEmpathy2015]. For instance, boredom may propagate between learners as the result of the first person pausing and puffing. At the same time, the emotion in one person can trigger a different affective reaction in the observer, but independently of inferential processes about the situation. For instance, the amusement of a colleague during a collaborative task may trigger anger in the observer who wants to stay focused, independently of the reasons why the colleague is amused. Whether the observer affective reaction mirrors or complements that of the person who has expressed it, the reaction will have consequences on the observer's behavior (*e.g.*, deciding to ignore the amused colleague) and/or both the observer and the person who expressed the *original* emotion (*e.g.*, both learners decide to take a break).
2.  *Emotional expressions elicit inferential processes in observers*. The second mechanism implies a more complex and deliberate act of information-processing by which the observer attributes to the expressed emotion potential causes of and consequences on behavior. Especially in appraisal theories of emotion [@moorsAppraisalTheoriesEmotion2013; @moorsFlavorsAppraisalTheories2014; @rosemanAppraisalTheoryOverview2001], which will be more thoroughly depicted in the next chapter, the evaluation a person makes of the situation is pivotal in determining what emotion she may feel. By a form of *reverse engineering* [@hareliWhatEmotionalReactions2010; @schererFacialExpressionsAllow2007], the specific emotion a person is feeling may be used to infer how that emotion came to be elicited.

The EASI model (*ibid.*) also specify two preconditions and two moderator factors that determine the inter-personal dynamics of emotion. The preconditions concern (1) the ability of the person experiencing the emotion to *encode* it in a form that can be communicated to the observer, which broadly refers to the concept of *emotion expressivity* [@kringIndividualDifferencesDispositional1994; @scarantinoHowThingsEmotional2017]; and (2) the observer's ability to *decode* the emotional information and make sense from it using emotional knowledge and understanding, which closely relates to the principle of socio-emotional competences depicted in Section \@ref(socio-affective-competences) [@brackettRULERTheoryDrivenSystemic2019; @chernissEmotionalIntelligenceClarification2010; @matthewsScienceEmotionalIntelligence2007]. The two preconditions reflect the displaying and the monitoring functions of awareness tools depicted in Section \@ref(displaying-monitoring-functions) [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. As stated by Van Kleef [-@vankleefInterpersonalDynamicsEmotion2018]:

> No matter how informative emotions may be, and however critical their social-regulatory functions, if emotions are not expressed and/or fail to be perceived by others, their signaling function is evidently lost and social interaction may be jeopardized.\
> -- @vankleefInterpersonalDynamicsEmotion2018, p.52

The EASI model also proposes two classes of moderating factors that may influence to what extent an expressed emotion results in an affective reaction and/or an inferential process by the observer (*ibid*.). For instance, it may be the case that the *confusion* expressed by a colleague may trigger an affective reaction of *anger* in the observer, because this slows down the collaboration. On the other hand, the same expression may also push the observer to understand the causes of the colleague's *confusion*, for then proposing some form of co-regulation. According to the EASI model, the engagement of the observer in deliberate and more cognitively demanding inferential processes may be facilitated by (1) the observer's motivation and capacity to allocate resources to make inferences about causes and consequences on the sender's behavior; and (2) the perceived *appropriateness* of the expressed emotions according to the observer's criteria of evaluation, for instance in relation to social norms [@fischerSocialFunctionsEmotion2016; @hareliWhatEmotionalReactions2010]. In other words, the observer may try to understand the colleague's confusion if (a) the observer has some *free* resources to dedicate, which may be difficult if the learning context is demanding, and (b) the observer evaluates *confusion* as an appropriate reaction to the situation, for instance by reckoning that the learning task is difficult.

The EASI theory (*ibid.*) provides an integrative framework of the social-function of emotion, which is founded on the assumption that one's own emotions may guide thoughts, actions and feelings in another person. Using emotion as social information may, for instance, be a first step in the social regulation of emotion, that is, the attempt to voluntary act upon other people's emotion [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013]. According to Reecks and colleagues [-@reeckSocialRegulationEmotion2016, p.48] , "[t]he goal-driven nature of social regulation distinguishes it from related phenomena, such as social sharing, empathy, or emotional contagion, where one person's actions are not strategically directed towards influencing another's emotions". The authors propose a cross-disciplinary model that implements the process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], illustrated in the intra-personal section, from an inter-personal perspective. In what they call the Social Regulatory Cycle, depicted in Figure @ref(fig:tf-social-regulation-model), the authors identify the regulator and the target person upon which the emotion regulation is intended. The regulator must proceed, first, by identifying the target emotion. Second, the regulator must assess whether the identified emotion corresponds to a suitable emotion or not. Third, in case of a discrepancy between the two, the regulator must plan a strategy aiming at producing in the target the suitable emotion. Last, this strategy must be implemented. At this point, the strategy may target a different stage of the process in the target (see the description of the individual model above for more details): the selection or modification of the situation; the orientation of the target's attention; the possibility of changing the way the target appraises the situation; or the modulation of the behavioral, physiological and experiential manifestation of the emotion. The cycle may start again either from an individual standpoint (*e.g.*, the target is dissatisfied with the new emotion induced by the regulator) or inter-individual perspective (*e.g.*, the regulator did not obtain the suitable emotion or identify an even more adapted emotion for the target).

(ref:tf-social-regulation-model-caption) The Social Regulatory Cycle at the inter-personal level, fromm the original Figure 2 in @reeckSocialRegulationEmotion2016, p. 51.

```{r tf-social-regulation-model}
#| fig.align="center", 
#| fig.cap="(ref:tf-social-regulation-model-caption)",
#| fig.scap = "Social Regulatory Cycle, from Reeck et al. (2016)",
#| out.width="80%"
knitr::include_graphics(
    here::here("./figure/theory/social-regulation-emotion.png")
)
```

To sum up, there is extensive work in emotion theory advancing reasons why emotions play a pivotal role at the inter-personal level. Learners may therefore benefit from socially conveyed emotional awareness both at the individual and collective level, assuming *appropriate* communicating and inferential processes are at play.

### Related Works

Eligio, Ainsworth and Crook [-@eligioEmotionUnderstandingPerformance2012] carried out two intertwined experiments aiming at exploring "what collaborators understand about each others emotions and the implications of sharing information about them" (*ibid*, p. 2046). In the first experiment, the authors asked pairs of same-sex, unacquainted participants to play a collaborative game in a co-located environment, where they shared the same computer equipment. At the end of the collaboration, participants were asked to fill in two questionnaires where they had to rate the intensity of 15 emotions: *happy*, *angry*, *sad*, *fearful*, *angry*, *bored*, *challenged*, *interested*, *hopeful*, *frustrated*, *contempt*, *disgusted*, *surprised*, *proud*, *ashamed* and *guilty*. In the *own* version of the questionnaire, they rated the intensity of each emotion as they have perceived it during the task. In the *partner* version of the questionnaire, participants had to project the intensity by which their partner in the dyad had experienced each of the listed emotion. The administration of the questionnaire was made individually, so that participants could not discuss the matter with their partner, and up until that moment, participants were not informed of the rating, so that they had no particular reason to pay attention neither to their own, nor to their partners' emotions. By comparing the responses to the two questionnaires, the authors concluded that, despite collaborating side-by-side, participants had little understanding of their partner's emotions. On the contrary, consistently with previous findings in the literature [@kruegerTrulyFalseConsensus1994; @tomaAnticipatedCooperationVs2010], participants tended to project their own emotional experience onto their partners. Eligio and colleagues (*ibid*) provides two possible explanations for these results. On the one hand, participants could simply not care about the emotions of their partner. On the other hand, participants could genuinely care about them, but lack the means to focus on them, for instance because the task was too demanding, and they decided to prioritize other mental states perceived as *more* instrumental to the task at hand. In both cases, the attribution of their own emotional experiences to that of the partner is caused by a lack of information available, but the reasons for this shortcoming are not the same depending on the intentions.

In the second experiment, Eligio and colleagues (*ibid*) therefore decided to intervene by providing (or not) participants with explicit awareness of their partner's emotions using a *scripting* strategy, that is, by interrupting the collaboration at specific moments for participants to express their emotions and projecting that of their partner. After filling the *own* and *partner* questionnaire, if participants disposed of awareness, they could look at the emotions of their partner's before resuming the task, otherwise they just continued with the collaboration. Furthermore, dyads were also assigned either in a co-located or a remote collaborative setting, resulting in a 2x2 factorial design with awareness vs. no-awareness, and co-located vs. remote conditions. Using experimental settings similar to the first experiment (but with a slightly different collaborative task and with only women as participants), the authors report evidence suggesting that participants benefited of emotional awareness in terms of performance both in the co-located and remote conditions. Furthermore, participants in the remote condition were more accurate in understanding emotions of their partner and also experienced more *positive affect*, computed by averaging the intensity of *happy*, *interested*, *hopeful*, *excited* and *challenged*.

From the two experiments, Eligio and colleagues (*ibid.*) concluded that participants do not have an accurate understanding of the emotion of their partner if their attention is not explicitly driven to it. Providing emotional awareness seems thus a promising way to increase mutual understanding, since participants with emotional awareness showed higher accuracy in estimating their partner's emotions, as well as better performance to the task at hand.

Following Eligio and colleagues (*ibid.*) findings, Molinari and colleagues [@molinariEmotionFeedbackComputermediated2013] conducted a study in which 30 dyads of same-sex participants (16 dyads of women, 14 dyads of men) performed a collaborative task in remote conditions, with the possibility of audio but not video connection. The aim of the collaborative task was to conceive a slogan against violence in schools using an argument graphic tool [@lundHowArgumentationDiagrams2007]. The task therefore implied some form of negotiation for deciding the final outcome of the collaboration, which was intended to create socio-cognitive tensions [@andriessenSociocognitiveTensionCollaborative2011] in participants as a means to elicit emotions. Half of the dyads were randomly assigned to a control condition, in which participants did not dispose of emotional awareness. The other half of the dyads were provided with emotional awareness through the use of an EAT persistently available on screen alongside the collaborative tool to build the slogan. (The tool will be described in more detail below, see Figure \@ref(fig:tf-eatmint-eat).) Contrary to Eligio and colleagues (*op. cit*.), in which participants shared emotions at specific moments during the task following a *scripting* strategy, in this case the setting was congruent with an *awareness* strategy, since participants could express and have access to their partner's emotions at any time during the task. A message also showed up after 5 minutes from the last expressed emotion to remind participants to express how they were feeling.

At the end of the collaborative task, participants individually filled-in a questionnaire aimed at gathering information about (1) the kind and intensity of the emotions of the participant at the end of the task, as well as the emotions the participant attributed to the partner, particularly with respect to the 20 discrete emotions available as buttons on the EAT; and (2) the quality of the perceived interaction based on the frequency by which the participant and the partner provided/imposed their own points of view, defended and argued their ideas, understood their partner's points of view, built up on their partner's ideas, as well as managed emotion during interaction. As for Eligio and colleagues (*op. cit.*), also Molinari and colleagues (*ibid*.) report that their result support the hypothesis of beneficial effect from the presence of an EAT, but only in dyads of women and with the presence of mixed results with respect to their initial hypothesis.

The tests upon which Eligio and colleagues (*op. cit*.) and Molinari and colleagues' (*ibid*.) results are based, though, do not consider the hierarchical structure of the dyads, which may inflate the rate of type I error since the non-independence of observations is not taken into account by the ordinary least square models they have adopted in their analyses [@brownIntroductionLinearMixedEffects2021; @singmannIntroductionMixedModels2020; @westLinearMixedModels2015]. As a consequence, the evidence provided by the two studies should be taken with caution. The experimental settings, on the other hand, are interesting and directly compare a *scripting* and an *awareness* perspective, with the latter presenting the advantage of persistent, moment-to-moment emotional awareness. The two studies also highlight the many methodological challenges in determining the effect of an EAT in computer-mediated learning environments, which concern how emotional awareness is provided, but also how the potential benefit are measured and analyzed.

A different approach was taken by Avry and colleagues [-@avryAchievementAppraisalsEmotions2020], who, rather than providing awareness through discrete emotions, took a dimensional approach adopting Pekrun's Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006] introduced in Section \@ref(learning-on-affect). In their study, the authors asked 28 dyads of same-sex participants to play a remote collaborative game. Beside the game window, participants disposed of a smaller window on the screen which reported two feedbacks: (1) a feedback on how well the dyds was mastering the game, representing the *Control* dimension; and (2) how well they were performing in a standing compared to other dyads, representing the *Value* dimension. The two feedbacks were manipulated by the authors as to create a 2x2 factorial design combining high vs. low *Control* on the one hand, and high vs. low *Value* on the other. After the collaborative task, participants filled in a questionnaire, on which they evaluated the overall collaboration in terms of (a) affective dimensions (Valence, Dominance, and Activation) and 16 discrete achievement emotions, and (b) six computer-supported collaborative exchanges (sustaining mutual understanding, information pooling, transactivity, reaching consensus, task management, and time management). For both category of measures, the rating was performed for the participant herself, as well as what the participant thought her partner would have experienced. The results obtained by Avry and colleagues (*ibid.*), who took into account the hierarchical structure of dyds by checking the intraclass correlation of dyads, corroborate an effect of the manipulation of the Control-Value appraisals on both category of measures (affective and socio-cognitive). These results are particularly interesting considering that in this study a form of emotional awareness was (1) manipulated, and (2) conveyed through a feedback aimed at eliciting a certain kind of appraisal of the situation, which influenced the kind of discrete achievement emotions experienced.

The three studies outlined in this section provide very different approaches to the study of emotional awareness in computer-mediated learning environments. Since this is a recent and cross-disciplinary field of inquiry, a fragmented stage of research is inevitable [@fiedlerCycleTheoryFormation2004]. On the other hand, efforts should also be dedicated to the possibility of direct comparison and incremental building of knowledge. In this regard, the adoption of a prototypical EAT could be beneficial, for similarities and differences in conveying emotional awareness can be more easily defined through objective parameters.

## Emotion in Computer-Mediated Learning Environments {#ea-in-computer-mediated-environment}

The third assumption underlying emotion awareness tools concerns the possibility to create emotional awareness in computer-mediated learning environments.

### Theoretical Underpinnings

For emotional awareness to emerge and be available, it is necessary that emotion may be fruitfully *encoded* and *decoded* in a computer-mediated learning environment. In this regard, the computer-mediated environment is somehow ambivalent with respect to affect-related phenomena. On the one hand, the combined presence of a learning task and a technological device captures learner's attention and thus, even in the case of co-located interaction or seamless audio/video connection in remote settings, diminishes the possibility to pay attention to efferent cues of affective experiences -- such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009] -- that are more readily available in face-to-face interaction [@baltesComputerMediatedCommunicationGroup2002; @lundHowArgumentationDiagrams2007]. On the other hand, the same presence of a technological device provides alternative or complementary means to create emotional awareness and even extend it over time [@derksRoleEmotionComputermediated2008; @gliksonDarkSideSmiley2018; @hegartyCognitiveScienceVisualspatial2011; @leonyProvisionAwarenessLearners2013; @derickEvaluatingEmotionVisualizations2017].

The presence of emotion in computer-mediated learning environments even without seamless audio/video connection can be sustained by a corollary to the EASI model, illustrated in the previous section, which Van Kleef [-@vankleefSocialEffectsEmotions2017] identifies as the *functional equivalence hypothesis*. The hypothesis posits that the role emotion play at the inter-personal level is equivalent regardless of the specific way it is communicated, as long as the emotional information passes from the sender to the receiver. In the words of the author:

> If one accepts the notion that emotional expressions can influence social interactions by providing information about what is on the expresser's mind, it follows that emotions can have such effects regardless of how they are expressed, as long as the expressions convey the relevant information. Consequently, EASI theory posits that expressions of the same emotion that are emitted via different expressive modalities (i.e., in the face, through the voice, by means of bodily postures, with words, or via symbols such as emoticons) have comparable effects, provided that the emotional expressions can be perceived by others\
> --- @vankleefSocialEffectsEmotions2017, p. 213

On the other hand, the fact that various forms of emotional expression are equivalent with respect to their social function does not mean that each way of expressing an emotion is equivalent [@vankleefInterpersonalDynamicsEmotion2018]. A colleague's anger expressed by shouting on your face how badly your part of a collaborative task has been handled has not the same intensity of *I am very angry with you* written in an email. A diminished *veracity*, though, may be compensated by more favorable conditions to engage in inferential processes rather than affective reactions as described by the EASI model (*ibid.*).

An EAT whose aim is to make learners' aware of their own and/or their colleague's emotions must therefore face the challenges of *encoding* and *decoding* emotional information, so that it maximized the preservation of its *functional* meaning.

In this regard, the *encoding* of emotional information has received so far greater attention than *decoding*, since it is tightly related to the action of detecting, representing or measuring emotions [@fuentesSystematicLiteratureReview2017; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @silvaComparativeStudyUsers2020]. In their review of technologies for emotion-enhanced interaction already cited in Section \@ref(affect-aware-systems), Cernea and Kern [-@cerneaSurveyTechnologiesRise2015] distinguishes between three types of techniques commonly adopted to estimate emotions: (1) perception-based estimation, derived from efferent manifestations of emotion such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009; @martinezContributionsFacialExpressions2016]; (2) physiological estimation, based on the detection of physiological patterns such as heart rate, blood pressure, or skin conductance [@ragotEmotionRecognitionUsing2018; @shuReviewEmotionRecognition2018]; and (3) subjective feelings, based on the person's self-report of her own emotional experience [@lavoueEmotionalDataCollection2017; @ritchieEvolutionSelfreportingMethods2016; @silvaComparativeStudyUsers2020]. As stated in the introduction, the thesis focuses on this last category. In fact, physiological estimation requires dedicated hardware and software, which would be difficult to provide at scale. Perception-based estimation may use more widely available hardware and software, for instance through a webcam or keyboard stroke, but the accuracy and usefulness of this kind of measure is still lively debated in the literature [@barrettEmotionalExpressionsReconsidered2019; @bahreiniMultimodalEmotionRecognition2016; @nahinIdentifyingEmotionKeystroke2014]. Barrett and colleagues [-@barrettEmotionalExpressionsReconsidered2019], for instance, argue that automatic recognition from facial expression are still limited in reliability, lack in specificity, and does not take sufficiently into account effects of context and culture. Voluntary self-report is therefore retained as the most parsimonious, portable and reliable way to provide emotional awareness both at the intra-personal and inter-personal levels, especially when the aim is to stimulate voluntary emotional introspection and/or inferential processes [@boehnerHowEmotionMade2007; @vankleefInterpersonalDynamicsEmotion2018].

Emotional self-report is tightly related to the underlying concept of *what an emotion is*, which will be discussed in Chapter \@ref(defining-emotion-unit). From a technical standpoint, emotion self-report is traditionally characterized by the dimensional or the discrete emotion approaches, as well as by a combination of the two [@bradleyMeasuringEmotionSelfAssessment1994; @cowenSelfreportCaptures272017; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005].

In a dimensional approach, an emotion is conceptualized as a rating on a number of continuous criteria, among which the most frequently adopted are *Valence* (also called *pleasure*) and *Arousal* (also called *activation*) [*e.g.*, @russellCircumplexModelAffect1980; @stanleyTwodimensionalAffectiveSpace2009]. Different or additional dimensions are nevertheless also possible. It is the case, for instance, of the Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] or the AffectButton [@broekensAffectButtonMethodReliable2013], which both use the *Valence*, *Arousal*, and *Dominance* dimensions, even if in different formats. The Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] uses three rows of figures (*i.e.*, the *manikin*), where each row represents one of the three dimensions: 5 figures, progressively modified in some features, represent the increasing or decreasing value on the specific dimension. The AffectButton [@broekensAffectButtonMethodReliable2013], on the other hand, uses a single iconic facial expression that changes according to the user's coordinates of the mouse on the surface of the icon: the horizontal movement determines the pleasure dimensions; the vertical movement the dominance dimension; and the arousal dimension is calculated according to the distance of the mouse from the central point of the image.

In the discrete approach, emotion is conceptualized as a phenomenon with distinctive features compared to other emotions, which may consist in different facial expressions when emotion is represented graphically, or semantic meaning when emotion is represented by natural language words or idioms [@torrePuttingFeelingsWords2018; @desmet2003measuring; @fontaineComponentsEmotionalMeaning2013; @ruizSupportingLearningConsidering2016]. The number, kind and organization of the representations varies depending on several aspects. In the case of verbal representations that adopts natural language nouns or adjectives, for example, the list may be determined according to emotion theories -- as in the case of basic emotion theory [@ekmanArgumentBasicEmotions1992] -- or determined empirically according to previous (or pilot) studies, often with the aim of retaining a list of the most frequently experienced or expressed items. For instance, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented in a self-report interface 20 buttons, 10 labeled with *negative* and 10 with *positive* emotion adjectives retrieved among the most frequently expressed during a computer-mediated collaborative task in a pilot study. As alredy mentioned in Section \@ref(affect-on-learning), @ruizSupportingLearningConsidering2016 adopted a similar approach with 12 discrete emotions. When emotion is conceptualized graphically, representations usually attempt at maintaining some degree of analogy with *reality*, for instance with respect to facial expressions [@desmet2003measuring; @lauransNewDirectionsNonVerbal2012]. In the discrete emotion approach, respondents can for instance: (1) choose the item from a predefined list that best matches the emotional experience; (2) rate their agreement on a scale for each item in a predefined list as to the extent they have experience that particular discrete emotion; and (3) rate the intensity by which each discrete emotion has been experienced [@schererGRIDMeetsWheel2013].

The dimensional and discrete approaches can be combined by providing discrete emotions a precise collocation on the dimensions. The result is often referred to as an *affective space* [@shumanConceptsStructuresEmotions2014; @schererWhatDeterminesFeeling2006; @gilliozMappingEmotionTerms2016]. Recent work on the meaning of emotional terms seems to suggest that four dimensions are needed to account for emotion differentiation : *Valence*, *Power*, *Arousal*, and *Novelty* [@fontaineGlobalMeaningStructure2013; @fontaineWorldEmotionsNot2007; @gilliozMappingEmotionTerms2016]. Gillioz and colleagues [@gilliozMappingEmotionTerms2016], for instance, empirically mapped through a principal component analysis 80 french emotions terms on the four-dimensional affective space. (More on this in Section \@ref(cpm-integration-module).)

Emotional *decoding* in a computer-mediated environment is tightly linked on how emotion is *encoded* and has received so far limited attention [@bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Furthermore, visualization of emotion in this context is usually derived from affective information available in students' productions or communication exchanges, for instance through sentiment analysis rather than a dedicated tool [@leonyProvisionAwarenessLearners2013]. In the absence of specific representations for emotional awareness, more general guidelines about the visuo-spatial representation of data should be applied [@hegartyCognitiveScienceVisualspatial2011; @hehmanDoingBetterData2021]. Emotional *decoding* is also influenced by the objective of the visualization. Learners' emotion may be represented individually or collectively, as a single unit in time or grouped through short or long time spans. They may also be partially or fully available to all other students or only to a selection of colleagues (e.g. in a group work). In other words, if the aim of emotional *encoding* is rather straightforward, emotional *decoding* depends on a larger number of factors.

### Related Works

Henritius and collegues conducted a systematic review of empirical research conducted on universities' students emotions in virtual learning based on 91 articles published between 2002 and 2017 in four journals [@henritius2019]. Among the conclusions stated from this review, the authors state that "on a more critical note, we observed that only a few of the studies actually pay attention to the fluctuation of emotions in the context and in the flow of events" (*ibid.*, p. 97). According to the authors, most studies analyzed emotions retrospectively and conceptualize them as traits rather than transitory states or processes (*ibid.*).

The lack of studies investigating affect or emotion in a dynamic perspective is also due to the fact that there is scant work that has been dedicated to instruments that combine both emotion *encoding* and *decoding* in a computer-mediated learning environment, especially in real-time [@lavoueEmotionalDataCollection2017; @ez-zaouiaEmodashDashboardSupporting2020]. As pointed out by Lavoué and colleagues [@lavoueEmotionalDataCollection2017], the few attempts that have been made are mostly ad-hoc solutions that do not aim at a general application. As a result, in learning settings, emotion self-report is often provided through questionnaires [@pekrunMeasuringEmotionsEpistemic2016; @pekrun2011] or adaptation of experience sampling methods [@scollonExperienceSamplingPromises2003; @csikszentmihalyiValidityReliabilityExperienceSampling2014; @molinariEMORELOutilReporting2016].

Emotion representation, on the other hand, is even less frequent and therefore less developed, especially in real-time [@ez-zaouiaEmodashDashboardSupporting2020; @bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Ez-zaouia and colleagues [-@ez-zaouiaEmodashDashboardSupporting2020], for instance, developed EMODASH, a dashboard for visualizing retrospective emotions for tutors in an online learning environment. @leonyProvisionAwarenessLearners2013 and @derickEvaluatingEmotionVisualizations2017 also propose dashboard inserted into a computer-mediated learning environment, but limited to a handful of emotions (e.g. *frustrated*, *confused*, *bored*, *happy*, and *motivated),* which are automatically derived from learners' activities. @leonyProvisionAwarenessLearners2013, in particular, propose a series of visualizations organized as time-based*,* context-based*,* visualizations of change in emotions*,* and visualization of accumulated information.

To the best of my knowledge, there is only a handful of tools reported in the literature that come close to the purpose of an EAT as considered in the present contribution. I will focus on three of them in the reminder of this section.

Feidakis and colleagues [@feidakisProvidingEmotionAwareness2014; @feidakis2013] developed the *emot-control* emotion aware system, depicted as a pop-up window in Figure \@ref(fig:emot-control-image). The tool combines the dimensional and discrete approaches by placing 12 icons of facial expressions *(inspired, excited, interested, relaxed, curious, confused, anxious, indifferent, bored, tired, angry, desperate*) in Russel's [-@russellCircumplexModelAffect1980] *Valence* x *Arousal/Activation* circumplex, with a neutral face in the middle. The tool also presents a text input, which originally shows the noun associated to the icon on which users have clicked, but that can also be switched to another semantic word/expression. Users can also provide more context to their feeling by typing a description through a second text-area. 5 other icons on the right-hand of the interface represent each a different mood (*sad*, *unhappy*, *neutral*, *happy*, and *very happy*). Users can access their own last affective episode reported directly on the interface, whereas they have to click on a button to open a new window to access their colleagues' emotions. The window shows a vertical timeline of affective episodes organized by group members, with the date, the time, the emotion (combination of icon and noun/expression), and the mood. In a study, Feidakis and colleagues [@feidakis2013] adapted the tool to the Moodle learning environment and, in accordance to the Affect-Aware perspective (see previous chapter), endowed the environment also with an Affective Virtual Assistant, which responded to learners affective episodes. Congruently with the approach advocated in this contribution, learners where free to use the tool as they wished, and they could therefore dispose of emotional awareness at any time in the learning environments. The authors report that students took advantage from the presence of the tool. In particular, they highlighted that "most of the students reported that the group emotion awareness was a very useful functionality since many members had the chance and the initiative to intervene in their groups when they noticed bad feelings of their peers" (*ibid.* p. 1656). In another, similar, study [@feidakisProvidingEmotionAwareness2014], the authors suggest that learners appreciated the presence of the tool, but mainly because it was a novel way to experience a computer-mediated learning environments. They did not seem to consider that the tool in itself added some value to the learning experience as a whole. In the study, the authors also administered the System Usability Score @brookeSUSQuickDirty1996 to measure the perceived usability of the tool. With $N = 29$ participants, they obtained a rating of $M = 67.91$ (no SD provided) out of 100.

(ref:emot-control-image-caption) Image of the overall interface in which the *emot-control* is inserted. Retrieved from @feidakis2013, Figure 7 in the original article. p. 1653.

```{r emot-control-image}
#| fig.cap = "(ref:emot-control-image-caption)",
#| fig.scap = "Emot-Control tool, from Feidakis et al. (2013)",
#| out.width = "80%" 
knitr::include_graphics(
    here::here("figure/theory/emot-control-feidakis.png")
)
```

The tool provided by Feidakis and colleagues comply with all the three main features of an EAT as advocated by the present contribution: it is based on voluntary-self report, it incorporates an emotional structure based on Russel's (1980) circumplex, and allows moment-to-moment emotional awareness. On the other hand, the tool separates the expressing-displaying and the perceiving-monitoring function of an awareness tool, which are provided in two separate windows. Furthermore, the use of icons representing synthetic facial expressions present some limitations. From a spatial perspective, augmenting the number of options would reduce the size of the icons and make identification more difficult. Depicting emotions with icons is also more prone to misunderstanding or difficulty to provide a sufficient number of images, and different enough to discriminate between similar feelings. For instance the emotion placed from noon to 1 (*inspired*) in the circumplex is very similar to that placed from 2 to 3 o'clock (*interested*). It is useful to note in this regard that a previous version of the tool combined the icon and the label, for the label being dropped in the latest version to overcome language barriers [@feidakis2013]. Finally, the tool combine emotions and moods on the same observation, which may be problematic given a growing consensus in considering the two as distinct phenomena: we may have moods and emotions at the same time, but also only moods or only emotions [@linnenbrink-garciaAdaptiveMotivationEmotion2016; @schererWhatAreEmotions2005; @scherer2022].

The Mood Meter Mobile Application[^1], depicted in Figure \@ref(fig:mood-meter-image), is a digital extension of the physical Mood Meter dashboard of the RULER approach [@brackettRULERTheoryDrivenSystemic2019; @hoffmannTeachingEmotionRegulation2020; @nathansonCreatingEmotionallyIntelligent2016] introduced in Section \@ref(socio-affective-competences). The physical Mood Meter consists in four colored quadrants organized according to two axes: *Valence* and *Activation/Arousa*l. Learners can therefore project themselves in one of the four quadrants according to (1) the extent by which they rate their emotional exprience as pleasant or unpleasant, and (2) how high or low is their energy. The top-right yellow quadrant corresponds to emotions which are pleasant and high on energy, such as *excitement*, *joy* and *elation*. The bottom-right green quadrant corresponds to emotions which are pleasant and low on energy, as *tranquility*, *serenity*, and *satisfaction*. The bottom-left blue quadrant corresponds to emotions which are unpleasant and low in energy, as *boredom*, *sadness* and *despair*. And finally, in the top-left red quadrant corresponds to emotions which are unpleasant and high in energy, such as *anger*, *frustration* or *anxiety*. In the digital app, the overall dashboard is divided in 100 points (25 per quadrant). Users can tap one of the point in the quadrant and obtain 9 emotion words, that is the word that is associated with the very point they have clicked, plus 8 alternatives, which corresponds to the 8 adjacent points on the dashboard. Once identified the emotion term that corresponds to how the learner is feeling, the person can: (1) describe the feeling by typing in more information; and (2) select a strategy between quotes, images or practical tips that are meant to help the person shift to another feeling if desired. Further features of the app include the possibility to track the feelings entered in the app, for instance with an overview of the percentage per quadrant; set a reminder to use the app at specific periods; and finally share their feelings via Facebook or Twitter.

```{r mood-meter-image}
#| fig.align = "center", 
#| fig.cap = "Image of the four quadrants of the Mood Meter, retrieved from Brackett et al. (2019), Figure 1 in the original article. For the Mood Meter app, see the official site in the footnote.",
#| fig.scap = "Mood Meter, from Brackett et al. (2019)",
#| out.width = "60%"
knitr::include_graphics(here::here("figure/theory/mood-meter-image.png"))
```

As the *emot-control*, also the Mood Meter app comply with the three main features of an EAT advocated in the present contribution, for they both are self-report tools, based on the same underlying emotion structure, and can be applied in a moment-to-moment perspective. On the other hand, the Mood Meter app seems to be more oriented towards intra-individual emotional awareness: the possibility to share the emotion via a social platform is a very limiting feature to provide group-specific and ongoing inter-personal awareness. Furthermore, to the best of my knowledge, the app is provided only in a mobile version, which would force learners to switch back-and-forth from the computer-mediated learning environment to their phone.

A third and final tool cited in this overview is the one adopted in the contribution of @molinariEmotionFeedbackComputermediated2013, already discussed in the related works of Section \@ref(ea-inter-personal) about inter-personal awareness. As a reminder, half of the dyads in a computer-mediated collaborative task disposed of a persistent EAT on the right-hand side of the screen. As shown in Figure \@ref(fig:tf-eatmint-eat), the EAT is vertically divided in two areas. On top, the monitoring-perceiving area consists in the last three discrete emotions expressed by the participant (green boxes) and the partner (blue boxes). The box on top of each pile, representing the very last emotion expressed, is highlighted with a paler color. The lighter green box is also editable by the participant, who can type-in an emotion not available through the buttons in the lower part of the screen. These buttons represent the displaying-expressing function and are organized in two columns: the right one with 10 *positive* emotions, and the left side with 10 *negative* emotions. The list of discrete emotions were defined mixing a previous study in the field [@dmelloDynamicsAffectiveStates2012] and 2 pre-experiments that aimed to identify the most frequent and intense emotions felt during a situation of collaboration, real or imagined.

(ref:tf-eatmint-eat-caption) The argument graphic tool and the EAT adopted in @molinariEmotionFeedbackComputermediated2013, from Figure 1 in the original article.

```{r tf-eatmint-eat}
#| fig.align="center", 
#| fig.cap="(ref:tf-eatmint-eat-caption)", 
#| fig.scap = "First version of EATMINT tool, from Molinari et al. (2013)",
#| out.width="80%"
knitr::include_graphics(
    here::here("figure/theory/eatmint-eat-and-task.png")
)
```

The EAT adopted by Molinari and colleagues (*ibid.*) is based on self-report and provide moment-to-moment emotional awareness, being persistently on screen. On the other hand, though, it adopts a purely discrete approach to emotion, without implementing any explicit emotion structure into the tool beside the dichotomous organization of emotion terms according to the *positive* vs. *negative* distinction discussed in Section \@ref(affect-on-learning) [@erbasRoleValenceFocus2015; @shumanLevelsValence2013; @colombettiAppraisingValence2009]. In this regard, as a follow-up of the experiment, the authors reckoned that the tool could be improved on a number of points, which became the input for my Master thesis [@fritzReinventingWheelEmotional2015] detailed in Chapter \@ref(dew-chapter).

The three EATs illustrated in this section highlight some of the many choices in *encoding* and *decoding* emotional information that can be made in providing emotional awareness through a dedicated self-report tool. These choices can influence emotional awareness both at the intra- and inter-personal levels in ways that are put into perspective in the next section, which provides an abstract model of the functions of an EAT.

## Abstract Model of the Functions of an Emotion Awareness Tool {#abstract-model-of-ea}

The information illustrated above, as well as in the previous chapters, can be integrated in an abstract model that depicts the mechanisms allegedly carried out by an EAT based on the three underlying assumptions, namely that emotional awareness is useful at the individual level, at the collective level, and that it can be fruitfully implemented in a computer-mediated learning environment. The proposed abstract model, though, does not have the pretension to be exhaustive. On the contrary, it aims at providing an organized overview of some of the many different affective, learning or human-computer interaction processes that are (or may be) assumed in the instrumentality of an EAT, as well as how design choices can sustain these processes. The model takes into account elements that are specific or particularly important from the perspective of an Emotion Awareness Tool, assuming that the general functions of an awareness tool illustrated in Section \@ref(awareness-tools) are also at stake.

The model, depicted in Figure @ref(fig:thesis-scm-model), take the perspective of a learner that disposes of an EAT in its computer-mediated learning environment and comprises boxes for four main conceptual elements:

-   **Learning activity**, which may broadly refer to any computer-mediated environment implementing an instructional design;

-   **Intra-personal emotion**, representing a single event or a set of events corresponding to the learners' expressed-displayed emotions;

-   **Inter-personal emotion**, representing a single event or set of events corresponding to emotions expressed-displayed by other learners sharing the same environment and that the learner herself can perceive-monitor;

-   An overarching ***meaning-making*** **process**, which encompasses learner's effort to extrapolate instrumental information from the emotional information available.

The boxes are connected with directional arrows, numbered from 1 to 7, representing a series of processes that are (or may be) implicated in each passage. The numbers are used for identifying the passages, but do not imply a fixed order, even though some processes may be built upon previous passages. This section describes each passage in more detail.

(ref:thesis-scm-model-caption) Abstract model of the functions of an EAT from the perspective of a single learner disposing of it in a computer-mediated learning environment. Numbers pinpoint passages or processes that may influence whether and how an EAT sustains the function at stake.

```{r thesis-scm-model}
#| fig.cap="(ref:thesis-scm-model-caption)", 
#| fig.align= "center",
#| fig.scap = "Abstract model of emotional awareness", 
#| out.width= "100%"
include_graphics(
    here::here("figure/intro/thesis-scm-model.png")
)
```

### From the Learning Activity to Intra-Personal Emotion

Passage #1 goes from the learning activity to an intra-personal emotion, implicating that the learning activity elicits emotions in oneself. An EAT may intervene in this passage at least in two ways, labelled here as emotion alertness and emotion conceptualization for illustration purposes only.

Emotion alertness broadly refers to the fact that the presence of the EAT represents a general reminder about paying attention to emotion during the learning activity, and therefore may increase the *sensitivity* about emotional experience. In other words, the presence of the EAT may push learners to bestow to emotional self-awareness more attention that they would normally do without the presence of the EAT [@brackettRULERTheoryDrivenSystemic2019; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Emotion alertness may be induced to different degrees, depending on, for instance, whether the EAT is always available or not, and whether explicit prompts are frequently sent to learners [@csikszentmihalyiValidityReliabilityExperienceSampling2014; @shiffmanEcologicalMomentaryAssessment2008].

Emotion alertness is not necessarily and automatically linked to the concrete action of encoding an emotion into the system. In fact, even in the case of an increased alertness, learners could genuinely not feel any emotion, or decide not to express their emotion through the EAT either for relatively stable *dispositional* characteristics [@kringIndividualDifferencesDispositional1994; @scherer2021] or for contingent reasons (*e.g.*, bestow priority to the learning task at hand).

Emotion conceptualization, on the other hand, broadly refers to the way learners self-report their emotions, which may vary greatly according to a number of theoretical and technical aspects, such as the dimensional, discrete, or combined approach to emotion measurement/reporting. Depending on how emotion is conceptualized and, by extension, planned for self-report, the intra-personal awareness of emotion may be more or less guided, inducing learners to pay attention to some elements of their emotional experience that they will not necessarily consider in the same way, or with the same weight, depending on the specifics of the EAT at hand.

At the same time, emotion self-report also depends on the learners' ability to identify their current emotional episodes and provide an accurate representation of them [@erbasRoleValenceFocus2015; @smithPatternsCognitiveAppraisal1985; @siemerSameSituationdifferentEmotions2007; @schererDynamicArchitectureEmotion2009]. Erbas and colleagues [-@erbasRoleValenceFocus2015], for instance, found evidence that people who differentiate only between a limited set of discrete emotions tend to use *Valence* alone as a discriminating criterion. On the contrary, people discriminating between a larger panel of discrete emotions focus more thoroughly on different features of the situation at hand (see also Chapter \@ref(defining-emotion-unit)). An EAT may therefore impinge learners to reflect on a greater variety of potential emotional experiences rather than reinforcing the *positive/pleasant* vs*. negative/unpleasant* loop.

Finally, the EAT may influence learners to focus only on the emotional experience *per se*, or also provide contextual information about the situation or potential antecedents of that experience at various level of details [@feidakisProvidingEmotionAwareness2014; @lavoueEmotionAwarenessTools2020]. The richer the details and the reflection to account for them, though, the greater the effort for learners to concentrate and report about the affective component.

To sum up, the presence and features of an EAT may not only contribute to determine whether learners will pay attention to their emotional experience, but also how they will be guided to think about it, planning the way to the next passage.

### From Intra-Personal Emotion to Meaning-Making

Once an emotion or a set of emotions are self-reported, it is assumed that the learner may extrapolate meaning from the very emotional information she has provided. In this regard, information may be punctual, for example the last emotion or set of emotions displayed concurrently. Conversely, the system may also implement some form of data aggregation and persistence, which allows to observe the accumulation and the evolution of the emotional experience over a varying time span [@leonyProvisionAwarenessLearners2013; @bersetVisualisationDonneesRecherche2018]. Depending on the way emotion information is graphically represented, emotional meaning-making can be guided by the system, which may privilege some form of representation (*e.g.*, changes over time) over another (*e.g.*, cumulative frequency of the same emotion expressed).

Meaning-making is also tightly related to the way emotion is expressed through the system (passage #1), not only from a technical standpoint -- since the information available is determined by what information is inserted into the system -- but also conceptually. If emotion is considered from a dimensional standpoint, learners will extrapolate meaning from the criteria through which dimensions are rated. On the contrary, in a discrete emotion approach, learners extrapolate meaning from an *information unity* that is supposed to provide unique meaning compared to other possible choices. If the system combines dimensional and discrete approach, learners can extrapolate meaning from both. Moreover, if contextual information is provided, learners can situate emotion more easily; otherwise, inferences on causes and consequences of behavior must be drawn from representations of emotion alone [@vankleefInterpersonalDynamicsEmotion2018].

The form through which emotional information is rendered on screen also influences the cognitive effort necessary to process it [@hegartyCognitiveScienceVisualspatial2011]. According to complexity and richness of the graphical rendering, processing can span on a continuum with *almost effortless perception* on the one side, and *deliberate cognitive effort* on the other.

To sum up, the design of the EAT determines the way by which the learner extrapolates meaning from her own emotional information and therefore influence to what extent the learner has interest in monitoring what she has inserted into the system.

### From the Learning Activity to Inter-Personal Emotion

Passage #3 determines how the learner becomes aware of emotional information provided by others. It therefore starts a complementary path with respect to passages #1 and #2, but from an inter-personal perspective. This path may in fact be *closed* when the EAT is exclusively concerned with emotion self-awareness [*e.g.*, @lavoueEmotionAwarenessTools2020], whereas it becomes an integral part when the EAT is inspired from a Group Awareness Tool perspective [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @feidakisProvidingEmotionAwareness2014; @molinariEmotionFeedbackComputermediated2013].

As for the intra-personal passage #1, the presence of the EAT can have, in the first place, an alertness effect. The tool may prompt the learner to pay more attention to her colleague(s) emotions of what she would have done without the presence of an EAT, as suggested by the results of @eligioEmotionUnderstandingPerformance2012 and @molinariEmotionFeedbackComputermediated2013 outlined in the empirical works of Section \@ref(ea-inter-personal). In this regard, the design of the EAT may determine to what extent emotion information about others is readily available during the learning activity, as with the EAT adopted my Molinari and colleagues (*ibid.*), or must be voluntarily sought after in another screen, as in the case of the *emot-control* in Feidakis and colleagues [@feidakisProvidingEmotionAwareness2014; @feidakis2013]. The closer the emotional information is to the task, the greater may be the chances to frequently notice and pay attention to it. At the same time, considering that affective stimuli often are bestowed precedence [@poolAttentionalBiasPositive2015; @broschPerceptionCategorisationEmotional2010] in cognitive processing, the ongoing availability of emotional information may also be perceived as disruptive by the learner.

To sum up, the features of an EAT can determine the frequency and effort for perceiving-monitoring the emotional information provided by other learners sharing the same computer-mediated learning environment. A useful EAT may therefore not be enough if paying attention to this information may be perceived useless or exceed learners' cognitive resources at a given time during the task.

### From Inter-Personal Emotion to Meaning-Making

Passage #4 consists in extrapolating meaning from the emotion of other learners sharing the same computer-mediated learning environment and whose emotions have been monitored by the learner herself. This passage can be illustrated through a series of questions the learner is likely to ask herself.

The first question may be: do I understand the emotional experience of my colleagues [@eligioEmotionUnderstandingPerformance2012; @vankleefInterpersonalDynamicsEmotion2018; @hallSocialPsychologyPerceiving2018]? This can be considered in two ways. First, in absolute and non-contextual terms. For instance, what does it mean to be *confused*, *happy*, *ashamed*, ... for someone else? Or what does it mean to be high vs. low on the *Valence*, the *Arousal,* ... dimensions for someone else? Is it the same as for me or not? Second, in relative and contextual terms. That is, what does it mean that my colleague is *confused*, *happy*, *ashamed*, ... in this specific situation? Or what does it mean that my colleague is high vs. low on the *Valence*, the *Arousal*, ... dimensions in this specific situation?

A subsequent questions may consists in wondering: how does the emotional experience of my colleagues impact their behavior? As seen in the inter-personal function of emotion in Section \@ref(ea-inter-personal), causes and consequences of behavior can be inferred from emotion in others. This passage does not only require the ability to discriminate between different emotional experiences in the abstract and in the relative terms, but also to *attach* potential effects on the learning task at hand.

The design of an EAT can intervene in this passage even more preeminently compared to passage #4 from the inter-personal perspective. In that case, the learner herself can be more confident in extrapolating meaning making from her own emotions, since she is the one who has both experienced and decided to inject them into the system. It is therefore safe to assume the learner can come closer to the *intended* meaning, since she disposes of background knowledge. On the contrary, inter-personal meaning making may vary greatly based on the amount of background knowledge, depending for instance, on how acquainted the learner is with her colleagues [@hareliWhatEmotionalReactions2010; @hallSocialPsychologyPerceiving2018]. In the extreme case of no acquaintance, as in empirical settings, the information available through the EAT may be the only *explicit* emotional information available, for as suggested by @janssenCoordinatedComputerSupportedCollaborative2013 in Section \@ref(mutual-modeling), socio-affective cues can also be derived *implicitly* by the content/collaborative space of the learning task at hand (see Figure \@ref(fig:janssen-bodemer-framework)).

Finally, as stated by EASI model depicted in Section \@ref(ea-inter-personal), emotion meaning-making from an inter-personal stance depends on the learner's motivation to try to decipher the emotional information at hand. The model also suggests that this may be further influenced according to how *appropriate* the learner perceived the emotions of others: the more inappropriate the emotion, the less effort may be put in extrapolating meaning-making from it.

To sum up, extrapolating meaning-making from emotion in others require the alignment of many factors. In some of them, the specific features of the EAT may have an helper function, whereas in others, the effort and motivation of the learner can not be by-passed.

### From Intra-Personal Emotion to Inter-Personal Emotion

Passage #5 corresponds to the acknowledged and voluntarily disclosure of one's own emotional experience to others. This passage only partially overlaps with passage #1. In fact, depending on the configuration of the EAT or the learning activity, there may be situations in which emotions are expressed by learners but not shared with others, as with the use of the EMORE-L in @molinariEMORELOutilReporting2016. One possible feature of an EAT may therefore consists in making a difference between an *expressed-per-se* versus an *expressed-per-alii* emotion.

When an emotion is knowingly shared with others, all the factors linked to the social sharing of emotion come into play [@rimeEmotionElicitsSocial2009; @parkinsonCurrentEmotionResearch2015; @parkinsonInterpersonalEmotionTransfer2011; @fischerSocialFunctionsEmotion2016]. These may include strategic, meta-cognitive factors. For instance, @vankleefEmotionInfluence2011 posit that one of the primary inter-personal function of emotion is to engender social influence, that is, modify the behavior in others by harnessing the *power* of affective phenomena. If one expresses *joy* after solving a problem, this may convey a more reassuring message about the accuracy of the solution compared to a manifestation of *boredom* or *stress*.

In a collaborative setting, the emotion may also target the content space or the relational space of the learning activity [@janssenCoordinatedComputerSupportedCollaborative2013], as well as any *category* of emotion implicated in learning contexts: achievement, epistemic, topic or social emotions [@pekrunIntroductionEmotionsEducation2014]. Strategically, learner can attempt to target the disclosure of her emotional experience exclusively or preeminently toward a space or *category* of emotions, intentionally avoiding others. For instance, one can disclose frustration towards the learning material, but inhibit the same kind of manifestation towards the quality of interaction with her colleagues.

An unavoidable topic on explicit and voluntarily self-disclosure of emotion is that of emotion authenticity [@salmelaWhatEmotionalAuthenticity2005; @desousaRationalityEmotion1987; @sep-emotion], which often intertwines with concepts such as spontaneity, sincerity, appropriateness, and the like. From the point of view of an emotion-as-interaction approach defended in this contribution, a naive interpretation of emotion authenticity is not a necessary requirement for emotional awareness to emerge. If one *really* feels *angry*, express *anger* whereas she actually feels *frustration*, or express *anger* as a strategic signal -- "*be aware, this can make me (or someone else in the group) angry"* -- they all comes down to the same symbolic element to manipulate in self-regulation, co-regulation or socially shared regulation of learning. On the other hand, if emotional expression is voided of any *true* affective experience, then it becomes only a codified system of signals according to the definition of the emotional terms adopted [@scarantinoDonGiveBasic2011; @scarantinoHowThingsEmotional2017].

Even though a voluntarily self-report EAT can not even scratch the surface of the complexity of emotion authenticity in the sense of correspondence *to the truth*, its defining features can make *internal authenticity* more or less manifest. For instance, if one is expected to express only a discrete emotion term, the *internal authenticity* reposes on only one unit of information. If one has to provide dimensional evaluation of some form of criteria, plus the discrete emotions term, the *internal authenticity* is augmented by the synergy of more than one piece of information. In other words, to *fake* *anger*, one has to go through more trouble in expressing a cohesive message that denotes what *anger* should be in a *truly* *angry* agent. By contrast, if we believe in learners' good-faith, the features of the EAT may precisely help them make the message more *authentic*, in the sense that it better conveys the felt experience. In this regard, an EAT that provides learners with a better experience in expressing how they feel could foster emotion disclosure even in reluctant learners.

To sum up, the disclosure of emotion, which technically means that an emotion from intra-personal becomes inter-personal on the EAT interface, can be sustained by an EAT at least by making the experience of expressing an emotion as valuable as possible. This mainly means providing learners with the possibility to *encode* in an expressed emotion information that they consider to have strategic potential.

### From Inter-Personal Emotion to Intra-Personal Emotion

Passage #6 concerns emotions that from the inter-personal level pass to the intra-personal level. This segment can be interpreted mainly by two processes: (1) the comparison between one's own emotions and that of others (which may have been grouped also with the previous segment); and (2) a form of emotional contagion, affective reaction or meta-emotion [@barsadeRippleEffectsEmotional2002; @miceliMetaemotionsComplexityHuman2019; @vankleefSocialEffectsEmotions2017] that entails from knowing the emotion of others.

Emotion comparison consists in assessing the extent by which the emotion experienced by others are similar or different from the one experienced by the learner herself. As mentioned above @fischerSocialFunctionsEmotion2016 identify two major social functions of emotion: affiliation and distancing. Direct emotional comparison can thus sustain these fundamental functions. As it is the case with the assumption that *positive* emotions lead to learning and *negative* emotions do not, though, an equation stating that similar emotions in oneself and others lead to affiliation, whereas different emotions lead to distancing seems oversimplified. In a learning group where everybody feels constantly *irritated* by each others, learners would be more distanced than affiliated. On the contrary, a group where some learners are *stressed* and other are *confident* may lead to a reciprocal affiliation: confident learners can calm down stressed ones, whereas stressed ones can make the others less confident and more attentive to the requirements of the assignment.

Emotional comparison in itself can be performed from an analytic point of view, without triggering any affective consequence. At the same time, the emotion in others can represent triggers for the emotion in oneself, directly or indirectly through the comparison itself. As stated by theories such emotion contagion [@barsadeRippleEffectsEmotional2002], affective reactions [@vankleefSocialEffectsEmotions2017], or meta-emotion [@miceliMetaemotionsComplexityHuman2019] already mentioned above, emotion in others can elicit emotions in oneself. In addition, the comparison in itself can be an emotional trigger. For instance, a *proud* colleague may not in itself trigger an affective reaction, but it may do so if the learner compares with her own *disappointment*. The comparison can at this point produce *jealousy* or *bitterness* depending on, for instance, whether the *pride* is considered deserved or not.

The transition from the inter-personal to the intra-personal emotion can then flows in the intra-personal or inter-personal meaning making according to the perspective that is conferred to the information by the learner. The learner can be more interested in asking herself *what does it mean for me that the others have such and such emotion?*, taking or not into account that she has -- also or instead -- such and such emotion. On the other hand, the learner can be more focused on asking herself *what does it mean to others that they have such and such emotion, when I -- also or instead -- have such and such emotions?*. This interplay can be particularly important in collaborative settings, where this mutual-modeling process is considered an integral part of learning [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009], as mentioned in Section \@ref(mutual-modeling).

To sum up, the specific features of an EAT may guide learners in positioning themselves with respect to the emotion of others. How emotion is represented can accentuate the differences or the commonalities in the emotional experience in oneself and in others, as well as sustaining to different extent the reciprocal representation that learners have of each others. Some form of direct and persistent comparison can also be more prone to entail affective reactions due to the comparison itself.

### From Meaning-Making to the Learning Activity

Finally, passage #7 coalesces all the other passages and goes back from the overarching meaning-making step to the learning activity. This is the passage that can have even more more intervening factors compared to the previous ones. As already mentioned, contrary to affect-aware systems -- where some form of intelligence may intervene and guide this passage -- with an awareness tool learners are bestowed with the full responsibility of fruitfully integrating the information into the task at hand. One simplified way to depict this passage is by pointing out three possible scenarios of how emotional awareness may be concretely implemented into the learning activity and, by extension, within learning processes and outcomes. Before describing the passages, though, it is important to recast that the model depicts iterative cycles in the use of an EAT: it is not the assessment of a comprehensive and definitive outcome. As the learning activity progress, there may be a number of cycles of one type, followed by cycles of another type, and so on. Each successive cycle takes into account what meaning the learner has extrapolated by combining information from emotional awareness specifically and all the other significant cues she may have retained.

The first scenario is that no meaning-making is extrapolated from emotional awareness, which entails that the course of action in the learning activity is unaffected by the use of the EAT. This may be due to a number of reasons, not all of them stemming from the inefficacy of the EAT or emotional awareness more generally. One obvious reason consists in the fact that no (new) emotion has been triggered since the last cycle. One of the reasons emotions are considered an important phenomenon is that they do not occur at all time, but rather when crucial events are at stake. Another reason for a *blank* iteration is that the available (new) information is ambiguous or unclear, so that the learner prefers to wait and not steer the course of action based on what she considers insufficient evidence. Yet another reason is that the learner has no immediate interest in wondering how the learning activity could be adapted based on the (new) emotional information available. She may be too focused on some other activity that at this particular moment may be blocking both the integration of the available information and the production or consumption of new emotional information.

The second possible scenario is that the learner decides to adapt the learning task by explicitly targeting some form of emotion regulation [@grossEmotionRegulationCurrent2015; @zakiInterpersonalEmotionRegulation2013; @hoffmannTeachingEmotionRegulation2020; @reeckSocialRegulationEmotion2016]. A modulation of the course of action is produced to attain specifically the emotional experience and not, for instance, the topic at hand or the negotiation of learning goals. As mentioned in the current and previous chapter, emotion regulation can apply at the individual or social levels (*ibid.*). For instance, after expressing an emotion and extrapolating the information that this very emotion is assessed as inadequate to the learner's goal, she may use one of the emotion regulation strategies (*i.e.*, situation selection, situation modification, attentional deployment, cognitive change or response modulation) to act upon her own emotional experience [@grossEmotionRegulationCurrent2015]. If the relevant emotional information comes from another colleague sharing the same computer-mediated environment, the learner can decide to use it as a reference for her own behavior [@vankleefInterpersonalDynamicsEmotion2018; @hareliEmotionsSignalsNormative2013] or to apply emotion regulation at the inter-personal level [@reeckSocialRegulationEmotion2016].

The last possible scenario presented here consists in a wider form of regulation of learning, identified in the triad self-regulation, co-regulation and socially-shared regulation of learning [@jarvelaEnhancingSociallyShared2015; @jarvelaSociallySharedRegulation2016; @millerScriptingAwarenessTools2015; @winneWhatStateArt2015], illustrated in Section \@ref(socio-affective-competences). In a sense, emotion regulation in learning settings can be viewed as a particular form of these wider strategies, which extend beyond the emotional experience. The (new) emotion information obtained through the EAT and processed through the overarching meaning-making process can be integrated into the learning task at various levels. A learner can for instance use the emotion of others as a feedback on its own contribution to a common task, duplicating the effort if those emotions denote appreciation, or assuming a more passive stance if those emotions question the quality of the input provided. The fact that the model is cyclical does not yet prevent learners to try to establish patterns in their regulatory processes. For instance, a learner could realize that she has a tendency to attribute to others emotional reactions that are diminishing of her own work [@schererEvidenceExistenceEmotion2021], when in fact this is not supported by the information available through the EAT. This could lead the learner to keep this *bias* in mind and eventually work on it over time.

To sum up, this last passage represents the core of emotional awareness as instrumental to learning processes and outcomes. It defines whether endowing computer-mediated learning environments with emotional awareness can create a virtuous circle consisting in the emergence and processing of meaningful emotional information, which is then translated in concrete elements related to the learning activity. As proposed in the description of this last passage, this translation can be viewed as the alternation of of three possible scenarios: the (new) emotional information has no impact, it impacts emotion regulation, or it impacts learning regulation more broadly. Throughout the model, emotional information has been preceded by the optional (new) prefix. This was meant to consider that, in case of persistence of emotional information over time, learners can also reassess past experience under new perspectives. For example, when looking at how their emotions have evolved during a semester, learners could assess that all the *confusion* and *frustration* they have been living was worth the trouble, considering their sentiment of competence acquired at the end of the course. The *confusion* and *frustration* are not new emotional information in itself, but it becomes *new* in light of how events have turned out.

## Use of the Abstract Model to Define Research's Aims

As stated in the introduction of this chapter, one of the aim of the model is precisely to evoke a sort of *domino effect*: all the techno-pedagogical pieces must align for emotional awareness to be effective. At the same time, it is possible to break down the whole mechanism in what may be more manageable parts to deal with, especially from the point of view of research in the domain.

The model can thus be adopted as a guideline to think about and communicate what part of emotional awareness is (primarily) aimed by a contribution. This does not mean that the whole process is beyond reach, but it presupposes that for studying the whole process, more *pieces* must be accounted for -- theoretically, technically and/or empirically. For instance, the thesis aims at investigate consistently passages 1 and 3, with 5 and 6 also implicated from a mutual-modeling perspective. On the other hand, passages 2 and 4 are considered with a more exploratory outlook. Finally, passage 7 is considered beyond the thesis's reach altogether.

By breaking down the all process, it is also possible to pinpoint what causal mechanism can be responsible for the outcome of that specific part(s). For instance, self-report can be instrumental in passages 1 and 3, by forcing learners to reflect on their own emotional experience. It also play a prominent role in passage 5, for it underpins the disclosure of the emotional information to others. The implementation of an emotion structure is a good example of an overarching causal mechanism, which is meant to mediate or moderate the whole gamut of sub-processes implicated in emotional awareness. In this regard, the next chapter of the thesis will define precisely what is meant by an emotion structure, and how this structure can intervene in producing and consuming emotional awareness. Furthermore, the empirical contributions of Part III will refer to the abstract model to explain their specific aim, so more concrete examples will be provided.

## Summary

This chapter introduced the concept of emotional awareness, around which emotion awareness tools are built. From the broad definition of the concept, three underlying assumptions have been derived. The first two assumptions concern the role that emotion play at the intra-personal and the inter-personal levels respectively. The third assumption posits that emotional awareness can be fruitfully conveyed in a computer-mediated learning environment, by harnessing for instance the power of computational devices. For each assumption, an overview of theoretical and empirical work has been provided, in an attempt to sketch the complexity and variety of approaches on the subject. In order to break down this heterogeneity in a reasoned organisation, an abstract model of emotional awareness, consisting in seven integrated passages, has been outlined. The abstract model can be used as an instrument to think and communicate about research on the topic. In this regard, the specific aims of the thesis have been used in a preliminary way to illustrate the model.

[^1]: <https://moodmeterapp.com/>
