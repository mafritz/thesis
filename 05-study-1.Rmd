# Emotion Awareness in Synchronous and Collaborative Settings

```{r s1-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

# Load the relevant data and graphs from the study-1 folder
source(here("data", "study-1", "s1-export.R"))
```

This chapter illustrates the first empirical contribution of the thesis, which aims at investigating the use of an Emotion Awareness Tool (EAT) in a synchronous and collaborative distance learning setting. In such a setting, learners share the temporal -- but not the spatial -- dimension, and also share a common learning task that must be achieved through some form of joint activity. A research field that has been investigating these learning conditions at large is Computer-Supported Collaborative Learning (CSCL) [@dillenbourgEvolutionResearchComputersupported2009; @dillenbourgWhatYouMean1999; @stahlComputersupportedCollaborativeLearning2006; @suthersTechnologyAffordancesIntersubjective2006] -- even though CSCL is not limited to distance learning, but also investigates the use of computational devices in co-located conditions. The field has, from the very beginning in the early '90, acknowledged the pivotal role of instructional designs and tools, which foster learners to build and maintain a shared understanding of the learning task [@dillenbourgSymmetryPartnerModelling2016; @roschelleConstructionSharedKnowledge1995; @suthersTechnologyAffordancesIntersubjective2006]. Whereas this shared understanding was originally driven mainly from a communicative point of view [@clarkGroundingCommunication1991], it has later been extended to encompass a variety of sources of information, such as what a person does, thinks, wants, or even feels [@bodemerGroupAwarenessCSCL2011; @dillenbourgSymmetryPartnerModelling2016; @engelmannKnowledgeAwarenessCSCL2009; @kirschnerAwarenessCognitiveSocial2015; @sanginFacilitatingPeerKnowledge2011]. One of the fundamental assumptions of CSCL is, in fact, that these cues are instrumental in two processes: (1) for a learner to build and maintain a holistic representation of the colleagues with whom she shares the learning activity; and (2) for the very same colleagues to possess sufficient information to build and update a holistic representation of the learner herself. Dillenbourg and colleagues [-@dillenbourgSymmetryPartnerModelling2016] define the former as *partner modeling* -- that is "the process of inferring one's partner's mental states" (*ibid*, p. 230) -- and the latter as *mutual modeling*, that is, *bi-directional* partner-modeling, which may happen at various degrees (*e.g.* Jane thinks Paul knows that she did not understand the assignment).

It is posited, in fact, that the effort learners put to build a symmetrical representation of the partners in a collaborative task is a pivotal determinant of learning processes and, by extension, outcomes [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginFacilitatingPeerKnowledge2011]. In this regard, is useful to point out two elements. First, symmetry does not mean that all the implicated learners must do, think, know, or feel the same thing. On the contrary, collaborative efforts are inherently characterized by fluctuations between socio-cognitive tensions and relaxations, which are instrumental to the learning process [@andriessenSociocognitiveTensionCollaborative2011; @jarvelaSociallySharedRegulation2016; @winneWhatStateArt2015]. Second, it is worth noting that, unlike other similar concepts implicating interpersonal accuracy [see @hallSocialPsychologyPerceiving2018 for a recent and integrative overview], mutual-modeling does not have to be persistently and precisely accurate, because that would require an effort that may take resources away from the learning activity [@dillenbourgSymmetryPartnerModelling2016]. Mutual-modeling is therefore particularly important at times when there are events of major concern for learners, who may benefit from cues about the suitability of persisting with or changing their behavior -- a process that incidentally shares many commonalities with eliciting events in emotions [@dmelloDynamicsAffectiveStates2012; @jarvenojaRegulationEmotionsSocially2013; @linnenbrink-garciaAdaptiveMotivationEmotion2016; @schererWhatAreEmotions2005].

CSCL literature provides broadly two non-mutually exclusive strategies to foster, among other phenomena pivotal to the effectiveness of CSCL, partner- and mutual-modeling: scripting and awareness tools [@millerScriptingAwarenessTools2015]. The first consists in accurately scaffolding the learning activity to introduce tasks through which learners can share their thoughts, doubts, experience, and so on [@dillenbourgOverscriptingCSCLRisks2002; @fischerScriptTheoryGuidance2013]. The second, which is more relevant to the present contribution, consists in the implementation of awareness tools, through which instrumental information about learners taking part in the joint activity is integrated into the CSCL environment, often alongside the technological artifact used to produce the outcome of the learning task [@bodemerGroupAwarenessCSCL2011; @buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013; @kirschnerAwarenessCognitiveSocial2015].

As mentioned in previous chapters, there is a growing consensus in considering that awareness tools should go beyond the face-to-face golden standard -- that is, they should not try to provide an *ersatz* of all the verbal and para-verbal cues that are available in a co-located interaction [@buderGroupAwarenessTools2011; @kirschnerAwarenessCognitiveSocial2015]. On the contrary, awareness tools should rather provide information, particularly socially-oriented, which is not available -- or is more difficult to access, process, or remember -- in face-to-face settings. In this regard, even if emotions can be derived from verbal and para-verbal cues in face-to-face interaction, they require a person to be focused on her colleague's efferent manifestations (e.g. facial expressions, prosody, etc.), which is not always possible, especially when performing, in the meantime, a learning task. As a consequence, computer-mediated interaction may benefit from an alternative and dedicated way to convey emotional information [@cerneaSurveyTechnologiesRise2015; @derksRoleEmotionComputermediated2008; @leonyProvisionAwarenessLearners2013; @parkinsonEmotionsDirectRemote2008]. As illustrated in previous chapters of the thesis, emotions provide useful information both at the intra- and inter-personal levels [@grandjeanConsciousEmotionalExperience2008; @levensonIntrapersonalFunctionsEmotion1999; @schererWhatAreEmotions2005; @vankleefEmergingViewEmotion2010; @vankleefInterpersonalDynamicsEmotion2018]

In this empirical contribution, an EAT is therefore implemented into a computer-mediated collaborative setting as a mean to improve computer-mediated collaboration, which has already been attempted in contributions discussed at length in previous chapters. As a reminder, for instance, Eligio and colleagues (2012) compared the effect of emotional awareness -- even though they a form of *scripting* rather than *awareness tool* strategy -- in computer-supported collaboration for pairs of women in co-located or remote conditions, either with or without emotional awareness. They found evidence that emotional awareness improves performances in both settings. Furthermore, in the remote condition, participants also understood better the emotions of their partners, as well as experienced more positive affects compared to the co-located condition. Molinari and colleagues (2013) compared same-sex pairs collaborating through an argumentation tool in two conditions, one with an EAT and the other without, and found evidence for a beneficial effect on collaboration, even though only for pairs composed by women.

The present study builds and extends on these contributions, but with some important differences. First, the study will focus on the use of the EAT rather than on its effects on learning processes and outcomes. Second, consistently with suggested practices in the assessment of awareness tools, all participants to the study will be provided with an EAT -- declined in three different versions -- rather than having a control group without emotional awareness and an experimental group with emotional awareness. Finally, the collaboration will be pretended -- with participants unaware of the manipulation -- rather than effective. This choice serves a double purpose. On the one hand, it guarantees that all participants will be exposed to the same stimuli about the *content* of the collaboration, controlling thus sources of variance dependent on interactions between participants or the quality of the learning task. On the other hand, the use of a *unique and consistent collaborator* control for gender-related effects, which neither this study, nor the overall thesis is interested in pursuing.

More specifically, this first empirical contribution investigates the reasons why a learner may be interested in using an Emotion Awareness Tool (EAT) with respect both to the emotional information it prompts to share and the information it provides with. In this intent, three different interfaces of an EAT will be compared, which differ in how socially-oriented they are by varying (1) whether the emotional information will be disclosed to the partner or remain visible only to the learner herself; and (2) whether the learner has access to her own emotional information, that of the partner, or both. Using a randomized trial and controlled environment, in which participants -- unaware of it -- will collaborate with a simulated partner in a joint problem-solving tasks, performance-based indicators of the use fo the EAT will be collected: number of emotions expressed for the interest in sharing emotions, and eye-tracking measures for the interest in seeking and processing emotional information. In comparing the three interfaces, it is posited that the more socially-oriented the interface is, the greater use of the EAT in expressing, seeking and processing the emotional information will be done.

The study also provides corollary analyses linked to *dynamic* and *moment-to-moment* use of the EAT during a computer-mediated task, such as the transitions of users' gaze between different zones of the interface and the evolution of the emotional episodes expressed by participants over time. Finally, taking advantage of the fact that the experiment uses the same experimental task as in Fritz [-@fritzReinventingWheelEmotional2015], an internal meta-analysis is also provided to assess reliability of measures as reference for future implementations.

## Study Overview

The moment-to-moment use of a voluntary self-report EAT implies that both the expressing-displaying and the perceiving-monitoring functions of an awareness tool [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002] require an effort that is not directly implemented in the content space, but is limited to the relational space of the task [@janssenCoordinatedComputerSupportedCollaborative2013; @dillenbourgSymmetryPartnerModelling2016]. As pointed out in the general theoretical framework of the thesis, expressing and perceiving emotions require cognitive effort, especially when (1) emotions are expressed deliberately and at a conceptual level, as stated by the Component Process Model theoretical framework [@sanderAppraisalDrivenComponentialApproach2018; @schererDynamicArchitectureEmotion2009], and (2) perceived emotions are linked to inferential processes about causes and consequences of behavior, as stated by the Emotion As Social Information (EASI) model [@vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018]. One of the pivotal aspects of voluntarily, self-reported emotional awareness is thus whether learners are keen to make this effort and under which conditions they are more prone to dedicate some of their cognitive resources for an activity outside the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013; @pashlerDualtaskInterferenceSimple1994].

More broadly, this phenomenon relates to one of the fundamental questions in theory of emotion: *why do people express and share emotions* [e.g., @darwinExpressionEmotionsMan1872; @fischerSocialFunctionsEmotion2016; @vankleefInterpersonalDynamicsEmotion2018]? Two main positions on the matter are relevant to the context of emotional awareness provided during a computer-mediated collaborative task.

On the one hand, there is an intra-personal perspective which is, for instance, represented by the concept of *affect labeling* [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018], according to which putting feelings into words is a form of emotion self-regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], which may even be implicit, since the person may not realize the regulation is taking place. Similarly, one of the main tenet of the RULER approach [@brackettRULERTheoryDrivenSystemic2019; @nathansonCreatingEmotionallyIntelligent2016] -- and emotional intelligence [@mayerAbilityModelEmotional2016; @saloveyEmotionalIntelligence1990] or competence [@schererComponentialEmotionTheory2007] more broadly -- resides in the individual ability to appraise the situation and recognize her own emotional episodes to better cope with the situation. Expressing emotions would therefore help learners to ask themselves how they are feeling by evaluating the situation [@erbasRoleValenceFocus2015; @hoffmannTeachingEmotionRegulation2020; @lavoueEmotionAwarenessTools2019] and finding the corresponding subjective feeling that best depict their emotional state [@grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009]. Applied to the computer-mediated collaborative context, learners could therefore use an EAT because it provides an affordance to differentiate and reflect about how they are feeling in a self-centered process [@boehnerHowEmotionMade2007], aimed at regulating the emotional episodes elicited by the socio-cognitive conflicts of the learning task [@andriessenSociocognitiveTensionCollaborative2011; @arguedasAnalyzingHowEmotion2016]. As a result, learners can benefit from emotional self-awareness, even in the absence of communication with the partner.

On the other hand -- even if the two positions are not mutually exclusive -- many researchers advocate that emotions have pivotal inter-personal functions [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018]. For instance, Rim√© [-@rimeEmotionElicitsSocial2009] posits that an individualistic view of emotion and regulation is untenable, and that the usefulness of emotions resides in the social sharing of them. According to this view, a person shares her emotions to make others aware of her emotional state as a way to gain social attention, arouse empathy, stimulate bonding or strengthen social ties (ibid.). Parkinson [-@parkinsonInterpersonalEmotionTransfer2011] introduces the concept of *social appraisal*, according to which emotions in others can serve as useful information to modify our own behavior. An integrated framework of the inter-personal function of emotions is posited by Van Kleef in the Emotion As Social Information (EASI) model [@vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], according to which emotional expression can modify behavior in others through affective reactions or inferential processes about elicited emotions' causes and consequences. For instance, the emotion of a person can represent a trigger for an emotional episode in another person, a phenomenon also known as meta-emotion [@miceliMetaemotionsComplexityHuman2019]. Applied to the computer-mediated collaborative context, thus, learners can therefore use the EAT to sustain the mutual-modeling activity to foster collaborative learning [@dillenbourgSymmetryPartnerModelling2016], especially in the absence of para-verbal cues that are usually available in face-to-face settings [@derksRoleEmotionComputermediated2008]. Learners can benefit from emotional awareness by adapting their behavior according to the crossed-over emotional information available, for instance by engaging in inter-personal emotion regulation [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013].

In the few studies that have investigated emotional awareness in computer-mediated collaboration so far [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013], participants who disposed of emotional awareness (*e.g.*, those in the *treatment* group) had access both to their own emotions and that of the partner, knowing beforehand that the emotional information would be crossed-over. This ecological situation is consistent with the mutual-modeling perspective [@dillenbourgSymmetryPartnerModelling2016], according to which the symmetry of the information available to learners is instrumental to build and update a holistic representation of the partner, upon which the collaborative effort can strive. It is nevertheless possible to break down this *full* use of an EAT by varying (1) the sender of emotional information provided, and (2) the recipient of the emotional information expressed [@vankleefInterpersonalDynamicsEmotion2018]. As suggested by Buder [-@buderGroupAwarenessTools2011], varying the characteristics of the same awareness tool -- rather than comparing a *control* group without the awareness tool and a *treatment* group with the tool -- allows a better assessment of its contribution. For instance, by varying the sender and the recipient of the emotional information, three different interfaces of the tool can be obtained:

-   *Self-Centered*: the learner can access only her own emotions, and the emotions she expresses are not conveyed to the partner;
-   *Partner-Oriented*: the learner can access only the partner's emotions and the emotions she expresses are conveyed to the partner, even though she can't access them herself;
-   *Mutual-Modeling*: the learner can access both her own and the partner's emotions, which provide direct and persistent comparison fostering symmetry in the emotional information shared by both partners.

The three versions of the EAT imply different reasons why a learner may be keen to use the EAT to express her own emotions, as well as to seek and process the emotional information provided by the tool. In fact, the interfaces mobilize different theoretical concepts associated with an intra- or inter-personal use of emotional awareness (see Figure \@ref(fig:s1-theory-diagram-figure) for a graphic representation). The *Self-Centered*, linked to the intra-personal function of emotions, activates concepts such as affect labeling, appraisal and self-regulation; the *Partner-Oriented*, related more towards an inter-personal perspective, implies meta-emotions, social-regulation, and social-sharing; and, finally, the *Mutual-Modeling* combines both perspectives by adding symmetry in the emotional information available to both partners. Comparing the three versions of the EAT will thus contribute to determine whether the more *socially-oriented* interface yields a more thorough use of emotional awareness through the EAT.

(ref:s1-theory-diagram-caption) Theoretical concepts mobilized by versions of the EAT differing in the use of, and access to emotional information.

```{r s1-theory-diagram-figure, fig.cap="(ref:s1-theory-diagram-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-theory-diagram.png"))
```

## Research Question and Hypothesis

The phenomenon under scrutiny in the present study is therefore whether a different use of, and access to emotional information elicit a different use of the EAT in terms of its expressing-displaying and the perceiving-monitoring functions [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. More specific hypotheses are stated with respect to each function.

### Use in Expressing Emotions

For expressing-displaying emotions, the three interfaces provide the learner with different reasons to express how she feels, as well as different affective triggers that may elicit emotional episodes (see also below the hypothesis about the use in perceiving emotions). More precisely:

-   With the *Self-Centered* interface, the learner knows she is the only recipient of the emotions she expresses. Therefore, if she decides to use the EAT to express an emotion, she probably does it out of self-interest, possibly linked to self-regulation as stated by the *intra-personal* perspective. In the meantime, the EAT does not provide any additional information about the partner's emotions that may serve as trigger for emotional episodes in the learner herself.
-   With the *Partner-Oriented* interface, the learner knows she will not have access to her own emotions once she has expressed them, but that these are conveyed to the partner. Therefore, if she decides to use the EAT to express an emotion, one can assume that she does it from an *inter-personal* perspective (even if the possibility that she does it exclusively in a *Self-Centered* perspective cannot be excluded). In the meantime, the learner can also access the partner's emotions, which may represent additional triggers for meta-emotional episodes (*e.g.*, *Jane expresses guilt because she thinks Paul has just expressed anger as a result of something she has done*).
-   With the *Mutual-Modeling* interface, the learner knows the emotions she expresses are available both to her and the partner. Therefore, if she decides to use the EAT to express an emotion, she does in a *Self-Centered*, *Partner-Oriented*, or a combined perspective. In the meantime, the learner also disposes of direct and persistent comparison between her own emotions and that of the partner, which may also represent an additional trigger for emotional episodes compared to the *Partner-Oriented* interface (*e.g.*, *Jane expresses relief because she saw from the interface that in the last few minutes both she and Paul were often confused*).

Hypothesis (*H1*) is therefore stated in the following terms: there will be an overall difference in the use of the EAT for expressing-displaying emotions depending on the interface the learner has at disposal. More specifically, in comparing the interfaces, a greater use of the expressing-displaying function of the EAT in the *Partner-Oriented* and *Mutual-Modeling* interfaces compared to the *Self-Centered* interface would corroborate an *inter-personal* interest in expressing emotions. Furthermore, a greater number of emotions expressed in the *Mutual-Modeling* interface compared to the *Partner-Oriented* condition would suggest that the possibility of direct and persistent comparison between one's own and the partner's emotions results in a *surplus* of expression-displaying of emotions. Translated in concrete use:

-   Jane will express more emotions when she knows she is sharing her emotions with Paul compared to when she is expressing emotions only for herself;
-   Jane will express even more emotions when she knows she is sharing her emotions with Paul *and* they can mutually dispose of direct and persistent comparison between their respective emotional episodes.

### Use in Perceiving Emotions

With respect to perceiving emotions, the three interfaces differ in the quality and quantity of the emotional information available on screen. The three interfaces will provide the learner with different reasons to seek and process the emotions expressed during the collaboration:

-   With the *Self-Centered* interface, the learner has access only to the emotions she has expressed over time during the collaboration. This may be interpreted as a *control* condition: what is the interest of having emotional information that the learner is already supposed to know? Seeking and processing the learner's own emotions may be explained by the interest of reflecting on the evolution of her own affective states during the task.
-   With the *Partner-Oriented* interface, the learner has access only to the emotions expressed by the partner, that is, information she does not already know. Seeking and processing the partner's emotions may be explained by the interest in knowing how the other is feeling and/or the evolution of the affective states of the partner during the collaboration.
-   With the *Mutual-Modeling* interface, the learner has access both to her own and the partner's emotions. This condition inserts an additional interest to the previous ones: the possibility of direct and persistent comparison between the learner's own emotions and that of the partner.

Hypothesis (*H2*) is therefore posited as follows: there will be an overall difference in the use of the EAT for seeking and processing the expressed emotions depending on the interface the learner has at disposal. More specifically, the *Partner-Oriented* and *Mutual-Modeling* interfaces will elicit a greater use in perceiving-monitoring emotions compared with the *Self-Centered* interface. Furthermore, greater information seeking and processing in the *Mutual-Modeling* interface compared to the *Partner-Oriented* interface would suggest an accrued interest due to direct and persistent comparison. Translated in concrete use:

-   Jane will seek more often and process the emotional information longer when she can access Paul's rather then her own emotions;
-   Jane will seek even more often and process the emotional information even longer when she can access, for direct and persistent comparison, both her own and Paul's emotions at the same time.

## Methods

I report how I determined the sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

### Participants and Design

48 participants (29 women, 19 men), aged 18 to 55 ($M=37.3, SD=10.01$), voluntarily participated to the study. The sample size was determined by time constraints, since data had to be collected in 15 days. 23 participants were university students from different faculties, both at undergraduate and graduate levels. 25 participants were professionals working for a company adopting distance learning practices. No remuneration was provided for taking part in the study. Participants were randomly assigned to one of the three conditions/interfaces (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*) in order to produce a balanced design with 16 participants per condition.

### Material

#### Overall Interface of the CSCL Task

The experimental material comprises different components; I therefore provide an overview before specifying the various details. Figure \@ref(fig:s1-task-interface-figure) shows the disposition of the screen during the experimental task. It comprises the EAT on the left-side of the screen, outlined in blue, and the simulated, joint-problem solving task on the right. The image indicates what part of the interface was simulated for the *Mutual-Modeling* condition. The interfaces of the other conditions are illustrated below. Some parts of the interface have been translated in English for the current contribution. In the experiment, though, the french language was used consistently for every condition.

(ref:s1-task-interface-caption) Overview of the interface that presents the various part of the material adopted for the CSCL task.

```{r s1-task-interface-figure, fig.cap="(ref:s1-task-interface-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-task-interface.png"))
```

#### Problem-Solving Task

The joint problem-solving task comprised four enigmas taken from a game. The first three enigmas had a clear response that could be inferred, whereas the last one was more of a non-nonsensical type. The same enigmas have been used in a previous study [@fritzReinventingWheelEmotional2015], where they elicited different emotions both in number and kind in a population similar to that of the current sample. Each enigma was subdivided in three phases:

-   40 seconds during which the text of enigma was showed on the interface. At this stage, participants could only express their emotions, but could not write their reasoning or the reply;
-   3 minutes and 20 seconds during which participants could write their reasoning and reply to the enigma, as well as see the *playback* reasoning (but not answer) of the simulated partner;
-   1 minute in which the given answers from the participant and the partner were displayed on screen with the expected solution to the enigma. At this stage, once again, the reasoning and reply fields were not available on screen.

#### Configuration of the Emotion Awareness Tool

The version of the DEW was composed as follows. The Valence and Control/Power dimensions [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005] determined the appraisals for the expression-displaying of emotional episodes. Valence was prompted with the question "*Is the situation pleasant?*", whereas Control/Power with the question "*Is the situation under your control?*". Both evaluations were determined with the extreme negative pole *Not at all*, and extreme positive pole *Yes, absolutely*. The underlying affective space was configured using a radial circumplex with the 20 EATMINT emotions also used in Fritz [-@fritzReinventingWheelEmotional2015]. The composition of the circumplex has been detailed in the previous chapter (add reference here).

For the monitoring/perceiving function of the EAT, each condition differed in the following ways (depicted in Figure \@ref(fig:s1-eat-interfaces-figure)):

-   *Self-Centered*: the interface comprises an emotion time-line, then a graphic line chart that depicts the evolution of the appraisal dimensions over time, and finally a tag cloud where the size of each subjective feelings is proportional to the frequency with which it has been expressed. The information provided is based only on the emotions expressed by the participant herself.
-   *Partner-Oriented*: the interface is the same as in the *Self-Centered* condition, but the information provided is based only on the emotions expressed by the simulated partner (see below).
-   *Mutual-Modeling*: the interface comprises an emotion time-line, but with both the participant and the simulated partner's subjective feelings organized in two different rows. Two line charts complete the interface, one with the appraisal dimensions of the participant, and the other of the partner.

The *Self-Centered* and *Partner-Oriented* conditions presents a tag cloud in order to balance the surface of the EAT that contains information. In this way, the EAT occupies more or less the same amount of the screen.

(ref:s1-eat-interfaces-caption) The three different interfaces used in the experiment. In order from left to right: the *Self-Centered*, the *Partner-Oriented*, and the *Mutual-Modeling* versions.

```{r s1-eat-interfaces-figure, fig.cap="(ref:s1-eat-interfaces-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-eat-interfaces.png"))
```

#### Simulated partner

```{r s1-simulated-partner}
s1.simulated_partner <- s1.dew_configuration$task$simulation %>%
  select(-user) %>%
  filter(time <= 1200) %>%
  left_join(s1.feelings_translation, by = c("feeling" = "label"))
```

The *playback* manipulations displayed on the interface were recorded in a pilot test with 4 confederates: 2 men and 2 women performed the same joint problem-solving task, but in a synchronous situation. The *playback* is thus comprised by: (1) all the emotional episodes expressed, with both the evaluation on the appraisal dimensions and the related subjective feeling expressed; and (2) what confederates have typed, at the very same moment, into the reasoning field, as well as the answer to each of the 4 enigmas. In this regard, confederates were explicitly asked not to communicate directly through the text fields, but limit their typing to the reasoning for solving the problem. One of the *playback* was then randomly chosen for the task and *injected* into the experimental task interface. The simulated partner expresses `r s1.simulated_partner %>% nrow()` emotions and finds the solution to 2 out of 4 enigmas. The full list of emotions -- comprising the time of expression, the associated Valence and Control/Power appraisals, and the subjective feeling -- are depicted in Table \@ref(tab:s1-simulated-emotions-table).

(ref:s1-simulated-emotions-caption) List of the emotions of the simulated partner. Time is expressed in seconds.

```{r s1-simulated-partner-table}
kable(s1.simulated_partner,
  col.names = c("Time", "Valence", "Control", "Feeling (FR)", "Feeling (EN)"),
  caption = "(ref:s1-simulated-emotions-caption)\\label{tab:s1-simulated-emotions-table}",
  caption.short = "Study 1: Simulated Partner's Emotions",
  longtable = FALSE,
  booktabs = TRUE
)
```

#### Eye-tracking

A Tobii T120 eye-tracker with Tobii Studio Pro v3.4.8 software [@tobiiabTobiiStudio2015] was used for eye-tracking measures. Areas Of Interest (AOI) were disposed on the EAT as a whole (left side of the screen) and the task (right side of the screen). The AOI of the EAT was further divided in the displaying/expressing upper zone, and the monitoring/perceiving lower zone.

### Procedure

Participants were given a specific time to come to the test, which was performed at Geneva University, and were reminded of the importance to be on time since another participant was performing the test in the meantime. The experimenter welcomed the participant and introduced him in the room with the eye-tracking equipment. Once installed, the experimenter proceeded to explain the outline of the study:

-   Introduction and explication (10 minutes)
-   Warm-up session with the DEW and instructions for the task (5 minutes)
-   Collaborative task (20 minutes)
-   Debriefing (10 minutes)

#### Introduction and explication

The general aim of the study was explained. The experimenter reassured participants about the fact that the data would be anonymous, and that they could stop the experiment at any time without any reason. A first consent form was then signed if the participant agreed to take part in the study.

At this point, the experimenter explained how the collaborative task would take place. She first showed a demo about the functioning of the DEW. Since in a previous study [@fritzReinventingWheelEmotional2015], whose aim was to observe the spontaneous use of the tool, participants were confused about the dimension of Control/Power, in this study the experimenter proposed a more thorough explanation of what the two sliders of the DEW stand for. The explication also aimed at reducing the risk that participants will move the cursors for the Valence and Control/Power dimensions until they found the *right* subjective feeling. Next, the experimenter showed the perceiving-monitoring part of the EAT, which was explained according to the experimental condition the participant was attributed to. In this way, participants were informed about both what the emotional information they provided would be used for (*Self-Centered* vs *Partner-oriented/Mutual-modeling*), and to which emotional information they would have access (*Self-Centered* vs *Partner-Oriented* vs *Mutual-Modeling*).

Finally, the experimenter explained the right-hand side of the screen, which implemented the joint problem-solving task. Participants were informed about the three parts (reading, solving and solution) that composed each of the 4 enigmas to solve. They were also prompted to write their reasoning to solve the problem in the appropriate field, but to avoid direct communication with the partner.

#### Warm-up session with the DEW

Participants were placed in front of the screen used for the test and could practice with a simplified version of the interface for the task. Participants could familiarize with the expression of emotions through the DEW, and random emotions were also injected in the interface at short intervals to emulate the emotion of the partner. The side of the screen devoted to the task was filled in with generic texts explaining what participants will see in the actual task (*e.g.*, *here will appear the text of the enigma*, *here you must write your reasoning*, ...)

#### Experimental task

Once the participant was ready for the test, the experimenter simulated to check-in with another confederate to simulate that the other participant was ready to start the experiment as well. Then the experimenter proceeded to calibrate the eye-tracker equipment. After being reminded about the general functioning of the eye-tracker and the importance of not moving during the task, the participant would then proceed with the task. At first, she had to fill a sort of *log-in form*, providing a random ID and an identifier for the pair. Once the task properly started, the participant had access to the overall interface depicted above, including the *playback* of all the manipulations made by a confederate.

#### Post-test debriefing

After the task, participants were asked to fill in a survey with information that will not be used in the present contribution, but have been analyzed by Perrier [-@perrierCollaborationEnvironnementMediatise2017]. At the end of the study, participants were informed about the manipulation of the simulated partner and the experimental reasons behind it. A second consent form was therefore submitted to participants, for them to confirm they understood the reason of the manipulation, and that they accepted the use of the data.

### A Priori Exclusion Criteria

Exclusion criteria determined beforehand concerned only technical issues that could jeopardize the task, especially with respect to the simulated partner. Any interruption of the task or technical failure would make the trial not recoverable. Exclusion caused for low quality of eye-tracking measures, due for instance to the participant moving too much, were also foreseen, but not yet quantified due the lack of a precise benchmark.

### Data analysis

For hypothesis *H1*, concerning differences in expression of emotions, a omnibus one-way ANOVA with pairwise comparison between all conditions was planned beforehand. The number of emotions expressed through the EAT represents the dependent variable, and the interface of the EAT the independent variable.

For hypothesis *H2*, concerning differences in perception of emotions, two indicators retrieved by eye-tracking measures [@blascheckVisualizationEyeTracking2017; @pooleEyeTrackingHumanComputer2005] are used as dependent variables. First, the total time (in seconds) that participants spent looking at the perceiving-monitoring zone of the interface. Such indicator is usually interpreted as a proxy for information processing and could account for interest (*i.e.*, people look at it longer because it is interesting) or complexity (*i.e.*, people look at it longer because they need more time to understand what it means). Second, the number of times participants sought information by orienting their gaze inside the perceiving-monitoring zone of the interface, which is usually interpreted as an indicator that the person intentionally seeks for information she may find useful or that she got lost and needs reorientation. Given the relative simplicity of the information provided -- even though people do not like graphs [@carpenterModelPerceptualConceptual1998; @pinkerTheoryGraphComprehension1990] -- and the use of a fixed interface of the EAT, both measures are used as indicators of interest. For both measures, a omnibus one-way ANOVA with pairwise comparison between all conditions were planned beforehand. A family-wise correction to account for inflation in Type I error has been planned for each pairwise comparison.

Within this experimental setting, a sensitivity analysis reveals that, given the planned sample $N = 48$ and conventional $\alpha$ = 0.05 and $\beta$ = 0.8 error rates control, each statistical test will be able to detect an effect size of Cohen's $d$ = `r printnum(s1.detectable_d)`. Taking a previous study using exactly the same task as benchmark, that would translate in practical terms as follows:

-   For expressing emotions, the statistical test will detect a difference of `r printnum(s1.delta_emotions)` expressed emotions or greater, given a mean of $M=`r printnum(mean(fritz2015_num_emotions))`$ ($SD = `r printnum(sd(fritz2015_num_emotions))`$) observed in Fritz (2015)
-   For information processing, the statistical test will detect a difference of `r printnum(s1.delta_processing)` seconds or greater, given a mean of $M=`r printnum(mean(fritz2015_eyetracking$Total_Fixation_Duration_Monitoring_Sum))`$ ($SD =`r printnum(sd(fritz2015_eyetracking$Total_Fixation_Duration_Monitoring_Sum))`$) observed in Fritz (2015);
-   For information seeking, the statistical test will detect a difference of `r printnum(s1.delta_seeking)` number of visits or greater, given a mean of $M=`r printnum(mean(fritz2015_eyetracking$Visit_Count_Monitoring_Sum))`$ ($SD =`r printnum(sd(fritz2015_eyetracking$Visit_Count_Monitoring_Sum))`$) observed in Fritz (2015).

All analysis are conducted using the statistical software R, version 4.0.5. Analysis of variance use the Afex package version `r packageVersion("afex")` [@singmannAfexAnalysisFactorial2020].

## Results

### Post-Hoc Exclusion

Results will be based on $N = 35$ participants. 10 participants were excluded due to technical issues during the task or low quality of eye-tracking measures. One participant was excluded for statistical reasons: the participant expressed 62 emotions during the task, against a mean of `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)`, that is, more than 8 standard deviations above the mean. Such a number, not even close to any participant to the same task in Fritz (2015), suggests a non representative use of the tool. The distribution of participants after post-hoc exclusions with respect to the experimental conditions is depicted in Table \@ref(tab:s1-observed-design-table).

(ref:s1-observed-design-capture) Number of participants retained for each experimental condition ($N = 35$).

```{r s1-observed-design}
s1.participants_repartition <- s1.et_data %>%
  group_by(group) %>%
  summarise(
    n = n()
  )

kable(s1.participants_repartition,
  col.names = c("Condition", "N"),
  caption = "(ref:s1-observed-design-capture)\\label{tab:s1-observed-design-table}",
  caption.short = "Study 1: Observed Experimental Design",
  longtable = FALSE,
  booktabs = TRUE,
) %>% 
  kable_styling(latex_options = "HOLD_position")
```

The resulting unbalanced design and overall small $N$, particularly low in the *Partner-Oriented* condition, decrease the power of the planned test and make it more exposed to violation of assumptions [@hoekstraAreAssumptionsWellKnown2012], which are checked in Appendix A. The interpretation of results must therefore take into account these limits.

### Differences in Expressing Emotions

Participants expressed a total of `r s1.dew_emotions %>% nrow()` emotions, which corresponds to a mean close to 14 emotions per participants (`r maf.print_m_sd(s1.aggregated_emotions$n, TRUE, FALSE)`). Participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions; `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Partner") %>% pull(n), FALSE, TRUE)` in the *Partner-Oriented* condition; and `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Mutual") %>% pull(n), FALSE, TRUE)` in the *Mutual-Modeling* condition (see Figure \@ref(fig:s1-expressed-emotions-graph)).

(ref:s1-expressed-emotions-graph-caption) Number of emotions expressed by experimental condition. Bars represent 95% confidence intervals.

```{r s1-expressed-emotions-graph, fig.cap="(ref:s1-expressed-emotions-graph-caption)", fig.align="center", out.width="60%"}

maf.plot_means_comparison(s1.aggregated_emotions, aes(x = group, y = n, color = group)) +
  labs(x = NULL, y = "Number of emotions") +
  theme(legend.position = "none")
```

No detectable difference, neither in the omnibus one-way ANOVA (`r apa_print(s1.anova.expressed_emotions)$full_result`), nor in the pairwise comparisons could be observed. Results of the comparisons are depicted in Table \@ref(tab:s1-expressed-emotions-comparison-table). Hypothesis (*H1*) is therefore rejected: a different access to and use of emotional information did not yield detectable differences in the number of emotion expressed.

(ref:s1-expressed-emotions-comparison-caption) Pairwise comparisons of the thee conditions with respect to the number of emotions expressed (*p*-values are adjusted with the Tukey method).

```{r s1-expressed-emotions-contrats}
s1.anova.expressed_emotions.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-expressed-emotions-comparison-caption)\\label{tab:s1-expressed-emotions-comparison-table}",
    caption.short = "Study 1: Number of Emotions Expressed per Condition",
    longtable = FALSE,
    booktabs = TRUE
  )
```

### Differences in Perceiving Emotions

The perception of the emotional information is divided in emotional information processing and emotional information seeking.

#### Processing Emotional Information

Participants spent on average `r maf.print_m_sd(s1.total_visit_duration$value, TRUE, TRUE)` seconds looking at any part of the perceiving-monitoring zone of the interface, which amounts to 4.28% of the total task time. Participants in the *Self-Centered* condition spent `r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)` seconds, whereas this time roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which time differed slightly (see Figure \@ref(fig:s1-total-visit-duration-graph)).

(ref:s1-total-visit-duration-graph-caption) Total time (in seconds) spent looking at the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-total-visit-duration-graph, fig.cap="(ref:s1-total-visit-duration-graph-caption)", out.width="60%", fig.align="center"}
maf.plot_means_comparison(s1.total_visit_duration, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Seconds") +
  theme(legend.position = "none")
```

An overall effect of the experimental condition on the time spent processing emotional information could be observed (`r apa_print(s1.anova.total_visit_duration)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-total-visit-duration-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-total-visit-duration-caption) Pairwise comparisons of the thee interfaces with respect to total time spent looking at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-total-visit-duration-contrats}
s1.anova.total_visit_duration.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-total-visit-duration-caption)\\label{tab:s1-total-visit-duration-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Total Visit Duration.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

#### Seeking Emotional Information

Participants' gaze entered the perceiving-monitoring zone of the interface on average `r maf.print_m_sd(s1.visit_count$value, TRUE, TRUE)` times. In the *Self-Centered* condition, the number of visits has been `r maf.print_m_sd(s1.visit_count %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)`, whereas the count roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which the count was similar (see Figure \@ref(fig:s1-visit-count-graph)).

(ref:s1-visit-count-graph-caption) Total visits count in the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-visit-count-graph, fig.cap="(ref:s1-visit-count-graph-caption)", out.width="60%", fig.align="center"}

maf.plot_means_comparison(s1.visit_count, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Number of visits") +
  theme(legend.position = "none")
```

An overall effect of the interface adopted on emotional information seeking could be detected (`r apa_print(s1.anova.visit_count)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-visit-count-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-visit-count-caption) Pairwise comparisons of the three interfaces with respect to the number of visits at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-visit-count-contrats}
s1.anova.visit_count.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-visit-count-caption)\\label{tab:s1-visit-count-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Visits Count.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

## Discussion

The planned analyses aimed at investigating whether a different use of, and access to, emotional information determine differences in use of an EAT during a computer-mediated collaborative task, which -- beside evident shortcomings in the *collaborative* nature -- may be considered representative of a CSCL application in distance learning. The reduced sample size upon which the analyses are based requires caution in interpreting the obtained results. The inter-individual differences in all measured dependent variables entail wide confidence intervals, whose source can be traced back to the many processes implicated in the task. Some of them may not be directly inherent to a genuine interest in emotional awareness, and may have potentially influenced participants' capacity in conveying and taking into account emotional awareness beyond their intentions. For instance, participants had to coordinate multiple functions, both cognitively and practically (e.g. writing on a keyboard, manipulating the EAT, etc.), under specific time constraints. Participants with less dexterity in writing at the keyboard or manipulating the interface may have found less time to dedicate to the EAT even if they were willing to. The small sample size cannot guarantee that these individual differences are sufficiently balanced by the randomized trial. Even in the presence of detectable effects, thus, the assessment of their relevance in terms of *practical* consequences is limited: their size is inherently high due to the small sample and they should not even be taken as reliable benchmarks for future studies [@albersWhenPowerAnalyses2018].

On the other hand, the controlled environment in which the *performance-based* measures have been obtained make them worth of interest in assessing to what extent the presence of an EAT serves as an affordance in conveying and taking notice of emotional awareness during a computer-mediated collaborative task. Eye-tracking measures, in particular, may be considered spontaneous reactions occurring to some extent even beyond participants' top-down control. The discussion of the obtained results may thus contribute to sketch a more defined outlook of the use of an EAT and provide cues for further hypotheses worth investigating or shortcomings to be taken into account in future studies.

### Expressing Emotions May not Depend Exclusively on Social Sharing

In the first hypothesis, it has been posited that learners expression of emotions through the EAT varies depending on what use would be made of them, and what emotional information they have access to through the interface. More precisely, it has been stated that participants in the *Self-Centered* condition would express fewer emotions, compared to the *Partner-Oriented* and *Mutual-Modeling* condition, because of the absence of social sharing. It has also been posited that participants in the *Mutual-Modeling* condition would express more emotions than in the *Partner-Oriented* condition by virtue of an additional prompt in social sharing due to the direct and persistent comparison between one's own emotions and that of the partner.

Results failed to corroborate any of these assumptions given that neither an overall effect, nor differences in the comparison between each condition could be detected. The effect size to yield significant results was already ambitious with the planned $N=48$ sample size, and was therefore even further undercut by post-hoc exclusions, for which even observed effect sizes above $d = 0.5$ (*Self-Centered* vs *Partner-Oriented* and *Self-Centered* vs *Mutual Modeling*) cannot yield a discernible difference. Hypothesis *H1* must therefore be provisionally rejected, even though the observed directions and size of the effects are, in part at lest, consistent with an increased expression of the emotions with a more social-oriented interface.

Notwithstanding the limits of the sample, it is also worth reversing the perspective and, rather, highlight how participants in all conditions expressed on average `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)` emotions, that is more than 1 emotion every 2 minutes of task. In particular, participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions despite knowing they were the only recipient of the information. This result could be considered, in principle at least, as support for *not* ruling out the interest of *intra-personal* interest in expressing emotions: the presence of an EAT could indeed serve as an *individual affordance* for emotional expression as a support for (implicit) emotion regulation. On the other hand, though, this result may also be explained by side effects of the experimental task. For instance, this number may be inflated by task compliance, since the overall experimental setting was overtly aimed at expressing emotions. Furthermore, the characteristics of the experimental task, whose timing is fixed and not determined by the participants' actions, may also have pushed participants to express emotions to *fill-in* idle time between enigmas or part of the task within each enigma, rather than for an urge to express and regulate their emotions. A more fine-grained analysis of the time of expression should therefore be taken into account.

All things considering, thus, the present contribution conveys limited and mixed evidence with respect to the interest for expressing emotions during a computer-mediated collaborative task. Nevertheless, the experimental settings elicited considerable variation in the number of emotions expressed: with an increased sample size, the experimental plan could potentially contribute to assess the matter more thoroughly, for instance using a planned equivalence test with the aim to rule out differences, rather than detecting ones [@fidlerEpistemicImportanceEstablishing; @lakensEquivalenceTestsPractical2017].

### Emotional Seeking and Processing Seem Related to Social Sharing

In the second hypothesis, it has been posited that learners would seek and process the emotional information available on screen depending on the source and the comparison it facilitates. More precisely, it has been stated that participants in the *Self-Centered* condition would seek less often and process for shorter time the information available through the perceiving-monitoring part of the interface, compared to the *Partner-Oriented* and *Mutual-Modeling* conditions. It has also been posited that participants in the *Mutual-Modeling* condition would seek information more often, and process it longer compared to the *Partner-Oriented* condition due to the increased interest enhanced by direct and persistent comparison of emotional information.

Results corroborate the presence of an overall effect of the interface both on emotional information seeking and processing. For information seeking, the experimental condition yielded a generalized effect size [@olejnikGeneralizedEtaOmega2003] of `r apa_print(s1.anova.visit_count)$estimate$group`, and of `r apa_print(s1.anova.total_visit_duration)$estimate$group` for information processing. In both cases, thus, the experimental condition seems to account for a considerable amount of variation in the perceiving-monitoring function of emotional awareness.

On a more fine-grained level, though, the differences between conditions only partially corroborated the directional hypothesis; differences were detected, both for information seeking and processing, only in pairwise comparisons between *Self-Centered* vs. *Partner-Oriented* (Cohen's $d$ \> 1 for both measures) and *Self-Centered* vs. *Mutual-Modeling* (again Cohen's $d$ \> 1 for both measures), but not between *Partner-Oriented* vs. *Mutual-Modeling*, which was actually a more severe test [@mayoStatisticalInferenceSevere2018].

The lack of a detectable difference between the *Self-Centered* and the more social-oriented interfaces would have undermined the usefulness of an EAT, whereas its presence may be explained as a *simple* novelty effect: the *Partner-Oriented* and *Mutual-Modeling* condition convey information that the learner does not know, whereas in the *Self-Centered* condition the emotions are just a reminder of what the learner should already know. Nonetheless, taking the raw measures as benchmark, it is reassuring to observe that in a task of 20 minutes, the time spent looking at emotional information is around 1 minute when information about the partner is available, compared to 30 seconds when it is not. As it is the case for the expression of emotions, though, information processing and seeking in the *Self-Centered* measures must be considered as support for an intra-personal interest for the use of an EAT, even in the absence of communication with the partner.

The comparison between the *Partner-Oriented* and *Mutual-Modeling* interfaces has deeper implications with respect to the *raison d'√™tre* of an EAT. The lack of a discernible effect between the two social-oriented conditions may suggest that there is no additional value conveyed by direct and persistent comparison available on screen. But the phenomenon could also be explained by the fact that the additional value is gained without the need for further information seeking and processing. That is, participants in the *Mutual-Modeling* condition were able to compare their own and the partner emotion thorough the interface without having to look at the perceiving-monitoring zone of the EAT more often or longer, because they could get more information for the same effort. The eye-tracking measures alone cannot unravel whether the lack of a discernible effect is a positive or negative outcome with respect to the social-oriented hypothesis. In future studies, the hypothesis should therefore also be assessed with the aid of self-reported measures about the perceived usefulness of direct and persistent comparison of learners' emotional states.

## Post-Hoc Corollary Analyses

In this section, I provide the results of additional analysis that have not been planned before the study. First, I extended the analysis of eye-tracking measures using transitions between Areas Of Interest as an interesting measure of the use of an EAT. Second, I provide indications of the use of the EAT in real-time with respect to the appraisals and subjective feeling measures collected through the task. And finally, I take advantage of the use of the same task as in @fritzReinventingWheelEmotional2015 to conduct a small, internal meta-analysis that can be of interest for the use of the same task in future studies.

### Transitions Between Areas of Interest

The eye-tracking measures used in the planned analyses of variance treated each zone of the interface as a separated element. Given the importance of dynamic, real-time phenomena in the overall thesis, it is worth investigating also transitions between the three main Areas of Interest (AOI) of the experimental task, that is (1) the expressing zone, which is common to all conditions; (2) the perceiving zone, which varies according to the experimental condition; and (3) the area dedicated to the *main* task, which is also common to all conditions. Given that transitions can go in either direction between AOI, there are 6 possible combinations of transitions: (1) Expressing to Perceiving and (2) Perceiving to Expressing; (3) Expressing to Task and (4) Task to Expressing; and finally (5) Perceiving to Task and (6) Task to Perceiving. An exploratory analysis of the transitions may reveal whether specific transitions are more frequent than others depending on the interface at disposal, and thus contribute to better assess the perceiving-monitoring function of an EAT.

The number of transitions between AOI was computed by searching for subsequent rows in the eye-tracking logs for each of the $N = 35$ participants in which the first row had a certain AOI activated, and the following row had another AOI activated. Is it worth noting, though, that this method is sub-optimal because the experimental task included the use of a keyboard. Therefore some transitions may have been lost due to the fact that each gaze-path may have been interrupted by a *detour* to the keyboard. Nevertheless, it is safe to assume that participants directed their gaze into the AOI they were interested in acting upon -- for instance, in order to focus the pointer into the text area -- before turning the gaze away from the screen if they needed to look at the keyboard for typing. All things considering, thus, this method can be of interest at least as an exploratory method, even though it lacks external validity and should be revisited before being deployed in a substantial analysis.

After seeing the data, one participant was excluded for having a number of transitions from Expressing to Perceiving and from Perceiving to Expressing much higher than all other participants regardless of the group: more than 100 against a mean of `r maf.print_m_sd(s1.transitions %>% filter(transition == "Expressing to Perceiving" || transition == "Perceiving to Expressing") %>% pull(num_transitions), FALSE, TRUE)` for the other participants regardless of the condition. Results are therefore based on $N = 34$ participants.

Participants made on average `r maf.print_m_sd(s1.transitions %>% group_by(ParticipantName) %>% summarise(total_transitions = sum(num_transitions)) %>% pull(total_transitions), TRUE, TRUE)` transitions between any two AOI. Figure \@ref(fig:s1-transitions-graph) reports the number of transitions stratified by experimental condition between the 6 AOI organized in three rows such as each row displays the transitions between the same two AOI in both directions.

(ref:s1-transitions-graph-caption) Number of transitions between Areas of Interest (AOI) on the interface. Transitions aggregated for $N = 34$ participants. Bars represent 95% confidence intervals.

```{r s1-transitions-graph, fig.cap="(ref:s1-transitions-graph-caption)", fig.height=6, out.width="100%"}
s1.transitions.graph
```

Data suggest that there are differences that may be accounted for by the type of interface the participants have access to. In particular, participants in the *Mutual-Modeling* condition seem to be more prone to make transitions between the two AOI that are more directly related to the social sharing of emotions. Transitions between the *expressing-displaying* and the *perceiving-monitoring* zone (first row in the graphic) may indicate that the possibility of direct and persistent comparison of one's own emotions with that of the partner could serve as *social reference* before expressing one's own emotions, or *social comparison* after having expressed them. Furthermore, transitions between the *perceiving-monitoring* zone and the *task* zone (last row in the graphic) may indicate that the emotional information is taken into account as instrumental information to the task at hand.

Participants in the *Self-Centered* condition seem to privilege the paths between the *expressing-displaying* zone and the *task* zone, which is consistent with the fact that the *perceiving-monitoring* zone has only information about their own emotions. It is interesting to notice, though, that in the *Self-Centered* condition, transitions from the *expressing-displaying* zone to the *perceiving-monitoring* zone (first row, graph on the left) do not seem to be more frequent compared to the *Partner-Oriented* condition. This may be relevant because it could rule out the possibility that a difference between the *Partner-Oriented* and *Mutual-Modeling* condition may be due simply to the fact that, in the *Mutual-Modeling* condition, participants only seek confirmation of what they have expressed, since this confirmation is not available in the *Partner-Oriented* condition.

Finally, the *Partner-Oriented* condition seems once again *stuck in the middle*, and results for this group are difficult to assess due to the greater inter-individual variance that is also present in the other planned analysis. As a rule of thumb interpretation, the *Partner-Oriented* condition seems to go hand-in-hand with the *Self-Centered* condition in transitions between the *expressing-displaying* zone and the *perceiving-monitoring* zone (first row); and with the *Mutual-Modeling* condition in the other transitions (second and third rows).

In an attempt to figure out whether this kind of analysis may be used in a more structured manner, a multilevel linear model, also known as mixed linear model [@batesFittingLinearMixedEffects2014; @kuznetsovaLmerTestPackageTests2017; @westLinearMixedModels2015], was fitted to the data at hand using the mixed function of the Afex [@singmannAfexAnalysisFactorial2020; @singmannIntroductionMixedModels2020] R package version `r packageVersion("afex")`. The model was fitted in the following terms: the aggregated number of transitions per participant for each possible path represented the outcome variable; the type of transition and the interface of the EAT (*i.e.* the experimental condition) were considered as fixed factors, with an interaction between the two; the participant was used as a random intercept to account for the non-independence of observations. A more complex model could have been more interesting, but hardly feasible due to the small number of participants [@batesParsimoniousMixedModels2018].

A Type III Analysis of Variance of the multilevel linear model confirms effects of both individual factors and the interaction. Results are depicted in Table \@ref(tab:s1-transitions-anova-table) using Kenward-Roger approximation for computing the *p*-value [@lukeEvaluatingSignificanceLinear2017].

(ref:s1-transitions-anova-caption) Results of a Type III ANOVA on the fitted multilevel linear model

```{r s1-transitions-anova-table}
s1.transitions.lmm.anova_table %>%
  kable(
    caption = "(ref:s1-transitions-anova-caption)\\label{tab:s1-transitions-anova-table}",
    caption.short = "Study 1: Transitions between AOI",
    booktabs = TRUE,
    longtable = FALSE
  )
```

Table \@ref(tab:s1-transitions-comparisons-table) reports the pairwise comparisons between the three experimental conditions stratified by the bi-directional path of the transition. Detectable differences in the pairwise comparisons can be observed between *Self-Centered* vs *Mutual-Modeling* and *Partner-Oriented* vs *Mutual-Modeling* in the transitions between expressing-displaying and perceiving-monitoring. In the four comparisons, the more *social-oriented* interface obtained more transitions, in both directions, than the less *social-oriented* one, corroborating the assumption that participants make use of the emotional information about the partner as a reference.

In the transitions between expressing-monitoring and the task, only the path going from the task to the expression-displaying zone yielded a detectable difference with more transitions in the *Partner-Oriented* than in the *Self-Centered* interface. The effect is nevertheless not corroborated by any other comparison in the same transition path.

Finally, in the transitions between perceiving and the task, detectable differences were observed between the *Self-Centered* and the *Mutual-Modeling* interfaces, with the *Mutual-Modeling* interface yielding more transitions in both directions. These results may support the role of emotional awareness as instrumental information directly related to the task at hand, but are not corroborated by a difference between the *Self-Centered* and the *Partner-Oriented* interfaces.

(ref:s1-transitions-comparisons-caption) Comparisons between the groups stratified by the path of the transitions. The Kenward-Roger approximation for the degrees of freedom is adopted and *p*-values are adjusted using the Tukey method for comparing a family of 3 estimates.

```{r s1-transitions-comparisons-table}
s1.transitions.contrasts$contrasts %>%
  as_tibble() %>%
  select(-transition) %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    caption.short = "Study 1: Pairwise comparison between transitions",
    longtable = FALSE,
    booktabs = TRUE,
    linesep = c("", "", "\\addlinespace"),
    col.names = c("Comparison", "Est.", "SE", "df", "t.ratio", "p.value"),
    caption = "(ref:s1-transitions-comparisons-caption)\\label{tab:s1-transitions-comparisons-table}",
  ) %>%
  kable_styling(
    latex_options = c("repeat_header")
  ) %>%
  pack_rows("Expressing to Perceiving", 1, 3) %>%
  pack_rows("Perceiving to Expressing", 4, 6) %>%
  pack_rows("Expressing to Task", 7, 9) %>%
  pack_rows("Task to Expressing", 10, 12) %>%
  pack_rows("Perceiving to Task", 13, 15) %>%
  pack_rows("Task to Perceiving", 16, 18)
```

All things considering, transitions may represent a more interesting measure of the perceiving-monitoring function of emotional awareness compared to the information seeking and processing measures adopted in the planned analyses. Even considering the shortcomings (e.g. transitions interrupted by the use of the keyboard), transitions provide a more *dynamic* outlook on how emotional information is integrated into the task. The concept of transition may even be pushed further by measuring at which moment the transition has occurred, which would provide useful information on the dual-task nature of emotional awareness. For instance, it would be possible to assess whether learners look at the emotions expressed by the partner as soon as they appear on the interface, or if they wait idle period in the task.

### Emotions and Time: Evaluating the Purpose of Real-Time Awareness

One of the main tenets of the present contribution is the advantage of *real-time* emotional awareness -- at least to the extent that participants do not have to wait predefined stops to share their emotions. Some exploratory analyses on the sample were performed in order to check to what extent the *real-time* feature has been exploited with respect to the expression of emotions.

#### Cognitive Evaluation Over Time

Congruently with appraisal theories of emotions -- which state that it is the evaluation one does of the situation and not the situation *per se* that elicits the emotion -- it is worth checking for the emergence of a pattern in the appraisals of Valence and Control/Power over time. Since all participants were exposed to the same stimuli (except participants in the *Self-Centered* condition, who did not see the emotions of the simulated partner), a clear pattern in the evaluation of the two criteria would not be congruent with appraisal theories. Figure \@ref(fig:s1-appraisal-evolution-graph) shows all the $N = `r nrow(s1.dew_emotions)`$ emotions that have been expressed by all the participants over the 20 minutes of the task, stratified by condition. For each observation, the value of Valence and Control/Power are displayed with respect to the elapsed time in the task. A Locally Estimated Scatterplot Smoothing (LOESS) -- that is, non-parametric curve that best fit the empirical data [@jacobyLoessNonparametricGraphical2000] -- is superposed to the raw data.

(ref:s1-appraisal-evolution-caption) Evolution of the appraisal dimensions over time with a LOESS smoother ($N = 35$, all emotions of participants are aggregated per condition).

```{r s1-appraisal-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-appraisal-evolution-caption)"}

grid.arrange(
  s1.appraisal_over_time_valence,
  s1.appraisal_over_time_control,
  nrow = 1
)
```

The graphic indicates that the smoother remain, overall, close to the neutral point, since data-points are evenly spread over the elapsed time in the task. It is worth noting, though, that in the *Mutual-Modeling* condition, both appraisals decreased as the task went on.

#### Subjective Feelings Over Time

The same analysis can be conducted with respect to the expression of subjective feelings over time. Figure \@ref(fig:s1-feelings-evolution-graph) below plots the evolution of the expression of the 20 subjective feelings -- which are part of the underlying affective space used in the study -- aggregated for all $N=35$ participants. The observations are stratified per condition. (In order to reduce the space of the graph, the legend for each condition has been omitted, but the colors are congruent with previous graphs.)

(ref:s1-feelings-evolution-caption) Expression of the subjective feelings. $N = `r nrow(filter(s1.dew_emotions, listed == TRUE))`$ emotions (out of `r nrow(s1.dew_emotions)`) whose subjective feeling belongs to the underlying affective space used by the DEW, aggregated for the $N = 35$ participants. (Legend omitted for reducing space, see previous graphs.)

```{r s1-feelings-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-feelings-evolution-caption)"}
s1.feelings_over_time_graph
```

Not considering the feelings that have been expressed only a few times (*e.g.*, *Envious* or *Disgusted*), most of the subjective feelings have been expressed rather uniformly over the 20 minutes of the task. Interesting exceptions are the feelings *Bored* and *Frustrated* that only starts around 5 minutes into the task -- that is, around the end of the first enigma -- which may be due to the repetitive nature of the task for boredom, and the increasing difficulty of the enigmas for frustration.

Finally, the overall small sample size combined with the unbalanced number of participants for experimental condition imply caution even on superficial interpretations about the effect of the interface. It is nevertheless worth noting how participants felt often *Relieved* or *Satisfied* in the *Mutual-Modeling* condition, but not in the *Self-Centered* or *Partner-Oriented* condition; or that the *Emphatic* feeling was expressed in the *Mutual-Modeling* and *Partner-Oriented* condition, but not in the *Self-Centered*. With a greater number of participants, it would be interesting to perform this kind of stratification in a more systematic way.

### Internal Meta-Analysis on Task Indicators

Taking advantage of the fact that the same task was adopted, under similar conditions, of a previous study [@fritzReinventingWheelEmotional2015], an internal meta-analysis was performed on the point-estimate means for the three dependent variables adopted in the current contribution. The interest of the meta-analyses is two-fold. On the one hand, they provide a better assessment of the point estimates about the performance-based indicators of the use of the EAT. On the other hand, those same indicators will be used as references in a subsequent study in this contribution.

Each meta-analysis has been conducted using the R meta package version `r packageVersion("meta")`, adopting the inverse of the variance weighting mechanism to account for differences in the sample size of the two internal studies. Results both for the fixed and the random models (using the DerSimonian-Laird estimator for $tau^2$) are provided.

#### Expressing Emotions Internal Meta-Analysis

The meta-analysis on the expression of emotions has been conducted on the whole sample size of both studies, since, even for participants in the *Self-Centered* condition, the overall situation in which participants have expressed their emotions are sufficiently close for an internal meta-analytic purpose. Consequently, the sample size are of $N = 16$ in Fritz (2015) and of $N = 35$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-expressing), assess an estimated mean of `r s1.meta_analysis_expressing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$fixed$upper %>% printnum()`] expressed emotions for the fixed effect model, and of `r s1.meta_analysis_expressing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis highlights the presence of considerable heterogeneity in the expression of emotions ($\tau^2$ = 10.29; $\tau$ = 3.21; I\^2 = 82.0% [23.9%; 95.7%]; H = 2.36 [1.15; 4.84]; $\chi^2$ = 5.55 *p* = .018). This may suggest that the different conditions of the two studies may have played a role in inflating the number of emotions expressed in Fritz (2015), where participants were explicitly asked, if possible, to express at least one emotion in each phase of the 4 enigmas (*i.e.* which would amount to 12), whereas in Perrier (2017) they did not receive any guidance. This may be interpreted as a warning about the importance of being careful in framing how the expression of emotional information is prompted, even if the inflation of the number of emotions may not be necessarily accounted by *forced* emotions, that is, emotional episodes that are not *really* felt, but nevertheless reported. It may also be the case that prompting for emotional expression may ease participants into expressing their emotions, something they could be less prone to do otherwise.

```{r s1-forest-expressing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of emotions expressed in the experimental task.", fig.height=2 }
forest.meta(s1.meta_analysis_expressing, hetstat = TRUE, xlim = c(5, 25), layout = "JAMA")
```

#### Information Processing Internal Meta-Analysis

The internal meta-analysis on information processing has been conducted using only the participants retained for the eye-tracking analysis ($N = 14$) in Fritz (2015), and only participants in the *Partner-Oriented* and *Mutual-Modeling* conditions ($N = 23$) in Perrier (2017), for the interface in these situations is identical or at least very similar with respect to the *social* information shared. Results, depicted in Figure \@ref(fig:s1-forest-processing), assess an estimated mean of `r s1.meta_analysis_processing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$fixed$upper %>% printnum()`] total visit duration, in seconds, for the fixed effect model, and of `r s1.meta_analysis_processing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 40.81; $\tau$ = 6.39; I\^2 = 45.2%; H = 1.35; $\chi^2$ = 1.82 *p* = .177), which may indicate that the time spent at looking at emotional information could be determined by a balance between the primary problem-solving activity and the sustaining emotional awareness. The point estimate of total visit duration being around 1 minute over the 20 minutes of the task, it corresponds to a proportion of 5% of the total time.

```{r s1-forest-processing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the time spent at processing emotional information available on screen.", fig.height=2}
forest.meta(s1.meta_analysis_processing, hetstat = TRUE, xlim = c(30, 80), layout = "JAMA")
```

#### Information Seeking Internal Meta-Analysis

With respect to emotional information seeking, the internal meta-analyses comprise the same samples as for information processing, that is $N = 14$ in Fritz (2015) and $N = 23$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-seeking), assess an estimated mean of `r s1.meta_analysis_seeking.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$fixed$upper %>% printnum()`] number of visits for the fixed effect model, and of `r s1.meta_analysis_seeking.summary$random$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$random$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 0; $\tau$ = 0; I\^2 = 0%; H = 1; $\chi^2$ = .56 *p* = .453), which is nevertheless rather due to the huge variability within studies rather than homogeneity between studies. In future studies, the number of transitions between AOI could represent a more informative measure for emotional information seeking.

```{r s1-forest-seeking, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of times emotional information has been visited on screen.", fig.height=2 }
forest.meta(s1.meta_analysis_seeking, hetstat = TRUE, xlim = c(50, 100), layout = "JAMA")
```

## Conclusion

This chapter presented a detailed illustration of an empirical study investigating whether a different use of, and access to emotional information expressed and available through an EAT had an effect on the actual use of the EAT itself. $N=48$ participants, then reduced to $N=35$ following exclusion criteria, were randomly assigned to three different interfaces of the EAT -- namely a *Self-Centered*, a *Partner-Oriented*, and a *Mutual-Modeling* interface -- which varied on how socially-oriented each interface was. The main assumption underlying the empirical investigation stated that the more socially oriented interface would yield a greater use of the EAT in terms of emotion expressed and emotional information seeking and processing. This assumption was not corroborated for the number of emotion expressed, since a detectable difference was not observed between the three conditions; and it was only partially corroborated for emotional information seeking and processing, for which participants in the *Partner-Oriented* and *Mutual-Modeling* conditions sought and processed emotional information more than in the *Self-Centered* condition, but no detectable difference was observed between the two more socially-oriented conditions.

Despite the hypotheses of the study being rejected or only partially corroborated, most of the performance-based indicators about the use of the EAT were congruent with the main assumption. These indicators included the number of emotions expressed through the tool, the number of visits and seconds spent looking at the perceiving-monitoring part of the EAT, as well as the transitions between the more socially-oriented parts of the EAT interface. In most of these cases, the *Mutual-Modeling* interface yielded the greater use of the EAT, followed by the *Partner-Oriented*. Congruently with a inter-personal perspective on emotional expression [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], these results seem to corroborate the usefulness of an EAT as an affordance to share emotion with a partner and take the partner's emotions into account during a computer-mediated collaborative task. Participants seemed to show a genuine interest in seeking and processing emotional information available about the partner, and knowing that their emotions would be conveyed to the partner did not stop participants to express them. The presence of the partner's emotions on the EAT interface also resulted in a more *dynamic* gaze-path, with more transitions between the part of the interface dedicated to the task, and that dedicated to the monitoring-perceiving function of awareness. This fact seems to corroborate the usefulness of providing real-time emotional awareness, since the emotional information may be truly integrated as instrumental information to the task at hand [@buderGroupAwarenessTools2011; @dourishAwarenessCoordinationShared1992]. A word of caution is nevertheless in order, since the method by which transitions have been computed is not externally validated yet. Whether and when it will, transitions could represent a more adequate measure of integrated and dynamic information seeking and processing compared to the *static* number of visits and seconds spent inside an Area of Interest used in the directional hypotheses of the study. All things considering, thus, a moderate optimism is warranted about the usefulness of an EAT during a computer-mediated collaborative task. Despite the fact that it does not directly provide information about the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013], sharing emotions could nevertheless sustain the mutual-modeling activity by which learners build and update a holistic representation of their partner in the collaboration [@dillenbourgSymmetryPartnerModelling2016].

At the same time, participants in the *Self-Centered* condition also seemed to harness the presence of the EAT, which is congruent with an intra-personal usefulness in expressing emotions [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018]. Even though participants in this condition knew beforehand that they were the exclusive sender and receiver of the emotional information, they still expressed emotions as well as sought and processed their own emotional information available on the EAT. This fact suggests that the presence of an EAT may prompt learners to inquiry about their own emotional state, appraising the situation and seeking for the congruent subjective feeling elicited by the circumstances [@boehnerHowEmotionMade2007; @grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009].

### Limitations and Future Development

The present contribution adopted a controlled environment in order to expose every participant to the same stimuli, except for the randomly assigned interface. The use of a simulated partner limited the inter-personal communication that would be normally available in a real collaborative setting. For the purposes of the study, a distinction between cooperation (roughly, doing the same thing with limited interdependence) and collaboration (integrating efforts into a common outcome) was not of primary concern. Nevertheless, this certainly represents a limitation to the generalization of the obtained results to a more articulated communication flow, which could overlap with the emotional information expressed through the EAT. For instance, learners could have used the text area dedicated to their reasoning to inject circumstantial information such as *I don't understand* or *I don't agree with you*, which would have conveyed social, cognitive and emotional information [@derksRoleEmotionComputermediated2008]. A focal question avoided by the present experimental setting is therefore whether participants would still have used the EAT the way they did, were they allowed to convey emotional information through the content space of the joint problem-solving activity. The task at hand, though, can be easily extended to a real collaboration between two participants, and therefore it would be possible to investigate the matter in a way that can be directly related to the results of this contribution. That could be obtained either by a direct replication of the current setting, but using pairs of participants, or by a split design in which part of the participants are randomly assigned to a simulated- or a real-partner. This second option would elicit a better and more reliable comparison, but its interest would be fairly limited to the *validation* of a simulated participant, which was an auxiliary instrument in the study.

A direct replication with real collaboration, on the other hand, would investigate the subject matter more thoroughly, even though the comparison with present results would have to take the difference in time and setting into account. In this regard, a possible solution is to retain a ratio between the duration of the joint-problem solving task and the performance-based measures of the use of the EAT. For example,participants in a dyad that solved the four enigmas in 12 minutes and expressed 18 emotions for one participant and 24 for the second would have a ratio of 1.5 and 2 respectively. Taking into account the temporal dimension would also add another element of interest: investigate whether the ration holds constant across different duration of the joint problem-solving task, or it yields a moderation effect.

Furthermore, the focal question of whether learners would use the EAT having a more thorough channel of communication could also be assessed by allowing learners to display or not the EAT on their screen. This choice may also be instrumental in investigating one of the main assumptions of the thesis, namely the interest for real-time emotional awareness: whether and when participants would decide to display the EAT may convey pivotal evidence about the usefulness of real-time emotional awareness compared, for instance, to a *scripting* strategy in which partners share their emotions at specific intervals outside the task.

### Acknowledgments

The experimental phase of the project has been carried out by St√©phanie Perrier, a Master MALTT students, as part of her Master thesis [@perrierCollaborationEnvironnementMediatise2017] that Mireille B√©trancourt and I co-directed.
