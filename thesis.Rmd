---
title: 'Implementation and Assessment of a Multipurpose Appraisal-Driven Emotion Awareness Tool Based on Self-Report. With an Application to Computer-Mediated Learning Environments.'
author: 'Mattia A. Fritz'
date: 'September 2021'
institution: 'Université de Genève'
division: 'TECFA'
advisor: 'Prof. Mireille Bétrancourt'
department: 'FPSE'
degree: 'PhD in Psychology'
knit: bookdown::render_book
site: bookdown::bookdown_site

# This will automatically install the {remotes} package and {thesisdown}
# Change this to FALSE if you'd like to install them manually on your own.
params:
  'Install needed packages for {thesisdown}': True
  
# Remove the hashtag to specify which version of output you would like.
# Can only choose one at a time.
output:
  thesisdown::thesis_pdf:
    extra_dependencies: ["flafter"]
    includes:
      in_header: "preamble.tex"
#  thesisdown::thesis_gitbook: default         
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default

# If you are creating a PDF you'll need to write your preliminary content 
# (e.g., abstract, acknowledgements) below or use code similar to line 25-26 
# for the .RMD files. If you are NOT producing a PDF, delete or silence
# lines 25-39 in this YAML header.
abstract: '`r if(knitr:::is_latex_output()) paste(readLines(here::here("prelims", "00-abstract.Rmd")), collapse = "\n  ")`'
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab 
# is needed on the line after the `|`.
acknowledgements: |
  I want to thank a few people.
dedication: |
  You can have a dedication here if you wish. 
preface: |
  This is an example of a thesis setup to use the reed thesis document class 
  (for LaTeX) and the R bookdown package, in general.
  
# Specify the location of the bibliography below
bibliography: references.bib
# Download your specific csl file and refer to it in the line below.
csl: csl/apa.csl
lot: true
lof: true
#header-includes:
#- \usepackage{tikz}
header-includes:
  - \raggedbottom
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of 
metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete the section entirely, or silence them (add # before each line). 

If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.

If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include=FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste('You need to run install.packages("remotes")",
            "first in the Console.')
    )
  }
}
if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
# Set how wide the R output will go
options(width = 70)
knitr::opts_chunk$set(
  echo = FALSE,
  message=FALSE,
  warning = FALSE
)
```

<!--
The acknowledgments, preface, dedication, and abstract are added into the PDF
version automatically by inputting them in the YAML at the top of this file.
Alternatively, you can put that content in files like 00--prelim.Rmd and
00-abstract.Rmd like done below.
-->



```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00--prelim.Rmd")}

```

```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00-abstract.Rmd")}

```

<!-- The {.unnumbered} option here means that the introduction will be 
"Chapter 0." You can also use {-} for no numbers on chapters.
-->

<!--chapter:end:index.Rmd-->

---
bibliography: bib/references.bib
---

# Introduction {.unnumbered}

```{r intro-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

Imagine that you are co-authoring a text with a colleague for an upcoming assignment that you must submit in a course: you are at home, she can be anywhere in the world. You are using an online text processor, which allows you to edit the document simultaneously. This particular text editor has the feature of attributing animals names to authors, so that you are currently identified in the work-space as *anonymous hippo*, and your partner is *anonymous turtle*. The only thing you can see about the *turtle* is the flow of letters that stems from the position of her cursor in the document whenever she is typing on her keyboard, and *vice versa*. Imagine being in this situation: what kind of information about the *turtle* would you like to know to better assess how the learning task is going? And what kind of information about yourself would you like the *turtle* to know, for her to better assess how the learning task is going?

Now imagine that you are coding a project in an introductory programming course: you are facing lines of condensed and often mysterious syntax, whose execution sometimes results in the desired and celebrated outcome, but oftentimes leads to irking and obscure message errors. You are alone in front of your screen -- riddled with software you barely know and dozens of tabs in your browsers with results to hazardous research queries -- wondering whether your colleagues in the course are facing a similar fate. What information about them would help you better cope with this situation? What information about yourself would you like to share, which could be beneficial both to you and them?

You have certainly guessed from the title of the thesis that the answer put forward to these questions is: *emotions*. In the last few decades, emotions have been widely studied from different perspectives and with different methods, giving rise to the interdisciplinary field of affective sciences [e.g., @davidsonHandbookAffectiveSciences2003; @sanderOxfordCompanionEmotion2009]. A consistent body of research has corroborated the importance that affective phenomena -- such as emotions, feelings, motivations, moods, or preferences -- play in various circumstances of life, both individually and collectively, and at different levels of processing [@adolphsNeuroscienceEmotionNew2018; @armonyCambridgeHandbookHuman2013; @barrettHowEmotionsAre2018; @colombettiFeelingBodyAffective2014; @damasioStrangeOrderThings2018; @fischerSocialFunctionsEmotion2016; @fontaineComponentsEmotionalMeaning2013; @pessoaCognitiveemotionalBrainInteractions2013; @rimePartageSocialEmotions2005; @sanderModelsEmotionAffective2013; @vankleefInterpersonalDynamicsEmotion2018]. As a consequence of that, leading scholars in the field [@dukesRiseAffectivism2021] have recently considered legitimate to ask whether research has entered a new era: the era of *affectivism*.

> The conceptual, methodological, and technical advances made within the last few decades have demonstrated that affective processes are unquestionably enlightening when it comes to understanding both behaviour and cognition. While it will ultimately be the responsibility of historians of science to determine whether or not a new era has begun, given the undeniable impact of affective sciences on our models of brain, mind, and behaviour, it seems relevant to ask today whether we are now in the era of affectivism. -- @dukesRiseAffectivism2021, p. 819

Even though the role of affect has received relatively little attention in learning and education in the past [@pekrunInternationalHandbookEmotions2014; @pekrunProgressOpenProblems2005], there is nowadays a growing consensus in considering learning as the results of cognitive, social, and affective interactions [@bakerAffectiveLearningTogether2013; @immordino-yangEmotionsLearningBrain2016; @brackettPermissionFeelUnlocking2019; @pekrunInternationalHandbookEmotions2014; @pekrunEmotionsSchool2018]. With respect to the role of affective phenomena more specifically, three intertwined research areas can be discerned. First, research investigating the effect of learning and teaching activities on learners' and teachers' affective experience [@dmelloSelectiveMetaanalysisRelative2013; @pekrunControlValueTheoryAchievement2006; @pekrunIntroductionEmotionsEducation2014]. Second, research that, conversely, focuses on the effect of affective phenomena on learning processes and outcomes [@dmelloConfusionCanBe2014; @hascherLearningEmotionPerspectives2010; @immordino-yangEmotionsLearningBrain2016]. Third, research that investigates the existence of social and emotional competences that may be beneficial inside and outside learning contexts, and how to empower learners with those competences [@brackettPermissionFeelUnlocking2019; @brackettRULERTheoryDrivenSystemic2019; @jarvelaSociallySharedRegulation2016; @winneWhatStateArt2015].

The role of affective phenomena in general, and emotions more specifically, is particularly challenging in computer-mediated learning environments -- a term used throughout the text to encompass, at least partially, concepts such as advanced learning technologies [@graesserEvolutionAdvancedLearning2013]; computer-based learning environments [@moosLearningComputerBasedLearning2009]; computer-supported collaborative learning [@dillenbourgEvolutionResearchComputersupported2009]; distance, remote or e-learning [@bates2005technology]; and technology enhanced learning [@kirkwoodTechnologyenhancedLearningTeaching2014]. The role of computational devices in both formal and informal education has become ubiquitous, especially in higher education, and a growing body of research is therefore devoted to investigating the interplay between affective phenomena and computer-mediated learning environments [@arguedasOntologyEmotionAwareness2015; @cerneaSurveyTechnologiesRise2015; @dmelloFeelingThinkingComputing2015; @feidakisReviewEmotionAwareSystems2016; @graesserEmotionsAdvancedLearning2014; @harleyDevelopingEmotionAwareAdvanced2017].

On the one hand, part of the literature on the subject highlights shortcomings in computer-mediated learning environments due to the lack of para-verbal cues, usually available in non-mediated face-to-face interactions, which play a prominent role in various aspects of learning such as coordination, communication, sense of belonging or inter-subjective meaning-making [@marchandRoleEmotionLearning2012; @michinovFacetofaceContactMidpoint2008; @kreijnsIdentifyingPitfallsSocial2003; @kreijnsSocialAspectsCSCL2013; @dillenbourgSymmetryPartnerModelling2016]. In fact, even in co-located settings, the presence of a computer captures and directs learners' attention [@eligioEmotionUnderstandingPerformance2012], and therefore reduce the availability of cues, such as facial expressions or body postures, that are well known to convey emotional meaning [@banzigerEmotionRecognitionExpressions2009; @martinezContributionsFacialExpressions2016; @schererInvestigatingAppraisaldrivenFacial2019; @shumanEmotionPerceptionComponential2017]. These shortcomings are further accentuated in distance learning, especially in asynchronous settings, where learners do not interact in real time, but have to cope with latency time and delayed exchanges [@bates2005technology; @jacquinotApprivoiserDistanceSupprimer1993; @paquelinDistanceQuestionsProximites2011]. There is a growing consensus in considering that such situations may benefit from some form of *social presence*, that is, the perception of others in the absence of *physical* presence [@gunawardenaSocialPresencePredictor1997; @jezegouCreerPresenceDistance2010; @lowenthalSearchBetterUnderstanding2017]. In this regard, several contributions highlight the importance of affective phenomena in providing a holistic experience of this social presence [@jezegouCreerPresenceDistance2010; @lowenthalSearchBetterUnderstanding2017; @rourkeLearningCommunitiesInquiry2009; @kirschnerAwarenessCognitiveSocial2015].

On the other hand, the literature also suggests that human beings' need to experience and express emotions transcends time and space [@derksRoleEmotionComputermediated2008; @parkinsonEmotionsDirectRemote2008; @rimePartageSocialEmotions2005; @vankleefSocialEffectsEmotions2017]. In this regard, the use of computers may provide alternatives, or even enhance emotional experience and communication, through dedicated or integrated artifacts [@boehnerHowEmotionMade2007; @derksRoleEmotionComputermediated2008; @parkinsonEmotionsDirectRemote2008; @vankleefSocialEffectsEmotions2017]. In other words, face-to-face interaction may not necessarily be considered as the golden standard to re-enact, for instance, by providing seamless audio/video connection [@buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013]. On the contrary, emotion in computer-mediated learning environments may be conveyed through a wide panel of options, ranging from *simple* emoticons to autonomic recognition and representation through dedicated hardware and software [@arguedasModelProvidingEmotion2018; @baralouEmotionsSpatialisationSocial2013; @gliksonDarkSideSmiley2018; @graesserEmotionsAdvancedLearning2014; @harleyDevelopingEmotionAwareAdvanced2017; @cerneaSurveyTechnologiesRise2015]. Computational devices open a wide spectrum of possibilities in the cycle of input, processing, and output of emotional information, including various means to gather data, persistence of data over time, visual-spatial displaying of data, as well as technologies that react or adapt based on learners' affective experience [@arguedasModelProvidingEmotion2018; @cerneaGroupAffectiveTone2014; @dmelloFeelingThinkingComputing2015; @grawemeyerAffectiveLearningImproving2017; @kimRoleAffectiveMotivational2012; @leonyProvisionAwarenessLearners2013; @fuentesSystematicLiteratureReview2017; @silvaComparativeStudyUsers2020].

The choice of how to endow the computer-mediated learning environment with affect-related information and the reasons to do so depends on various criteria, with pedagogical and technical aspects often intertwined in mutual influence. In this regard, a growing body of research in the last decade has started to investigate the possibility to endow computer-mediated learning environments with emotional awareness, which is roughly defined as information about one's own emotions and/or about the emotions of other learners sharing the same computer-mediated learning environment [@arguedasOntologyEmotionAwareness2015; @avrySharingEmotionsContributes2020; @cerneaSurveyTechnologiesRise2015; @dmelloFeelingThinkingComputing2015; @eligioEmotionUnderstandingPerformance2012; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Contributions pertaining to this line of inquiry explicitly or implicitly share one ore more of the following underlying assumptions. First, it is posited that learners may benefit from being aware of their own emotions, that is, that their own emotions provide instrumental information for self-regulation of learning [@ez-zaouiaEmodashDashboardSupporting2020; @lavoueEmotionAwarenessTools2020; @molinariEMORELOutilReporting2016; @winneWhatStateArt2015]. Second, it is assumed that emotion may convey instrumental information to others, for them to integrate that information into their own learning task, as a means to improve self-regulation, co-regulation and social regulation of learning [@eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013; @winneWhatStateArt2015; @jarvelaSociallySharedRegulation2016; @jarvelaEnhancingSociallyShared2015]. Third, it is also implied that emotion can be fruitfully conveyed in a computer-mediated learning environment, that is, the emotional information is made available and processed by users in the learning environment where learning activity and emotional awareness coexist in some form [@bersetVisualisationDonneesRecherche2018; @cerneaSurveyTechnologiesRise2015; @leonyProvisionAwarenessLearners2013; @harleyMeasuringEmotionsSurvey2015; @fuentesSystematicLiteratureReview2017].

One way for learning activity and emotional awareness to coexist is through the use of an Emotion Awareness Tool (EAT). An EAT coalesces features of affect- or emotion-aware systems on the one side, and of awareness tools on the other. Affect- or emotion-aware systems [@calvoFeelingThinkingComputing2015; @graesserEmotionsAdvancedLearning2014; @grawemeyerAffectiveLearningImproving2017; @harleyDevelopingEmotionAwareAdvanced2017] stem mainly from the interdisciplinary field of affective computing [@picardAffectiveComputing2000; @schererBlueprintAffectiveComputing2010] and are therefore driven, *in primis*, by computational modeling of affective phenomena [@marsellaComputationalModelsEmotion2010; @schererComponentProcessModel2010]. Awareness tools [@buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013; @janssenGroupAwarenessTools2011], on the other hand, emerged primarily from the field of computer-mediated collaboration and were later implemented in computer-mediated learning environments, especially through the interdisciplinary field of computer-supported collaborative learning [@dillenbourgEvolutionResearchComputersupported2009; @kreijnsSocialAspectsCSCL2013; @millerScriptingAwarenessTools2015; @suthersTechnologyAffordancesIntersubjective2006]. Awareness tools have thus an intrinsic inter-personal stance, which emphasizes learners' need to build and update a mutual understanding of the situation at hand [@clarkGroundingCommunication1991; @roschelleConstructionSharedKnowledge1995] as a means to maximize inter-subjective meaning-making, considered as a determinant of learning processes and outcomes [@dillenbourgSymmetryPartnerModelling2016; @janssenCoordinatedComputerSupportedCollaborative2013; @janssenGroupAwarenessTools2011; @suthersTechnologyAffordancesIntersubjective2006].

It follows that an EAT is a technological artifact, whose aim is to provide learners with emotional awareness in computer-mediated learning environments, under the assumption that its availability is beneficial to learners at the individual and/or collective levels [@arguedasAnalyzingHowEmotion2016; @avryAchievementAppraisalsEmotions2020; @avrySharingEmotionsContributes2020; @cerneaSurveyTechnologiesRise2015; @feidakisEndowingElearningSystems2011; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Such a claim has received in the last decade an increasing interest in the affective, computer, and learning sciences from different theoretical, technical, methodological and empirical standpoints (*ibid.*). Applications of an EAT in computer-mediated learning environments may in fact range from providing learners with the possibility to self-report their emotions during a distance learning course as a means to increase self-reflection and self-regulation [@lavoueEmotionalDataCollection2017] to the implementation of a multi-modal system that provide emotional awareness through an unobtrusive brain-computer interface during computer-mediated collaboration [@makhkamovaAugmentingCollaborationInvisible2019]. Despite the differences in application, though, research gravitating around an EAT usually aims at investigating which factors -- intrinsic to the tool, deriving from the interaction between learners and the tool, between learners themselves, as well as between learners and the instructional design -- determine the adoption, perception, use and instrumentality of emotional awareness in computer-mediated learning environments. The thesis aims at extending this area of investigation by implementing and assessing a multipurpose, appraisal-driven emotion awareness tool based on self-reported emotions, which learners may express and perceive voluntarily and at any time while performing learning-oriented tasks in a computer-mediated environment.

## Thesis Perspective and Objectives {.unnumbered}

In the last few years, psychology has faced what has been widely referred to as the replication crisis \[@earpReplicationFalsificationCrisis2015; @anvariReplicabilityCrisisPublic2019; @chambersSevenDeadlySins2017; @opensciencecollaborationEstimatingReproducibilityPsychological2015\]. What started as primarily a problem of statistical practices and inferences that undermined the reliability of experiments' results \[@opensciencecollaborationEstimatingReproducibilityPsychological2015; @cassidy2019; @nuijten2016; @benjaminRedefineStatisticalSignificance2018; @lakensJustifyYourAlpha2018\] quickly turned in a wider metholodogical assessment of psychology's theories and overall practices \[@chambersSevenDeadlySins2017; @scheel2020; @mcphetres2021; @makelQuestionableOpenResearch2019; @johnMeasuringPrevalenceQuestionable2012\]. The term *credibility crisis* is sometimes adopted to refer to this wider approach and often includes four sources of improvement \[@vazire2022\]. First, construct validty, which broadly refers to how concept are defined and measured \[@schererWhatAreEmotions2005; @shumanConceptsStructuresEmotions2014\]. Second, internal validity, which relates to identifying potential causal mecanisms explaining or predicting phenomena of interest \[@pearl2016; @pearl2000; @pearl2018\]. Third, external validity, that is how knowledge built in the *micro-world* of experiments can be generalized to the *macro-world* of intereset. And finally statistical validity, which focuses on the link between domain knowledge and probability/statistical theory \[@mcelreathStatisticalRethinkingBayesian2020; @rodgersEpistemologyMathematicalStatistical2010\].

Given that the field of emotional awareness in computer-mediated learning environments is recent, the thesis mainly focuses on construct and internal validity. The overall aim of the thesis can thus be divided in two intertwined objectives. On the one side, the methodological implementation of an EAT with specific features, among which the connection to appraisal theories of emotion \[@moorsAppraisalTheoriesEmotion2013; @rosemanAppraisalTheoryOverview2001; @sanderModelsEmotionAffective2013; @schererWhatAreEmotions2005; @siemerSameSituationdifferentEmotions2007\] as a means to provide emotional awareness based on emotion as a construct being implemented into the tool. On the other side, the empirical assessment of the causal mecanisms which could explain the adoption, use, perception and instrumentality of emotional awareness in computer-mediated learning environments as a means to evaluate and inform -- adopting a cyclical interaction design perspective [@rogersInterctionDesignHumanComputer2011] -- the implementation of the EAT itself. Both aims are briefly outlined in the remainder of the section.

### Methodolocial Objectives {.unnumbered}

Alongside other scholars [@arguedasOntologyEmotionAwareness2015; @brackettRULERTheoryDrivenSystemic2019; @feidakisReviewEmotionAwareSystems2016; @harleyDevelopingEmotionAwareAdvanced2017; @lavoueEmotionAwarenessTools2020; @molinariEMORELOutilReporting2016], I reckon the way in which emotional awareness is implemented in a computer-mediated learning environment plays a prominent role in determining whether the EAT is adopted in the first place, and whether it serves its purpose in the wide spectrum of potential applications in which it may be deployed. All things considering, the role of an EAT may be broken down to two intertwined factors:

1.  Whether, when, how, and why emotional information is *inserted/encoded* into the computer-mediated learning environments through the EAT, creating thus the necessary -- but not sufficient -- conditions for emotional awareness to emerge. In awareness tools, this function refers to *displaying* information [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002] and is the equivalent of *expressing* emotion [@darwinExpressionEmotionsMan1872; @scarantinoHowThingsEmotional2017; @vankleefSocialEffectsEmotions2017] when applied to an EAT.
2.  Whether, when, how, and why emotional information is *retrieved/decoded* from the EAT and processed by learners, for emotional awareness to be enacted and hopefully contribute to learning processes and outcomes. In awareness tools, this relates to *monitoring* information [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002], which corresponds to *perceiving* or *recognizing* emotion [@hareliWhatEmotionalReactions2010; @schlegelNomologicalNetworkEmotion2017; @hallSocialPsychologyPerceiving2018] in the context of an EAT.

One of the central claim of the thesis is that both functions may be better served with the interaction of three elements: (1) voluntary self-report of emotion; (2) implementing an emotion structure into the tool; and (3) moment-to-moment availability of emotional awareness in the computer-mediated learning environment.

**Voluntary Self-Report**. I consider the opportunity that learners may best profit from emotional awareness through voluntary expression of their emotions [@fuentesSystematicLiteratureReview2017; @mortillaroEmotionsMethodsAssessment2015; @ritchieEvolutionSelfreportingMethods2016; @silvaComparativeStudyUsers2020], as opposed to automatic emotion detection [@jordivallverduHandbookResearchSynthesizing2015]. Self-report may encourage students to interpret, reflect on, extrapolate and construe meaning from emotion [@boehnerHowEmotionMade2007; @fontaineComponentsEmotionalMeaning2013; @lavoueEmotionAwarenessTools2020] in a more flexible and adaptable process compared to the automatic detection of predefined *matches* from uni- or multi-modal sources [@calvoAffectDetectionInterdisciplinary2010; @jordivallverduHandbookResearchSynthesizing2015].

**Implementing an Emotion Structure into the Tool**. I evaluate the possibility that an EAT may best serve its purpose if its functioning is driven by emotion theories [@sanderModelsEmotionAffective2013; @sep-emotion\]. In this regard, the thesis builds on appraisal theories of emotion \[@moorsAppraisalTheoriesEmotion2013; @rosemanAppraisalTheoryOverview2001; @schererWhatAreEmotions2005\] to propose a parsimonious computational model bridging the cognitive evaluation of a situation -- responsible, according to appraisal theories, for emotion elicitation and differentiation \[@schererDynamicArchitectureEmotion2009; @smithPatternsCognitiveAppraisal1985; @siemerSameSituationdifferentEmotions2007\] -- and the resulting conscious emotional experience that learners may use for intra-personal and inter-personal emotion meaning-making \[@grandjeanConsciousEmotionalExperience2008; @schererHumanEmotionExperiences2013\]. It is also argued that implementing an emotion structure provides the conditions to link the use of the EAT and emotional competence within the same framework \[@schererComponentialEmotionTheory2007; @schlegelGenevaEmotionalCompetence2018].

**Moment-to-Moment Availability of Emotional Awareness**. Finally, I surmise that an EAT may increase its usefulness if learners are allowed to insert and retrieve emotional information at any moment [@graesserEmotionsAdvancedLearning2014; @molinariEmotionFeedbackComputermediated2013\]. This refers to a distinction in instructional design between scripting and awareness tools \[@millerScriptingAwarenessTools2015\]. Within a scripting scenario, the learning activity is planned for learners to become aware of relevant information at specific and predefined moments \[@dillenbourgOverscriptingCSCLRisks2002; @fischerScriptTheoryGuidance2013\]. Conversely, awareness tools \[@buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013] bestow learners with the responsibility of *producing and consuming* information at the very moment they see fit, either from an instrumental point of view ("*I think this information is important right now"*) or depending on the possibility to set their mind to it when coordinating multiple actions (*I can share or process this information now*).

The methodological objectives of the thesis are twofold:

1.  Provide detailed information about the implementation of an EAT where these three factors coalesce. To meet this objective, I build on and extend previous work conducted in the Emotion Awareness Tool for Computer-Mediated Interactions (EATMINT) project [@avryAchievementAppraisalsEmotions2020; @avrySharingEmotionsContributes2020; @cereghettiSharingEmotionsComputermediated2015; @Chanel2013; @chanelGrandChallengeProblem2016; @molinariEmotionFeedbackComputermediated2013]. More specifically, a prototype of an EAT named Dynamic Emotion Wheel [@fritzDynamicEmotionWheel2015], designed during an internship in the project and as the subject of my Master thesis [@fritzReinventingWheelEmotional2015], will serve as the basis for the implementation of a functioning proof of concept.
2.  Make abstraction of some fundamental features of the implemented EAT in order to provide a multi-purpose toolbox, which can be adapted to different situations where emotional awareness through the use of an EAT with the aforementioned features is of interest. The toolbox is intended for scholars and practitioners alike, for them to dispose of a ready-to-use EAT, which can be integrated alongside different computer-mediated tasks and environments, without the need for dedicated hardware and with minimal software setup [@cerneaSurveyTechnologiesRise2015\]. Furthermore, the toolbox provides scholars with the possibility to adopt different and multi-dimensional affective spaces as the underlying structure of emotion \[@fontaineWorldEmotionsNot2007; @gilliozMappingEmotionTerms2016; @russellCircumplexModelAffect1980; @schererWhatDeterminesFeeling2006\]. This is also intended to increase comparability of contributions, as well as encouraging sharing of data and material in open formats \[@lowndesOurPathBetter2017; @scheelWhyHypothesisTesters2020; @schererWhatAreEmotions2005; @Nosek1422].

### Empirical Objectives {.unnumbered}

In order to evaluate and inform the methodological objectives, the thesis also provides two empirical contributions, whose objectives are divided in two intertwined levels. The *upper*-level adopts a human-computer interaction perspective [@mackenzieHumanComputerInteractionEmpirical2013; @rogersInterctionDesignHumanComputer2011\] and aims at investigating potential causal factors determining the adoption, use, perception and instrumentality of the EAT with respect to the aforementioned characteristics. The EAT will be assessed in two different computer-mediated learning environments: one adopting a synchronous and collaborative scenario \[@avryAchievementAppraisalsEmotions2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013\], the other asynchronous and individual \[@lavoueEmotionalDataCollection2017; @lavoueEmotionAwarenessTools2020; @molinariEMORELOutilReporting2016]. The individual and comparative assessment of how learners relate to the EAT will provide useful information -- at the theoretical, methodological, and technical levels -- upon which the implementation of the tool can be steered. On a *lower*-level, each empirical contribution has specific research questions, which aim at extending the growing, but sill relatively scant, literature investigating the benefit of endowing computer-mediated learning environments with emotional awareness.

**Emotional Awareness in Synchronous and Collaborative Settings**. Considerable research has been devoted in the last few decades on how collaboration may best be sustained when learners share the temporal but not the spatial dimension [@dillenbourgEvolutionResearchComputersupported2009; @jarvelaEnhancingSociallyShared2015; @kreijnsIdentifyingPitfallsSocial2003; @roschelleConstructionSharedKnowledge1995; @suthersTechnologyAffordancesIntersubjective2006]. It is nowadays widely accepted that one of the determinants of the quality of interaction as well as learning processes and outcomes consists in learners building and maintaining a holistic representation of each others in order to maximize mutual understanding, coordination, regulation and inter-subjective meaning-making \[@dillenbourgSymmetryPartnerModelling2016; @janssenCoordinatedComputerSupportedCollaborative2013; @molinariKnowledgeInterdependencePartner2009; @jarvelaEnhancingSociallyShared2015;\]. More recently, some scholars have started to hypothesize that sharing emotions with the partner could contribute to achieve this goal [@avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @feidakisProvidingEmotionAwareness2014; @molinariEmotionFeedbackComputermediated2013]. The contributions investigating the matter have provided learners -- often divided in dyads -- with exactly the same emotional information for all learners: all the emotions shared by both learners, or none at all when using a control group without emotional awareness. When emotional awareness is provided, this ecological condition is consistent with the attempt to foster learners' mutual modeling [@dillenbourgSymmetryPartnerModelling2016]. At a closer look, though, there are three intertwined layers implicated in construing emotional awareness, which are replicate for each member of the dyad: (1) the learner express her own emotions, and have access to them; (2) the learner has access to the emotions of the partner; and (3) the learners can compare her own emotions with that of the partner. To my knowledge, research has insofar not yet investigated the *individual contribution* of each layer in determining whether, how, when and why learners interact with an EAT. The first empirical contribution therefore attempts to determine to what extent a different use of, and access to the emotional information provided by learners' elicit differences in the way learners relate with the EAT.

**Emotional Awareness in Asynchronous and Individual Settings**. As the recent events linked to the global pandemic have vividly purported, education cannot be transported *as-is* from the classroom to distance settings [@tecfaEnseignerDistanceDans2019\]. This certainly came as no surprise for scholars that have widely investigated potentials and limitations of distance learning in the last decades \[@bates2005technology; @jacquinotApprivoiserDistanceSupprimer1993; @jezegouCreerPresenceDistance2010; @paquelinDistanceQuestionsProximites2011; @sherryIssuesDistanceLearning1995\]. A growing consensus has emerged about the need to attune the cognitive, pedagogical, and socio-affective dimensions in order to take advantage from distance learning and limit pitfalls, among which the sense of isolation resulting from the difficulty learners experience to connect with teachers or peers \[@lowenthalSearchBetterUnderstanding2017; @jezegouCreerPresenceDistance2010\]. In the meantime, over the last decade, research in affective sciences has also stressed the pivotal role emotion plays in inter-personal dynamics \[@fischerSocialFunctionsEmotion2016; @vankleefInterpersonalDynamicsEmotion2018\]. Emotion may contribute to establish affiliation to a group, provide useful cues about social norms and conduct in challenging situations, provide useful information about causes and consequences of behavior, as well as helping to regulate emotional experience in others \[@hareliEmotionsSignalsNormative2013; @reeckSocialRegulationEmotion2016; @rimeEmotionElicitsSocial2009; @vankleefEmergingViewEmotion2010]. The second empirical contribution explores the extent to which learners may benefit from emotional awareness when they do not share neither the spatial, nor the temporal dimensions with their peers.

## Thesis Outline {.unnumbered}

The thesis is organized in four parts. Part I introduces the fundamental theoretical background that is common both to the methodological and empirical parts of the thesis. Chapter 1 gathers inter-disciplinary convergence about the importance of implementing affective phenomena in learning as a means to improve both learning processes and outcomes. Chapter 2 discusses emotion awareness and emotion awareness tools in computer-mediated learning environments, proposing fundamental assumptions about the reasons why it may be of use, as well as sketching the numerous intervining factors that may determine the adoption, use, perception and instrumentality of an EAT. Chapter 3 focuses on emotion theories and appraisal theories more specifically in order to discuss the reasons why they are considered as the reference for the emotional structure to be implemented in the EAT. Finally, Chapter 4 resumes previous work in the design of a prototype, named Dynamic Emotion Wheel, which will serve as the basis for further development of the multi-purpose toolbox.

Part II presents the implementation of the EAT and the toolbox built around it as a proof of concept, that is, a functioning device for which conceptual features have been prioritized over more fundamental practices in software development such as error handling or testing. Chapter 5 discusses the requirements and the overall design principles of the EAT. Chapter 6 then illustrates the inner functioning of the parsimonious computational model that links appraisals criteria to the conscious emotional experience adopting a *k-nearest neighbor* approach with respect to an multi-dimensional underlying affective space. Finally, Chapter 7 briefly outlines how a user-friendly toolbox has been built around this core algorithm for scholars and practitioners to adapt the tool to their needs,as well as the features of the toolbox which are related to comparability of studies and sharing of material and data.

Part III comprises the two empirical contributions, as well as the overall and comparative assessment of the EAT using results from both contributions. Chapter 8 illustrates the experiment in synchronous and collabrative settings, whereas Chapter 9 the observational study in asynchronous and individual settings described above. Finally, Chapter 10 provides the overall and comparative assessment of the EAT.

Part IV concludes the thesis with the general discussion of Chapter 11 and the Conclusion. The Appendices at the end of the manuscript only provide links to supplemental material which are available online.

## Code and Data

<!--chapter:end:10-intro.Rmd-->

# (PART\*) Theoretical Foundations {.unnumbered}

<!--chapter:end:20-theory.Rmd-->

# Theoretical Convergence About the Instrumentality of Affective Phenomena in Computer-Mediated Learning Environments

```{r theoretical-background-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

This chapter provides an overview of the literature, combining different disciplines and perspectives, which corroborates the interest for endowing computer-mediated learning environments with affective phenomena. The chapters starts with an organized overview of current trends in learning psychology and education sciences, which consider the interplay between affect and learning as a crucial interaction to foster learning processes and outcomes. Then, the chapter focuses on affect-aware systems and awareness tools respectively, for an Emotion Awareness Tool (EAT) shares similarities with both systems. The two systems, though, are driven mainly by different perspectives: affective computing in the first case, and Computer-Supported Collaborative Learning in the second. The emergence of an EAT with the characteristics stated in the introduction can therefore be compared and contrasted with the different perspectives provided in the literature.  

## Affect and Learning

The interplay between affect and learning is being investigated at various level of granularity, from the micro-level of neuroscience [*e.g.*, @immordino-yangEmotionsLearningBrain2016] to the macro-level of how education curricula and institution should reform their programs in order to take affective experiences into account [*e.g.*, @brackettPermissionFeelUnlocking2019]. Rather than a layered analysis, though, this section proposes a selection of contributions -- privileging research related to learning with technology whenever possible -- organized in three non-mutually exclusive and highly intertwined categories: (1) research that focuses *primarily* on the effects of learning processes and outcomes on affect; (2) research that focuses *primarily* on the effects of affect on learning processes and outcomes; and (3) research that focuses *primarily* on how either or both directional effects can be mediated or moderated by various forms of socio-affective competences such as emotional intelligence, emotional competence, intrinsic or extrinsic emotion regulation. The main aim of the section is to depict an overall and growing cross-disciplinary and inter-disciplinary interest in considering learning as the results of cognitive, social, and affective interactions, and how emotional awareness through the use of an EAT relates to these research areas.

### The Effects of Learning on Affect

Research investigating the effect of learning on affective phenomena is primarily concerned in determining what affective reactions are elicited by learning processes and situations, and what are the reasons and mechanisms that concur in eliciting such reactions. According to Pekrun [-@pekrunControlValueTheoryAchievement2006; -@pekrunProgressOpenProblems2005], for instance, the scientific study of emotion in education emerged in the last few decades by extension of previous work that was primarily focused on reducing anxiety during test exams. In an effort to broaden the perspective on the interplay between emotion and learning, Pekrun set forth an overarching theory, named the *Control-Value Theory of Achievement Emotions* [@pekrunControlValueTheoryAchievement2006], which accounts for eliciting mechanisms and consequences of learning situations where some form of achievement is at stake (*e.g.*, passing an exam). The theory states that learners evaluate their learning experience according to two criteria: the subjective control that learners feel over the learning process, and the intrinsic value that they confer to the achievement outcome. Different emotions, such as *frustration*, *enjoyment*, or *boredom*, are then elicited based on the specific evaluation that learners make about the situation, combined with the temporal dimension of the evaluation, that is, whether it is retrospective, perspective, or ongoing. The Control-Value theory is depicted more thoroughly in the next chapter, but is presented here as a line of inquiry that aims at explaining how and why certain emotions are elicited by learning processes. In this regard, it is also worth mentioning that achievement emotions are only one possible *kind* of emotion that may be elicited in learning. Pekrun and Linnenbrink-Garcia [-@pekrunIntroductionEmotionsEducation2014] proposes a taxonomy articulated around four *families* of emotions related to learning and educational settings:

1. *Achievement emotions*, already mentioned, elicited by activities or outcomes that refer to standards of evaluation. Examples of achievement emotions are *pride* or *shame* that may occur depending on the result of an exam. 
2. *Epistemic emotions*, elicited by processes that learners enact to assimilate and accommodate new information in the intent to build knowledge. Examples of epistemic emotions are *curiosity* or *surprise* emerging from incoming information that may contradict pre-existing knowledge;
3. *Topic emotions*, elicited directly by the content of the learning activity, independently of achievement or epistemic processes. Examples of topic emotions are *empathy* towards the fate of a character in a novel, or strong affective reactions that may arise from historical or political events;
4. *Social emotions*, elicited by inter-personal dynamics that are pervasive in learning and educational contexts, such as the relationship with colleagues or teachers. Examples of social emotions are *admiration* for a colleague, who was of help in solving a problem, or *anger* towards a free-rider in a group assignment. 

One important element to retain from this taxonomy is that the same *discrete* emotion may pertain to different *families* according to what is the object focus of the eliciting situation [@pekrunControlValueTheoryAchievement2006; @pekrunIntroductionEmotionsEducation2014]. For instance, *frustration* may be triggered by the result of an exam (achievement), by the self-oriented perception of inability to understand information (epistemic), by the unwillingness to manipulate scientific equipment in a science class (topic), or by the difficulty in explaining to others one's opinion (social).

Another research approach pertaining to the effects of learning on affective phenomena consists in focusing on the kind and frequency of specific affective phenomena emerging from learning experiences, which is in itself valuable knowledge both for instructional designers and researchers, without necessarily proposing an eliciting mechanism. Of particular interest to the present thesis, for instance, is a selective meta-analysis conducted by D'Mello [-@dmelloSelectiveMetaanalysisRelative2013] on affective states experienced during individual learning with technology. Combining 24 studies from 5 different countries and from different level of formal education (from middle school to adult students), D'Mello (*ibid.*) computed the relative frequency of different affective states recorded from different sources, from self-report to retrospective video analysis by trained judges. Results show that only one affective state, defined as *engagement/flow*, was consistently frequent in most of the studies. More *traditional* affective reactions such as *anger*, *anxiety*, *boredom*, *fear*, *frustration*, *happiness*, *sadness* and *surprise* were either infrequent, or subject to great heterogeneity between studies.

Reis and colleagues [-@reisAffectiveStatesComputersupported2018], on the other hand, performed a similar meta-analysis, but encompassing affective states during Computer-Supported Collaborative Learning (CSCL) [@dillenbourgEvolutionResearchComputersupported2009; @stahlComputersupportedCollaborativeLearning2006; @suthersTechnologyAffordancesIntersubjective2006], a field more thoroughly depicted later in the chapter. Contrary to the D'Mello (*ibid*), this second meta-analyses integrates also a collective, human-to-human interaction perspective. Starting from an initial corpus of over one thousand contributions between published articles and conference papers, the authors extracted 58 papers from whith they computed the number of papers citing different discrete affective states, belonging to different categories of affective phenomena: personality traits, emotions, moods, and socio-emotional factors. With respect to affective states identified as emotion, the authors tallied 29 discrete emotions that appeared in a minimum of two and a maximum of eight papers. The most frequently cited discrete emotions appearing 4 times or more are: *angry*, *confused*, *disgust*, *pride*, and *tired* (in four papers); *curious*, *excited*, *interested*, *joy*, *sadness* and *surprise* (in five papers); *anxious*, *bored*, *relaxed* and *anger* (in six papers); *fear* (in seven papers); and *frustration* (in eight papers).

This brief selection of contributions corroborates the fact that learning does indeed elicit affective phenomena, but that those phenomena varies greatly depending on a number of situated factors. Research is therefore implicated in determining the mechanisms of elicitation, as well as explaining the potential causes of variance in affective experiences according to different learning contexts. In this regard, an EAT may be deployed in different learning contexts, for instance in individual vs. collaborative settings, and gather information about eliciting conditions and learners' affective experience.

### The Effects of Affect on Learning

Another line of inquiry, which is highly intertwined with the one illustrated in the previous subsection, aims at investigating whether affect in general, and specific affective states more specifically, have effects on learning, for instance by fostering or hindering learning processes or outcomes. Research in this area may vary greatly depending on the level of analysis and the affective phenomena they target. 

With respect to the level of analysis, it may concern the interplay between affective and cognitive processes such as attention, perception, memory and decision making, which are deeply implicated in learning processes and outcomes [@broschImpactEmotionPerception2013; @immordino-yangEmotionsLearningBrain2016; @pessoaCognitiveemotionalBrainInteractions2013]. Research may target more complex and compounded information processing phenomena, such as creativity or problem-solving [@avryAchievementAppraisalsEmotions2020; @avrySharingEmotionsContributes2020; @davisUnderstandingRelationshipMood2009]. But also more inter-personally oriented processes, in which affective phenomena may regulate or undermine collaborative situations [@andriessenSociocognitiveTensionCollaborative2011; @jarvenojaRegulationEmotionsSocially2013; @bakerAffectiveLearningTogether2013], as in the case of emotional contagion [@barsadeRippleEffectsEmotional2002; @parkinsonInterpersonalEmotionTransfer2011].

Concerning the kind and nature of affective phenomena, research often adopts a dichotomy between *positive* and *negative* affective phenomena [@shiotaWhatArePositive2021; @pekrunPositiveEmotionsEducation2002; @hascherLearningEmotionPerspectives2010; @harleyDevelopingEmotionAwareAdvanced2017; @roweUnderstandingRoleNegative2018]. This categorization, even though intuitive at first, is nevertheless problematic at a closer look. Shiota, Sauter and Desmet [-@shiotaWhatArePositive2021], for instance, identify three different ways in which the polarity of an affective phenomena is conceptualized in the literature:

1. the valence of subjective feelings, which usually refer to an assessment of the situation as pleasant or unpleasant, even though the concept of valence is in itself multifaceted [@colombettiAppraisingValence2009; @shumanLevelsValence2013]; 
2. the direction of motivation, which broadly relates to the tendency of approaching or avoiding a given situation; 
3. the desirability or goal-conduciveness of an emotion-eliciting situation, which focuses on the interplay between the situation and what the person is aiming to achieve.

The polarity of an affective phenomenon may therefore change greatly depending on the adopted perspective. For instance, D'Mello and colleagues [@dmelloConfusionCanBe2014] induced confusion in participants by providing, through a virtual agent in a learning environment, contradictory and incorrect information about the topic at hand. The authors found empirical evidence suggesting that if appropriately induced, regulated and resolved, confusion may be beneficial to learning. Is therefore *confusion* positive or negative? If we consider the first criteria proposed by Shiota and colleagues (*ibid*), confusion is usually considered unpleasant, and therefore negative on valence. The situation is nevertheless less clear for the two other criteria: confusion may be considered *approachable* in situations such as enigmas, and even goal-conducive in situations where the initial confusion then evolve in understanding, as in the case of D'Mello and colleagues (*ibid*.). 

Rowe and Fitness [-@roweUnderstandingRoleNegative2018] conducted a qualitative study on *negative* emotions in university's teachers and students, and also found mixing results. According to their analysis, negative emotions promote learning by inciting learners to seek assistance; enhancing cognition; increasing motivational drive; and enhancing productivity or performance. On the other hand, negative emotions inhibits learning by hindering communication; preventing engagement and reducing motivation; impairing cognition; and diminishing productivity or performance.

The overall picture that emerges is necessarily scattered and sometimes contradicting. Affect and learning are two extremely complex phenomena and the combination of the two leads, exponentially, to further complexity. Considering a simple correlation between intrinsic *positive* emotion and *positive* learning outcomes, and conversely between instrinsic *negative* emotion and *negative* learning outcomes, is therefore diminishing of both learning and affect [@hascherLearningEmotionPerspectives2010; @pekrunInternationalHandbookEmotions2014; @shumanConceptsStructuresEmotions2014]. As summarized by Pekrun and Perry [-@pekrunControlValueTheoryAchievement2014, p. 134] "with few exceptions, any emotion can prove to be either adaptive or maladaptive". A similar view is taken by Hascher [-@hascherLearningEmotionPerspectives2010]:

>It is too simple to suppose that negative emotions have negative effects on learning, and positive
emotions have positive effects. What arguments can be found to clarify
a statement like ‘Happiness during learning is negative for the learning process’? The question of
negative effects of positive emotions (and of positive effects of negative emotions) is very
interesting, because it goes beyond plausibility arguments. 
--- @hascherLearningEmotionPerspectives2010, p. 15

In this regard, an EAT takes in somehow an agnostic position, since it may register and convey all sort of affective phenomena that are considered potentially relevant to the learning context in which it is deployed, regardless of whether they are supposed to hinder or foster learning processes and outcomes. At the same time, though, the EAT should build emotional meaning making and awareness on the ground of affective phenomena that are known to play a prominent role in learning processes and outcomes.

### Socio-Affective Competences in Learning

Finally, a third approach in research about affect and learning focuses on empowering learners with skills, allowing them to deal with the social and emotional challenges that they face during various phases of the learning process. Oftentimes, the scope of this line of research goes beyond learning and educational settings, and aims at preparing learners -- especially in early educational programs -- to take advantage of their social and emotional skills in life more generally. Research of this third kind often -- but not always -- refers to the concept of Social and Emotional Learning (SEL), which has a long tradition in education [see @osherAdvancingSciencePractice2016]. Brackett and colleagues [-@brackettRULERTheoryDrivenSystemic2019] provide a recent definition of SEL:

>Today, SEL refers to the process of integrating cognition, emotion, and behavior into teaching and learning such that adults and children build self- and social awareness skills, learn to manage
their own and others’ emotions and behavior, make responsible decisions, and build positive relationships. --- [@brackettRULERTheoryDrivenSystemic2019, p. 144] 

One extensive approach to this form of social and emotional learning is represented by the RULER approach [@brackettEnhancingAcademicPerformance2012; @brackettRULERTheoryDrivenSystemic2019; @hoffmannTeachingEmotionRegulation2020; @nathansonCreatingEmotionallyIntelligent2016], an acronym that stands for:

* Recognizing emotions in oneself and others; 
* Understanding the causes and consequences of emotions; 
* Labeling emotions with a nuanced vocabulary; 
* Expressing emotions in accordance with cultural norms and social context; 
* Regulating emotions with helpful strategies. 

The RULER approach relates to various psychological theories, among which the concept of emotional intelligence as originally posited by Salovey and Mayer [-@saloveyEmotionalIntelligence1990] and more recently updated in light of further development [@mayerAbilityModelEmotional2016]. According to the authors, emotional intelligence is an ability comprising four branches: (a) perceive emotions accurately, (b) use emotions to accurately facilitate thought, (c) understand emotions and emotional meanings, and (d) manage emotions in themselves and others. 

Other models and conceptualization of emotional intelligence have emerged in the last few decades, which have sometimes increased the hype over the concept (*e.g.*, its primacy over *traditional* intelligence), but also contributed to a better assessment of its fundamental nature and relation with other form of intelligence [@chernissEmotionalIntelligenceClarification2010; @mestreModelsCognitiveAbility2016; @petridesDevelopmentsTraitEmotional2016; @schererComponentialEmotionTheory2007; @schlegelGenevaEmotionalCompetence2018; @murphyIntelligenceInterpersonalSensitivity2011; @lockeWhyEmotionalIntelligence2005; @ashkanasyReintroducingEmotionalIntelligence2015]. There is nowadays a growing consensus in differentiating emotional intelligence -- or related formulations -- either as relative stable trait [*e.g.*, @petridesRoleTraitEmotional2004], or an ability or competence that can be trained [*e.g.*, @mayerAbilityModelEmotional2016; @schererComponentialEmotionTheory2007] and also measured from a performance standpoint [@schlegelGenevaEmotionalCompetence2018].

Another line of inquiry bridging learning and affect relates to various forms of regulatory processes, which are considered as fundamental to learning processes both at the individual and collective levels. A consistent body of research, for instance, refers to three types of regulatory processes: (1) self-regulated learning, (2) co-regulated learning, and (3) socially-shared regulation of learning [@jarvelaEnhancingSociallyShared2015; @jarvelaSociallySharedRegulation2016; @millerScriptingAwarenessTools2015; @winneWhatStateArt2015]. According to Miller and Hadwin [-@millerScriptingAwarenessTools2015, p. 574], regulation of learning "can be defined as an intentional, goal directed metacognitive activity in which learners and groups take strategic control of their actions (behaviour), thinking (cognitive), and beliefs (motivation, and emotions) in the context of dynamic social interactions". In self-regulated learning, the person intentionally acts upon her own internal states that may have effect on her learning process. In co-regulated learning, learners engage in mutual, but still individually-oriented, regulation of internal states, in an effort to improve the collaborative task. Finally, in socially-shared regulation, which emerges from the coordination of self- and co-regulated learning, the group as a whole becomes the target of the regulatory processes in a synchronized and productive manner (*ibid*). 

This third line of inquiry, thus, consider affect in the light of integrated processes, which require both effort and *some* form of competence for them to be instrumental to learning. This perspective resonates with the voluntary use of an EAT, which requires effort and inferential processes, but also with the possibility that the use of the EAT may contribute to better regulate socio-affective dynamics.

### Synthesis

This first section of the chapter presented a brief overview of three complementary research areas linking affect and learning. The implementation of an EAT relates to all three areas since it presupposes (1) that learning elicits affective phenomena, so that learners have affect-related information upon which they can reflect and that they can convey; (2) that affective phenomena experienced during learning have an impact on learning processes and outcomes depending on the specifics of the situation at hand, rather then on a predefined list of discrete affective states that always foster or hinder learning ; and (3) that for taking full advantage of the affect-related information, both at the intra- and inter-personal levels, some effort and ability in conveying meaning with, and extrapolating meaning from emotional information is necessary.

## Affect-Aware Systems in Computer Mediated Learning Environments

The pivotal role attributed to affective phenomena in learning and education at large has recently pushed several scholars to investigate modalities for, and benefits in endowing computer-mediated learning environments with systems that integrate affective phenomena [@arguedasOntologyEmotionAwareness2015; @calvoFeelingThinkingComputing2015; @calvoFrontiersAffectAwareLearning2012; @feidakisReviewEmotionAwareSystems2016; @grawemeyerAffectiveLearningImproving2017; @harleyDevelopingEmotionAwareAdvanced2017; @cerneaSurveyTechnologiesRise2015; @calvoAffectDetectionInterdisciplinary2010]. These systems, which are often referred to as affect-aware or emotion-aware systems (*ibid.*), may vary greatly in their complexity, scope, ways to measure affective phenomena, and use of the affective information. For instance, affective systems can be oriented -- exclusively, primarily or equally -- towards the learner's individual affective states, the collective affective states of a group sharing common learning processes and outcomes, the affective states of teachers, or even the affective states of computerized agents implemented into the system, such as embodied tutors [@cerneaGroupAffectiveTone2014; @craigEmoteAloudLearning2008; @lavoueEmotionAwarenessTools2020; @leeTeachersEmotionsEmotion2016; @mantySocioemotionalInteractionCollaborative2020; @naykkiSocioemotionalConflictCollaborative2014]. 

Even though affective systems are mainly driven by autonomic affect detection [@calvoAffectDetectionInterdisciplinary2010] and often aim at providing the learning environment with some form of *adaptive intelligence*, such as affective feedback, that reacts to or acts upon learners affective states [@dmelloFeelingThinkingComputing2015; @harleyDevelopingEmotionAwareAdvanced2017], recent and overarching contributions have attempted to put affective systems in computer-mediated learning environments into broader perspectives [@calvoFrontiersAffectAwareLearning2012; @cerneaSurveyTechnologiesRise2015; @dmelloFeelingThinkingComputing2015; @feidakisReviewEmotionAwareSystems2016; @harleyDevelopingEmotionAwareAdvanced2017]. It is therefore worth to take advantage of the extant literature to seek for commonalities and differences with respect to an EAT with the characteristics stated in the introduction.

### Affective Systems and Affective Computing

Affective systems can be broadly categorized according to the objectives of affective computing, a recent interdisciplinary field investigating "computing that relates to, arises from, or deliberately influences emotions" [@picardAffectiveComputing2000, p. 3]. Picard [-@picardAffectiveComputing2009] identifies four, non mutually exclusive, research areas in affective computing: 

1. technologies for sensing, recognizing, modeling, and predicting emotional and affective states;
2. methods for computers to respond intelligently and respectfully to handle perceived affective information; 
3. technology for displaying emotional information or mediating the expression or communication of emotion;  
4. computational mechanisms that stimulate internal emotions or implement their regulatory and biasing functions. 

Affective systems in computer-mediated learning environments more often relate to the first two categories identified by Picard [@calvoFrontiersAffectAwareLearning2012; @dmelloFeelingThinkingComputing2015; @feidakisReviewEmotionAwareSystems2016; @harleyDevelopingEmotionAwareAdvanced2017]. For instance, in their glossary to an overarching chapter on the integration of affect in advanced learning technologies, D'Mello and Graesser [@dmelloFeelingThinkingComputing2015, p. 2] define an affect-aware learning technology as "an intelligent technology that considers a learner's affective and cognitive states in its pedagogical decision making". The authors (*ibid.*) identify two ways in which affect-aware learning technologies manifest their *intelligence*: (1) by proactively inducing or impeding particular affective states, which are considered as either fostering or hindering learning; and (2) by reacting to specific affective states as they arise, for instance in an attempt to remedy to affective states that may undermine the overall learning experience (see above the general discussion about the interplay between learning and affect). More recently, Harley and colleagues [@harleyDevelopingEmotionAwareAdvanced2017] have extended the dichotomy proposed by D'Mello and Graesser (*ibid*) and adopted it in an effort to provide a theoretically-guided taxonomy for the development of emotion-aware learning technologies, which foster *positive* emotions. The taxonomy itself is not relevant for the present contribution, since it presupposes some form of *intelligence* in the system. It is nevertheless worth mentioning Harley and colleagues (*ibid.*) contribution, for it adopts Pekrun's Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006], already introduced above, as the guiding framework for the taxonomy, corroborating the importance to bridge techno-centric and theoretical approaches.

### An Interactional Approach to Affect^[This section builds on, but extends with more recent contributions, a a similar section in Fritz (2015)]

At first blush, the third research area of affective computing identified by Picard [-@picardAffectiveComputing2009] stands out as the best match for an EAT: technology for displaying emotional information or mediating the expression or communication of emotion. According to Boehner and colleagues [-@boehnerHowEmotionMade2007], though, this particular line of research has been neglected in affective computing, whose mainstream approach considers emotion as an internal and individual phenomenon. They identify this mainstream approach to emotion as *informational*, which they qualify as "rooted in a longer laboratory-science tradition of studying emotion in which subjective experience is considered suspect, to be replaced by objective measures" [@boehnerHowEmotionMade2007, p. 276]. The authors advocate instead an *interactional* approach, which (1) focus on how computers can help users understand their emotions, rather than how computers can understand users’ emotions; and (2) sees emotions as a cultural, dynamic and social phenomena, constructed in action and interaction. The position taken by Boehener and colleagues (*ibid.*) is stated very clearly in this passage:

> The role of affective systems is not to transmit pre-existing emotional units, but to provide a resource for emotional meaning-making. Success of such a system is measured by whether users find the system’s responses useful for interpreting, reflecting on, and experiencing their emotions. Evaluation does not aim at finding a user’s original, "true" emotions, but in tracking how emotions are constructed and interpreted over time, and correlating these dynamics with aspects of system design. --- @boehnerHowEmotionMade2007, p. 287

More recently, Cernea and Kerren [-@cerneaSurveyTechnologiesRise2015], inspired by the work of Boehener and colleagues (*ibid*.), proposed a categorization of technologies *on the rise* for emotion-enhanced interaction. The authors main objective "was to focus on technologies that satisfy two main requirements: having been used increasingly often in affect detection for interaction purposes, and having a proven track of *portability*, *non-intrusiveness* and *low cost*" [@cerneaSurveyTechnologiesRise2015, p. 72]. Even though the authors target more complex affective system -- often using multi-modal sensory information, such as tracking and recognizing facial expressions, eye-movements or pupil dilation, or brain-computer-interfaces -- the categorization they propose is relevant also to a *simpler* system. The authors identify three types of affective systems from an interactional stand-point, proposed here in an condensed form (*ibid.*, p. 78):


1. *Emotion-adaptive systems*: which adapt interaction based on the use affective state *or* mimic affective states with the purpose of enhancing the user experience and efficiency.
2. *Emotion modeling systems*, which do not simply adapt to user affective states, but try to influence these states as well.
3. *Affective-awareness systems*, designed to improve affective self-awareness of a user (i.e., internal affective awareness) *or* awareness of the emotions of other users (i.e., external affective awareness), for instance, in collaborative or competitive environments.

An EAT belongs clearly to the last category of *affective-awareness systems*. The presence of this category corroborates the fact that emotional awareness is considered crucial also from an affective computing perspective. 

### Emotion-Aware Systems in E-Learning

Examples of contribution from an *affective-awareness systems* perspective mainly driven by affective computing is provided by Feidakis and colleagues [@feidakisEndowingElearningSystems2011; @feidakisProvidingEmotionAwareness2014; @feidakisReviewEmotionAwareSystems2016], who investigated emotion-aware systems in computer-mediated environments, both from theoretical and empirical perspectives, highlighting though that the topic has received so far limited attention. Their work is also primarily concerned with intelligent learning environments, but some of their contribution have more similarities with the use of an EAT as posited in the present thesis. 

For instance, the authors draw extensively from emotion theories [@feidakisReviewEmotionAwareSystems2016; @feidakisProvidingEmotionAwareness2014]. Furthermore, in an overarching review of emotion-aware systems for e-learning, Feidakis and colleagues [-@feidakisReviewEmotionAwareSystems2016, p. 217] define emotion awareness as "the *implicit* or *explicit collection of emotion data* and the *recognition* of *emotion patterns*" (italics in the text). They therefore draw a distinction between data derived from autonomic affect detection (implicit), and data provided through first-person subjective report of feelings (explicit). Feidakis and colleagues' work is also relevant to the present contribution because they advocate the adoption of emotion-aware systems into *simpler* learning management systems, whereas affective systems are often deployed in more technology-intensive settings, such as virtual environments or educational games.

### Affective Feedback

Affective systems often -- but not always -- complement affective detection with what are commonly referred to as *affective feedbacks* [@arguedasOntologyEmotionAwareness2015; @feidakisProvidingEmotionAwareness2014; @grawemeyerAffectiveLearningImproving2017; @robisonEvaluatingConsequencesAffective2009], which are often distinguished from more traditional *cognitive feedbacks* [@nelsonNatureFeedbackHow2009]. As the field more generally, affective feedbacks in affective systems are a recent line of inquire and the extant literature is therefore very cautious about their deployment and efficacy, as well as the very nature of what represents an affective feedback (*ibid*.). Broadly speaking, an affective feedback is a reaction of the system that explicitly targets the affective phenomena detected and attempts to instrumentally act upon it to improve learners' situation, either from an affective (*e.g.*, *empathetic*) or task-oriented point of view [@robisonEvaluatingConsequencesAffective2009]. 

Affective feedback is tightly related to a form of *intelligence* in affective systems, which computes appropriate treatment and useful output, and is therefore outside the scope of a simpler system. They nevertheless call for attention in evaluating whether emotional expression can be effective on itself, or it should be coupled with some form of feedback from the system.

### Self-Report From An Affective Computing Perspective

A last element of interest that can be derived from the literature driven mainly by an affective computing perspective concerns how self-report is assessed. Given that this field makes extensive use of autonomic identification of affective phenomena, it is of interest to seek for identified drawbacks in the use of a voluntary, self-report tool. Views from an affective computing perspective can therefore complement, with information derived from computer-mediated environment, more *traditional* assessment about validity and reliability of self-report in measuring affective phenomena [*e.g.*, @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005].

In a recent and overarching contribution about measuring emotions in computer-mediated learning environments -- referred to as Computer-Based Learning Environments (CBLE) in the contribution -- Harley [@harleyMeasuringEmotionsSurvey2015] reviews four main methodologies: facial expression coding, body posture and physiological measurement devices, log file data, and self-report measures. The author confirms that self-report is a widely adopted method of measure even in CBLE, which can be administered before, during or after the learning activities. It has the advantage of being easy to administer, and collected data are generally also easy to analyze and compare, especially when the self-report consists of a set of discrete affective states. On the other hand, though, Harley (*ibid.*), points out that self-report is an *off-line* measure, since learners must divert their attention from the learning activity to the self-report measure. This diversion is particularly disrupting when the self-report is asked at frequent intervals, or is repetitive in its administration. These seem the main drawbacks that are specific, or at least of particular relevance, to CBLE. Other drawbacks identified in the contribution are in fact shared with general and known shortcomings in self-report measures more generally, and can be roughly divided in: (a) authenticity of reported experience; (b) unwillingness to disclose one's own affective experience for normative pressure or social desirability; (c) latency time between the felt affective phenomena and the report, which may introduce bias in recollection; and (d) the action of self-reporting may elicit corollary phenomenon that modifies the original affective state [@harleyMeasuringEmotionsSurvey2015; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005].

### Synthesis

This section provided an overview of affective systems implemented in computer-mediated learning environments from perspectives mainly driven by affective computing. The literature provides similarities between affective systems and an EAT as the one prospected in the thesis. First, there is a growing interest in affective computing for *interactional* affective technologies that provide users with the possibility to impinge on emotion meaning-making. Second, emotional awareness is an active research topic in affective computing, and in computer-mediated learning environments more specifically. Third, even though the field of affective computing was originally theory-agnostic and more focused on technical features in affect detection and processing [@calvoAffectDetectionInterdisciplinary2010], recent developments in the field corroborate a more tight relationship with theories, which can be beneficial in both direction: theory can inform affective systems, and affective systems can inform theory [@calvoAffectDetectionInterdisciplinary2010; @marsellaComputationalModelsEmotion2010; @schererBlueprintAffectiveComputing2010; @pekrunInternationalHandbookEmotions2014].

On the other hand, most affective systems make the assumption that specific affective phenomena can hinder or foster learning and, congruently, deploy proactive or reactive strategies to intervene in the process. As stated in the introduction, the kind of EAT targeted by the thesis attempts to maintain a certain degree of agnosticism towards the instrumentality of specific affective phenomena and bestow learners with the responsibility to integrate the emotional information at hand. In this sense, an EAT is therefore closer to an awareness tool than it is to an affective system. The next section thus proposes an overview of the literature on awareness tools.

## Awareness Tools in Computer-Mediated Learning Environments

Whereas affective systems in computer-mediated learning environments stem directly from affective computing, that is, from an affect-centered approach, awareness tools adopted in learning settings mainly originated from research in Computer-Mediated Communication [@baltesComputerMediatedCommunicationGroup2002; @lickliderComputerCommunicationDevice1968; @fjermestadAnalysisCommunicationMode2004] and Computer-Supported Cooperative (or Collaborative) Work [@dourishAwarenessCoordinationShared1992; @grossUsercenteredAwarenessComputersupported2005; @grudinComputerSupportedCooperativeWork1994]. Consequently, they are essentially driven by an inter-personal perspective, complying with the need to provide users sharing a computer-mediated environment with instrumental information about others to perform the task at hand [@dourishAwarenessCoordinationShared1992; @gutwinDescriptiveFrameworkWorkspace2002; @gutwinWorkspaceAwarenessRealtime1996; @schmidtProblemAwareness2002]. The transition and integration of that heritage in learning settings span different disciplines and approaches, encompassing perspectives that go beyond the communication or cooperation perspective and delves in learning psychology and instructional design [@buderGroupAwarenessTools2011; @engelmannKnowledgeAwarenessCSCL2009; @janssenGroupAwarenessTools2011; @kirschnerAwarenessCognitiveSocial2015; @lowenthalSearchBetterUnderstanding2017; @tuRelationshipSocialPresence2002; @jezegouCreerPresenceDistance2010]. As a result, the concept of *awareness* covers a wide range of intra- and inter-subjective processes that may sustain learning processes and outcomes at different levels -- cognitive, social and affective (*ibid.*) -- and foster regulation at the three levels identified above: self-regulation, co-regulation, and socially shared regulation [@jarvelaEnhancingSociallyShared2015; @jarvelaSociallySharedRegulation2016; @millerScriptingAwarenessTools2015; @winneWhatStateArt2015].

This section provides at first an overview of the literature related to the flexible notion of *awareness* in computer-mediated learning environments, starting by a functional definition of the concept. Next, it focuses on awareness tools more specifically, by highlighting how current trends in the interpretation of the role of these tools are compatible with emotional awareness.

### Awareness Results from Displaying and Monitoring Functions

As stated in the preamble of this section, the term awareness in the context of awareness tools emerged primarily from the fields of Computer-Mediated Collaboration(CMC) and Computer-Supported Cooperative Work (CSCW) as a means to overcome reduced contextual information in computer-mediated environment with respect to face-to-face interaction [@bodemerGroupAwarenessCSCL2011; @buderGroupAwarenessTools2011; @grossUsercenteredAwarenessComputersupported2005; @schmidtProblemAwareness2002]. According to Schmidt [-@schmidtProblemAwareness2002, p. 287], though, "[t]he very word ‘awareness’ is one of those highly elastic English words that can be used to mean a host of different things. Depending on the context it may mean anything from consciousness or knowledge to attention or sentience, and from sensitivity or apperception to acquaintance or recollection". As a consequence, thus, even within CMC and CSCW, the concept has been used with different meanings, with often various preceding adjectives that attempted to better qualify the phenomenon such as *general awareness*, *collaboration awareness*, *peripheral awareness*, *background awareness*, *passive awareness*, *reciprocal awareness*, *mutual awareness*, or *workspace awareness* [@schmidtProblemAwareness2002, p. 286-287 citing use in other articles]. Schmidt (*ibid*) argues that such attempts are doomed to fail, because awareness is not a distinct mental state that exists *per se*: it rather refers to something, and it is *that* something that determines what awareness is. 

Schmidt reckons that there are two intertwined functions that allow awareness to emerge. First, information must be made available, or produced, for awareness to be possible. Schmidt define this function as *displaying*. The emergence is nevertheless not enough, information must also be perceived, or consumed, so Schmidt define a second necessary function, which consists in *monitoring* the computer-mediated environment in search of displayed information. As a consequence, awareness "is not the product of passively acquired ‘information’ but is a characterization of some highly active and highly skilled practices" [@schmidtProblemAwareness2002, p. 293]. In the reminder of the section and the manuscript, awareness is therefore considered in functional terms as the result from the interaction of displaying and monitoring instrumental information.

### Instrumental Information in Computer-Mediated Learning Environments

Defining awareness as the function of displaying and monitoring instrumental information determines the need to frame what is considered as instrumental information. In this regard, when the concept of awareness was transferred into the context of learning, a change of perspective materialized on some important aspects, which are here resumed in three separated points, even though there is ample overlapping and interaction among them.

First, there is a growing consensus in considering that awareness in computer-mediated learning environment should not attempt to reproduce the face-to-face golden standard [@bodemerGroupAwarenessCSCL2011; @buderGroupAwarenessTools2011; @janssenGroupAwarenessTools2011; @kirschnerAwarenessCognitiveSocial2015]. This perspective is warranted by two intertwined assumptions. 

On the one hand, the *universal* primacy of face-to-face over computer-mediated interactions has been challenged by several scholars [@baltesComputerMediatedCommunicationGroup2002; @derksRoleEmotionComputermediated2008; @fjermestadAnalysisCommunicationMode2004; @stoneUnderstandingCoordinationComputermediated2008]. Even though the absence of para-verbal cues that are usually available in face-to-face interactions may impoverish inter-personal dynamics, endowing computer-mediated learning environment with seamless audio/video connection is not the solution to all problems (*ibid*.), especially in asynchronous situations. Rather than the quality of the information provided, in fact, face-to-face interaction increases the possibility and frequency of social exchanges that can sustain learning processes, even in informal or auxiliary forms [@kreijnsIdentifyingPitfallsSocial2003; @kreijnsSocialAspectsCSCL2013]. Kirschner and colleagues [-@kreijnsSocialAspectsCSCL2013], for instance, provide the example of the water cooler as a *social affordance* (*ibid.*), which prompts learners to gather and talk about various aspects of their learning experience. According to the authors, thus, computer-mediated learning environments should not only provide the equivalent of the water cooler, but also ensure that learners use it, since the mere availability of social affordances does not assure neither their use, nor their effectiveness [@kreijnsIdentifyingPitfallsSocial2003]. 

On the other hand, information and communication technologies available in computer-mediated environments allow learners and instructional designers to provide information that is not necessarily available -- or at least hard to notice, process or remember -- in face-to-face interaction [@buderGroupAwarenessTools2011]. In other words, rather then reproduce the contextual information available in face-to-face interaction, instructional designer should endow computer-mediated learning environments with content, activities, or technological artifacts that leverage on computational and human-computer interaction principles to improve learning processes and outcomes. For instance, learners could display their background knowledge about the topic at hand and monitor previous knowledge of their colleagues [@dehlerGuidingKnowledgeCommunication2011; @sanginFacilitatingPeerKnowledge2011], reducing the difficulty in judging what someone knows about a subject [@nickersonHowWeKnow1999; @nickersonProjectiveWayKnowing2001]. Another possibility provided by computer-mediated environment consists in sharing information about learners' levels of participation in collaborative or cooperative settings [@janssenGroupAwarenessTools2011], which may contribute to reduce free-riding and social loafing behaviors [@karauInterpersonalRelationsGroup1993; @salasMeasuringTeamCohesion2015].

Second, even though awareness is considered a secondary -- or corollary -- source of information with respect to the *content* of the learning activity [@janssenCoordinatedComputerSupportedCollaborative2013], scholars are considering a more flexible approach to the amount of resources that can or should be oriented to displaying and monitoring instrumental information that is not directly related to the learning outcome [@dillenbourgSymmetryPartnerModelling2016; @bodemerGroupAwarenessCSCL2011; @kreijnsIdentifyingPitfallsSocial2003]. Rather than minimizing at all costs exogenous activities in an attempt to limit distraction and interference caused by a dual-task [@pashlerDualtaskInterferenceSimple1994], researchers are contemplating trade-offs based on the ratio between the effort of displaying and monitoring information, and the benefit resulting from disposing of that information [@dillenbourgSymmetryPartnerModelling2016]. This trade-off is particularly important since researchers agree that awareness seldom emerges as a natural by-product of the learning task *per se*, but rather needs to be appropriately enhanced [@bodemerGroupAwarenessCSCL2011; @kreijnsIdentifyingPitfallsSocial2003]. In this regard, two non-mutually exclusive pedagogical strategies are often identified in the literature  as a means to enact awareness: *scripting tools* and *group awareness tools* [*e.g.*, @millerScriptingAwarenessTools2015].

Scripting tools [*e.g.*, @dillenbourgOverscriptingCSCLRisks2002; @fischerScriptTheoryGuidance2013] are mainly adopted to scaffold the learning process, so that awareness is created as the results of planned interactions. In other words, learning is organized as a sequence of steps, where some or all of these steps foster or require the emergence of awareness, by explicitly asking learners to display and monitor instrumental information. A simple -- and questionable -- example of this process are the many cases of Massive Open Online Courses (MOOCs), which prompt thousands of users, located all over the world, to present themselves to others, often by posting a message in a forum with information about themselves and their learning goals. More elaborate scripts may require learners to engage in collaborative or argumentative activities, negotiation of goals and means to attain them, or discussion between peers about the quality of productions or interactions (*ibid*). The common factor shared by all the possible activities is the fact that awareness is mainly produced by the *script*, that is, by the learning design itself. 

Group awareness tools [*e.g.*, @buderGroupAwarenessTools2011; @engelmannKnowledgeAwarenessCSCL2009; @janssenCoordinatedComputerSupportedCollaborative2013], often shortened in awareness tools, on the other hand, take a different and mainly unstructured perspective. Rather than sequencing specific moments or activities during which awareness is enacted, group awareness tools bestow learners with the responsibility of displaying and monitoring information whenever it is considered appropriate and instrumental to the learning activity (*ibid.*). In other words, group awareness tools are usually integrated alongside the learning activity -- persistently or *on demand* -- and support learners in making others aware of one's instrumental information, as well as becoming aware of others' instrumental information. As stated by Miller and Hadwin [-@millerScriptingAwarenessTools2015. p. 582], compared to scripting tools, "group awareness tools take a more non-directive or reactive approach placing the locus of control in the hands of the learners". In this regard, Buder [-@buderGroupAwarenessTools2011] -- adopting Schmidt [-@schmidtProblemAwareness2002] functional definition of awareness in learning contexts -- identifies eight features of awareness tools about which the trade-off between effort and benefit of bestowing learners with the locus of control should be assessed. For the displaying function, Buder (*ibid*) identifies the following four features: 

* To what extent information is displayed explicitly or implicitly, with explicit information demanding more effort and therefore potentially interfering more with ongoing learning activity, but also providing more meaningful and reflected-upon information;
* The frequency by which new information is provided, which is inherently linked with the previous feature insofar as the information is displayed explicitly;
* To what extent information is displayed voluntarily or is enforced by the system, for instance by blocking other interactions with the computer-mediated environment until the information is not provided;
* To what extent information is displayed in a closed (*i.e.*, structured) or open (*i.e.*, unstructured) format, where closed formats usually foster immediacy but limit freedom, for instance by clicking a pre-determined choice, whereas open formats allow more flexibility (*e.g.*, a text field where learners can type whatever they see fit).

With respect to the monitoring function, Buder (*ibid*.) identifies four other features on which trade-offs in noticing and processing the available information should be assessed. These features are the following:

* To what extent available information is integrated with the learning content or activity, assuming that, in general, the more distant the information, the greater the effort necessary to switch between the primary learning task and auxiliary awareness;
* To what extent available *chunks* of information are easy or hard to process. For instance, information may be aggregated or graphically represented to foster comparison between information belonging to the learner herself and information provided by other participants to the computer-mediated learning environment;
* To what extent available information foster or pressure the display of further information. Learners may benefit from the social affordance provided by the tool, which reminds them of the interest to share instrumental information; but may also feel coerced to share information by normative pressure, for instance in an attempt to level-up and match the amount of information provided by others;
* To what extent available information prompts immediate or diffuse consequences on learners behavior. Immediate consequences may be of proximal instrumentality, but also disrupt the course of action; whereas diffuse consequences are more flexible, but may require more meta-cognitive and reflective efforts. 

Third, even though there is a tendency to divide group awareness tools in three categories -- *behavioral awareness*, *cognitive awareness*, or *social awareness* [@bodemerGroupAwarenessCSCL2011] -- depending on the information they provide, there is also an increasing consensus in considering these categories as non-mutually exclusive, allowing interactions or spill-over effects between *types* of awareness [@buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013; @kirschnerAwarenessCognitiveSocial2015]. Furthermore, social and affective dimensions are receiving increasing attention in an attempt to compensate "the tendency to restrict social interaction to educational interventions aimed at cognitive processes while social (psychological) interventions aimed at socio-emotional processes are ignored, neglected or forgotten" [@kreijnsIdentifyingPitfallsSocial2003, p. 336]. In this regard, the next two subsections provide examples of theories that integrate the concept of awareness from cognitive, social, and affective perspectives in computer-mediated learning contexts: social presence in distance learning, and mutual-modeling in Computer-Supported Collaborative Learning.

### Social Presence in Distance Learning

The broad concept of awareness can be related with issues that have been identified in the last few decades about the articulation between distance learning and the need for *social presence*, especially in higher-education [@bates2005technology; @garrisonFirstDecadeCommunity2010; @jacquinotApprivoiserDistanceSupprimer1993; @jezegouCreerPresenceDistance2010; @kreijnsIdentifyingPitfallsSocial2003; @rourkeAssessingSocialPresence2001; @tuRelationshipSocialPresence2002]. The definition of social presence and whether this presence is a necessary element for learning to happen is still source of lively debate [@jezegouCreerPresenceDistance2010; @lowenthalSearchBetterUnderstanding2017; @rourkeLearningCommunitiesInquiry2009; @kirschnerAwarenessCognitiveSocial2015]. For instance, Lowenthal and Snelson [-@lowenthalSearchBetterUnderstanding2017] recently collected definitions from the most cited articles on social presence and identified five categories in which different definitions could be divided into (*ibid*, see Table 1 in the original article, p. 144):

* Social presence as *being there*, as in the definition by Dunlap and Lowenthal (2009): "the degree of salience (i.e., quality or state of 'being there') between two communicators";
* Social presence as *being real*, as in the definition by Gunawardena and Zittle (1997): "the degree to which a person is perceived as 'real' in mediated communication";
* Social presence as *projecting*, as in the definition by Rourke and colleagues (1999): "the ability of learners to project themselves socially and affectively into a community of inquiry";
* Social presence as *connecting*, as in the definition by Tu (2002): "the degree of feeling, perception, and reaction of being connected on CMC [computer-mediated communication] to another intellectual entity";
* Social presence as *belonging*, as in the definition by Picciano (2002): "a student's sense of being in and belonging in a course and the ability to interact with other students and an instructor".

Another definition of particular interest to the present contribution is provided by Kreijins and colleagues [-@kreijnsMeasuringPerceivedSocial2011, p. 366], who define social presence "as the degree of illusion that others appear to be a ‘real’ physical persons in either an immediate (i.e., real time/synchronous) or a delayed (i.e., time-deferred/asynchronous) communication episode". This definition explicitly refers to the synchronous and asynchronous temporal dimensions, which are of interest in the empirical contributions in Part III.

The debate about which category or specific definition better defines social presence, though, is beyond the scope of the present contribution, for which rather than the differences, it is worth pointing out commonalities, especially in considering socio-affective phenomena. In this regard, the definition of Rourke and colleagues (1999) clearly cites affect as a necessary dimension for learners' to project themselves in the social *milieu*, in which learning happens. Vaughan and Garrison (2005), cited in @lowenthalSearchBetterUnderstanding2017, states it very similarly: "social presence refers to the potential of participants to project themselves socially and emotionally". A view shared by Swan and colleagues (2008), also cited in @lowenthalSearchBetterUnderstanding2017, for whom "social presence refers to the degree to which learners feel socially and emotionally connected with others in an online environment". In the works of Jézégou [-@jezegouCreerPresenceDistance2010], *socio-affective presence* is one of the three fundamental dimensions of social presence alongside *cognitive presence* and *pedagogical presence*. According to Jézégou (*ibid*):

>the socio-affective presence in e-learning allows to support the cognitive presence resulting from  learners' transactions  to solve a problematic situation. It is generated by social interactions that contribute to establishing the symmetry of the relationship and the friendliness between learners, thus creating a socio-affective climate that is favorable to transactions within a digital communication space. --- @jezegouCreerPresenceDistance2010, p. 267, our translation from French.

There is therefore a growing consensus in considering affective phenomena as an integral part of social presence in distance learning. Displaying and monitoring information about learners' affective states can therefore contribute to enhance the social presence in computer-mediated learning environments.

### Mutual Modeling in Computer-Supported Collaborative Learning

Another field of research that widely adopts awareness as a focal concept is Computer-Supported Collaborative Learning (CSCL) [@dillenbourgEvolutionResearchComputersupported2009; @dillenbourgWhatYouMean1999; @stahlComputersupportedCollaborativeLearning2006; @suthersTechnologyAffordancesIntersubjective2006]. Even though the field is not limited to distance learning, but encompasses the use of computational devices also in face-to-face interaction, research in CSCL has, from the very beginning in the 1990's, reckoned the importance for learners to build a *shared-understanding* of the learning activity [@dillenbourgEvolutionResearchComputersupported2009; @dillenbourgSymmetryPartnerModelling2016; @roschelleConstructionSharedKnowledge1995]. For instance, Suthers [@suthersTechnologyAffordancesIntersubjective2006, p. 332] posits that CSCL is characterized by "processes of intersubjective meaning making and how technological affordances mediate or support such processes". Technology affordances are defined by Norman [-@normanDesignEverydayThings2013, p. 11] as the “relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used” [see also @pucilloFrameworkUserExperience2014; @rizzoOriginDesignIntentional2006; @turnerAffordanceContext2005].

According to Dillenbourg and colleagues [-@dillenbourgSymmetryPartnerModelling2016], learners must engage themselves in two intertwined processes to achieve shared understanding and inter-subjecitive meaning making: (1) a learner has to build and maintain a holistic representation of the colleagues with whom she shares the learning activity; and (2) the very same colleagues must possess sufficient information to build and update a holistic representation of the learner herself. Dillenbourg and colleagues (*ibid*) define the former process as *partner modeling* -- that is "the process of inferring one's partner's mental states" (*ibid*, p. 230) -- and the latter as *mutual modeling*, that is, *bi-directional* partner-modeling, which may happen at various nested levels (*e.g.* Jane speculates that Paul is aware of the fact that she expects him to be more involved in the argumentation).

It is posited, in fact, that the effort learners put into building a symmetrical representation of the partners in a collaborative task is a pivotal determinant of learning processes and, by extension, outcomes [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginFacilitatingPeerKnowledge2011]. In this regard, it is useful to point out some important elements. First, symmetry does not mean that all implicated learners must do, think, know, or feel the same thing. On the contrary, collaborative efforts are inherently characterized by fluctuations between socio-cognitive tensions and relaxations, which are instrumental to the learning process [@andriessenSociocognitiveTensionCollaborative2011; @jarvelaSociallySharedRegulation2016; @winneWhatStateArt2015]. Second, it is worth noting that, unlike other similar concepts implicating interpersonal accuracy [see @hallSocialPsychologyPerceiving2018 for an integrative overview], mutual-modeling does not have to be persistently and precisely accurate, because that would require an effort that may take resources away from the learning activity [@dillenbourgSymmetryPartnerModelling2016]. Mutual-modeling is therefore particularly important at times when there are events of major concern for learners, who may benefit from cues about the suitability of persisting with or changing their behavior -- a process that incidentally shares many commonalities with eliciting events in emotions [@schererWhatAreEmotions2005]. Finally, even though mutual-modeling is mainly driven  from a communicative point of view [@clarkGroundingCommunication1991], it also encompasses a broader variety of sources of information, such as what a person does, thinks, wants, or -- precisely -- feels about herself or about their colleagues [@bodemerGroupAwarenessCSCL2011; @dillenbourgSymmetryPartnerModelling2016; @engelmannKnowledgeAwarenessCSCL2009; @kirschnerAwarenessCognitiveSocial2015; @sanginFacilitatingPeerKnowledge2011; @eligioEmotionUnderstandingPerformance2012]. 

An example of this integrated perspective is provided by Janssen and Bodemer [@janssenCoordinatedComputerSupportedCollaborative2013], which argue against a clear-cut separation between cognitive and social awareness. In their framework, illustrated in \@ref(fig:janssen-bodemer-framework), the authors suggest that there is inter-dependency between cognitive and social information on the one side, and the content and relational space in which the collaboration takes place on the other side. As it will more thoroughly illustrated later in the thesis, emotions are known to play a prominent role both cognitively and socially, and are therefore particularly suited to integrate information at different levels. 

(ref:janssen-bodemer-framework-caption) Framework of relationships between cognitive and social group awareness, coordination of the content and relational space, and effectiveness
of collaboration. From the original Figure 1 in @janssenCoordinatedComputerSupportedCollaborative2013, p. 52.

```{r janssen-bodemer-framework, fig.align="center", fig.cap="(ref:janssen-bodemer-framework-caption)", out.width="100%"}
knitr::include_graphics(here("figure/theory/cognitive-social-group-awareness.png"))
```

### Synthesis

This section provided an overview of the literature about awareness tools in computer-mediated learning environment as a means to display and monitor instrumental information for learning processes and outcomes. The section highlighted how the flexible concept of awareness encompasses a wide variety of potential sources of instrumental information. In this regard, there is an increasing convergence in considering affective phenomena as viable information, for instance, to build and update a social presence in remote learning environment or a holistic representation of the partners in a Computer-Supported Collaborative Learning task. The literature on awareness tools also points out that these tools bestow learners with the locus of control in making the best out of the information provided through the awareness tool. That is, it is mostly up to learners (1) to decide when to display or monitor the information; and (2) how to extrapolate meaning from the available information and integrate it in their course of action during the learning activity. 

## Summary

This chapter provided an overview of the interplay between affect and learning from different perspectives. In computer-mediated learning environments in particular, affective information is integrated through the use of technological artifacts, which provide learners with the possibility to express and perceive affect-related information considered to be instrumental for learning processes and outcomes. This theoretical convergence about the instrumentality of affect-related information therefore needs a concrete, technical way to be implemented. In this regard, this chapter highlighted some features of affect-aware systems on the one hand, and of awareness tools on the other, which have contributed to define a broad sketch of some of the features an EAT should provide, as well as some of the conditions or intervening factors that may influence its use and usefulness. Compared to affective systems, awareness tools are more aligned with the type of EAT contemplated by the present contribution. In the meantime, awareness tools are mostly oriented by the idea that *others* could benefit from information provided by oneself, whereas the individual benefits of displaying information are considered to a lesser extent. As previously mentioned, affective phenomena are not often central in learning environments, and therefore emotional *self-awareness* may also play a prominent role in an EAT. These aspects are further defined in the following chapter. 

<!--chapter:end:21-general-theoretical-background.Rmd-->

---
bibliography: references.bib
---

# Emotional Awareness Tools in Computer-Mediated Learning Environments: Overview, Assumptions, Mechanisms, and Methodological Issues

```{r rationale-eat-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

After a general overview about the relationship between affect and learning in general, this chapter focuses on emotional awareness and, by extension, an Emotion Awareness Tool (EA) more specifically. First, the concept of emotional awareness is defined within the context of computer-mediated learning environments. Second, three main assumptions are derived by this definition. The theoretical underpinnings of each assumption are set forth, complemented by the overview of related works related to the thesis. In the second part of the chapter, an abstract model of an EAT is laid out in an attempt to frame the different interactions between the learners and an EAT, for emotional awareness to be instrumental. The model also allows to highlight a number of design choices that have both theoretical and technical consequences in maximizing the chances for the EAT to be useful to learners. In this regard, the chapter ends with the claim that an EAT may best serve its purpose if it is built upon and integrates a theory-driven emotional structure.

## Definition of Emotional Awareness in Computer-Mediated Learning Environments

Emotional awareness in computer-mediated learning environments is often proposed in the literature as self-explaining or defined somehow implicitly. For instance, as stated in the previous chapter, Feidakis and colleagues \[-@feidakisReviewEmotionAwareSystems2016, p. 217\] define emotion awareness as "the *implicit* or *explicit collection of emotion data* and the *recognition* of *emotion patterns*" (italics in the text). Cernea and colleagues [-@cerneaSurveyTechnologiesRise2015], in their overview of rising technologies in emotion-enhanced interaction, identify the category of affective-aware systems as "designed to improve affective self-awareness of a user (i.e.,internal affective awareness) or awareness of the emotions of other users (i.e.,external affective awareness)" (*ibid.*, p.78). More precise definitions are provided by Lavoué and colleagues [-@lavoueEmotionAwarenessTools2020], which are partially derived from clinical psychology. They first definition provided by Lavoué and colleagues is based upon @bodenFacetsEmotionalAwareness2015. Emotional awareness is defined as "the ability to perceive, identify, and understand emotions" \[@lavoueEmotionAwarenessTools2020, p. 270\]. Later in their article, Lavoué and colleagues (*ibid.*) propose another definition, based upon Rieffe and colleagues [-@rieffeEmotionAwarenessInternalising2008], referring to emotional awareness as "the attentional process by which individuals identify, explain and differentiate between their own emotions as well as the others' emotions" \[@lavoueEmotionAwarenessTools2020, p. 270\]. These two definitions resonate with socio-emotional competences briefly illustrated in the previous chapter. Finally, later in the same article, Lavoué and colleagues (*ibid.*) provide a more technical definition of emotion awareness tools: "\[e\]motion awareness tools can be defined as tools that display information on own' own \[sic\] or partners' emotions, circumstances and antecedents" \[@lavoueEmotionAwarenessTools2020, p. 284\]. Compared to the more abstract definition in Feidakis and colleagues or in Cernea and colleagues (*ibid.*), the definition in Lavoué and colleagues (*ibid.*) implies that the tool shall also provide information about circumstances and antecedents that concurred in eliciting the emotional episode.

From these definitions it is possible to extrapolate three fundamental assumptions about emotional awareness and, by extension, the instrumentality of an EAT in a computer-mediated learning environment:

1.  Learners may benefit from intra-personal emotional awareness by using information about their own emotions as valuable information for their own learning processes (self-regulation).
2.  Learners may benefit from inter-personal emotional awareness by using information about their partners' emotions and/or by communicating their own emotions to their partners and/or by a combination of the two. One or all of these circumstances may contribute to learners' own learning processes (self-regulation), the learning processes of their partners individually (co-regulation), or the learning processes of the group as a whole (socially shared regulation).
3.  Learners may benefit from emotional awareness conveyed by a dedicated tool, which *encodes* emotional information into the computer-mediated learning environment (expressing-displaying function), and *decodes* that information (perceiving-monitoring function), for learners to extrapolate emotional meaning-making instrumental to learning processes.

Each assumption is discussed in the following three sections. Every section proposes an overview of related theories, as well a selection of related works, which are of particular interest for the present contribution in terms of objectives, methodologies, or findings.

## Emotional Awareness at the Intra-Personal Level

The first assumption about the instrumentality of emotional awareness implies that learners can benefit from it at the individual, intra-personal level.

### Theoretical Underpinnings

The role of emotion at the individual and intra-personal level has received over the last few decades extensive consideration from different perspectives and using different methods of investigation. A widespread consensus has emerged over the years about the fact that emotion, rather then disruptive of behavior and in antithesis with cognition, plays a prominent role in helping the organism to cope with a complex environment [@adolphsNeuroscienceEmotionNew2018; @armonyCambridgeHandbookHuman2013; @damasioDescartesError2006; @damasioStrangeOrderThings2018; @schererEmotionTheoriesConcepts2009; @immordino-yangEmotionsLearningBrain2016; @levensonAutonomicNervousSystem2014; @levensonIntrapersonalFunctionsEmotion1999; @leventhalRelationshipEmotionCognition1987; @pessoaCognitiveemotionalBrainInteractions2013; @sanderModelsEmotionAffective2013].

For instance, emotion is known to influence high-level cognitive functions such as attention, perception, memory, and decision-making [@broschImpactEmotionPerception2013], which are all implicated in learning processes. A large body of research suggests that a dedicated system to emotional attention may complement and interact with the exogenous and endogenous systems [@broschAdditiveEffectsEmotional2011]. Emotionally charged stimuli are also known to often bestow precedence in perception over non emotionally-charged stimuli [@broschPerceptionCategorisationEmotional2010]. Many links between emotion and memory have been established both in the encoding and retrieving of information [@sharotHowArousalModulates2004; @kensingerRetrievalEmotionalEvents2020], which may have important consequences for learning [@beegeMoodaffectCongruencyExploring2018; @tyngInfluencesEmotionLearning2017]. Finally, it has been determined that when emotion is not taken into account in decision-making, consequences can be highly disruptive for the person [@becharaRoleEmotionDecisionmaking2004; @rollsEmotionDecisionmakingExplained2014].

Emotional awareness, though, goes a step forward in considering the importance of emotion at the intra-personal level, because it presupposes that learners can benefit from being aware of their emotions [@lavoueEmotionAwarenessTools2020]. Whether consciousness is a necessary condition for emotion to *exist* [@ledouxHigherorderTheoryEmotional2017; @liebermanBooConsciousnessProblem2019] is a debate outside the scope of the present contribution (see also next chapter): the use of voluntary self-report requires consciousness for emotional awareness to emerge in the first place and therefore *dodges* the issue. Conscious processing derived from emotional awareness has been related more specifically to three overlapping factors in learning processes [@lavoueEmotionAwarenessTools2020]: (1) emotion as useful information to direct or redirect cognitive resources; (2) emotion as useful information to assess and guide learners' interest and motivation; and (3) emotion as useful information to regulate one's own behavior, including one's own emotions.

Learners may use their achievement, epistemic and topic emotions as useful cues to evaluate and direct cognitive processes [@pekrunControlValueTheoryAchievement2014]. The work of D'Mello and colleagues [-@dmelloConfusionCanBe2014], for instance, highlights how confusion is a signal of cognitive disequilibrium, which learners are therefore incited to re-balance. The work of Vogl and colleagues [-@voglSurpriseCuriosityConfusion2019] about the role of epistemic emotions suggests that experience of curiosity, pride and shame may inform learners about different reasons for knowledge exploration.

Awareness of one's emotions may prompt learners to assess their motivation to keep engaged or disengage from a learning activity [@linnenbrink-garciaAdaptiveMotivationEmotion2016]. For instance, Baker and colleagues' work [-@bakerBetterBeFrustrated2010] draws attention on the importance of recognizing boredom and frustration as warnings of inefficient and potentially misleading efforts.

Finally, emotion may be used as useful information to regulate one's own behavior, including emotion regulation itself [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]. When performed explicitly rather than implicitly [@torrePuttingFeelingsWords2018], emotion regulation requires the person to be aware of the emotion to be regulated [@lavoueEmotionAwarenessTools2020]. According to Gross, "\[e\]motion regulation refers to the processes by which we influence which emotions we have, when we have them, and how we experience and express them" \[@grossEmotionRegulationAffective2002, p. 282\]. The process model of emotion regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015] posits that regulation can happen at five successive stages:

1.  *situation selection*, which consists in avoiding or experiencing events according to their probability of eliciting unwanted or sought after emotions;
2.  *situation modification*, consisting in attuning the situation once it has been selected or it is forced upon the person;
3.  *attentional deployment*, by which some characteristics of the situation are considered more relevant than others;
4.  *cognitive change*, through which the focal points of the situation can be reappraised in an attempt to shift, prolong or increase an emotional experience;
5.  *response modulation*, which occurs once an emotion has already been elicited and the person attempts, for instance, to suppress its manifestations (*e.g.*, trying not to shake during and oral exam or suppressing a laughter during a lesson).

The fourth and fifth passages have been often considered as two alternative strategies for emotion regulation, with potentially different impact on person's cognition and behavior [@bonannoImportanceBeingFlexible2004; @grossEmotionRegulationAffective2002; @richardsEmotionRegulationMemory2000]. In some circumstances, cognitive change can be more beneficial, since it reassesses the situation in what shall be more favorable terms for the person, freeing resources that would otherwise be monopolized by the undesired emotion [@richardsEmotionRegulationMemory2000]. In other circumstances, a cognitive re-evaluation of the situation may be too demanding, and the person may have more benefits in trying to suppress the response modulation [@bonannoImportanceBeingFlexible2004].

Torre and Lieberman [-@torrePuttingFeelingsWords2018], on the other hand, hypothesize that emotion regulation may occur even implicitly through *affect labeling* [@liebermanAffectLabelingAge2019; @liebermanPuttingFeelingsWords2007; @liebermanSubjectiveResponsesEmotional2011], that is, when people assign a word to the their emotional experience (*e.g.* "I feel angry"). In their article, the authors provide an overview of research about affect labeling and state that it "has demonstrated a modulation of emotional output effects in the same experiential, autonomic, neural, and behavioral domains as found in other forms of emotion regulation" \[@torrePuttingFeelingsWords2018, p. 117\]. The authors also highlight, though, that the mechanisms by which affect labeling intervene as implicit emotion regulation are still unclear. In this regard, Torre and Lieberman (*ibid*.) propose four possible mechanisms:

1.  *distraction*, for resorting to language shift the attention from the situation itself and therefore attenuates full processing of the eliciting event;
2.  *self-reflection*, as a means to initiate an introspection process fostering self-distancing from the emotion;
3.  *reduction of uncertainty*, which results from categorizing an intense and often nuanced experience using known and community-shared words;
4.  *symbolic conversion*, consisting in events assuming symbolic status through the associated word, which may induce more abstract thinking about the eliciting event.

Emotional awareness may therefore be useful either as explicit [@grossEmotionRegulationCurrent2015] or implicit [@torrePuttingFeelingsWords2018] emotion regulation. It may also help learners in evaluating whether a regulatory strategy may be more or less appropriate given the situation at hand [@lavoueEmotionAwarenessTools2020].

TO SUM UP

### Related Works

Molinari and colleagues [-@molinariEMORELOutilReporting2016] developed a self-reporting system from an experience sampling method [@csikszentmihalyiValidityReliabilityExperienceSampling2014] perspective, the EMORE-L (EMOtion REport for E-Learning), which was used in an ecological setting by 16 university students who voluntarily adopted the system during 15 days of distance learning in a blended bachelor program. The system consists in a short online questionnaire that students were reminded to fill once per day through an email that they received at a time previously concerted with the investigators. The EMORE-L consists in nine short questions, which are meant to reduce the amount of time needed to fill-in the questionnaire, organized in 4 parts: a first part about the situation the students were facing, with information about the activity students were conducting; a second part about the cognitive evaluation of the situation on three dimensions (Control, Value, Activation); a third part about the emotional experience, with the possibility of choosing between eight discrete emotions (*pleasure*, *anxiety*, *curiosity*, *boredom*, *engagement*, *confusion*, *surprise*, and *frustration*), for which students could also provide the intensity; and a last part about the social sharing of emotion, with items related to the wish for students to share their emotions with their colleagues, as well as the mutual knowledge of emotions between the student and their colleagues.

Through the 169 questionnaire that were filled throughout the 15 days, Molinari and colleagues (*ibid.*) were able to assess three main exploratory subjects. For each element, the authors used results of the study to discuss consequences on the implementation of emotion self-reporting tool in a context of emotional awareness.

The first subject under scrutiny concerned what emotions were most frequently associated with the different situations a student may encounter during distance learning. In this regard, results highlight four main activities, ordered from the most frequent: reading resources, synthesis of the same resources, individual work, and group work. The three most likely emotion to be experience are *pleasure*, *anxiety*, and *surprise*, whereas students made a scant use of the other five emotions proposed by the list. The authors also evinced from their data that the frequency of emotion changed as a function of the activity. On this ground, Molinari and colleagues suggest that self-report tool should take into account the specific activity that they aim to sustain, avoiding thus generic list of emotions.

The second phenomenon explored in the study investigated to what extent students wished to share their emotions with their colleagues, and in what situation. According to their results, Molinari and colleagues posit that students prefer to share their emotions during individual and group works. Furthermore, students were also more inclined to share their emotions when they experienced *anxiety* or *pleasure*. Based on these results, the author suggest that self-reporting tools shall not be provided at all time, but only when the activity is likely to elicit emotions learners wish to share.

Finally, Molinari and colleagues assessed the contribution of the dimensional approach of the cognitive evaluation items and of the eight discrete emotions in allowing students to express what they felt. The dimensional vs. discrete emotion approach to emotion self-reporting is a longstanding debate [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005], with both approaches presenting advantages and shortcomings in terms of user experience and quality of data (see below in the chapter). Results observed by Molinari and colleagues, augmented by feedbacks from students regarding the use of the system, confirm this trend. For instance, the authors highlight the scant use of the different discrete emotions, with only three out of eight being adopted frequently. At the same time, the cognitive evaluation provided through the *control*, *value* and *activation* dimensions also presented shortcomings: *pleasure*, *anxiety* and *surprise*, in spite of their diversity, were in fact associated to similar appraisal profiles. On this ground, the authors point out how important it is that the self-reporting system helps students in identifying their emotional experience.

Molinari and colleagues (*ibid*) exploratory work provide useful elements with respect to emotional awareness in a voluntary self-reporting setting. On the technical standpoint, the EMORE-L combines antecedents of emotions, in the form of cognitive evaluation on dimensions, with emotional experience expressed as discrete natural language words, even though the two blocks of items are not intertwined in the user experience. Also, even if relatively short, the use of 9 questions to express one's emotions is more suitable for a *scripting* rather than a moment-to-moment emotional awareness.

At a conceptual level, Molinari and colleagues introduced emotional awareness in individual settings, as a means to provide students with self-awareness of their own emotions. At the same time, the authors also measured, though, students' wish to dispose of collective emotional awareness. To this extent, Molinari and colleagues highlight how students expressed the wish to have full control over when and which emotional information to disclose. In addition, they also manifested the wish of being aware of their colleagues emotions, that is, bringing emotional awareness at the inter-personal level, which is the subject of the next section.

## Emotional Awareness at the Inter-Personal Level

The second assumption about the instrumentality of emotional awareness posits that learners can benefit from emotional information available at a collective level, either for their own learning processes, that of their partners, or both at the same time.

### Theoretical Underpinnings

Whereas the intra-personal function of emotion has received extensive attention in the last few decades, several researchers have more recently advocated the need for investigating the inter-personal function of emotion [@parkinsonEmotionSocialRelations2005; @rimePartageSocialEmotions2005; @vankleefInterpersonalDynamicsEmotion2018; @fischerWhereHaveAll2010]. Fisher and Van Kleef [-@fischerWhereHaveAll2010], for instance, posit that

> \[i\]t is an indisputable fact that emotions are mostly reactions to other people, that emotions take place in settings where other people are present, that emotions are expressed towards other people and regulated because of other people. It is hardly possible to imagine the elicitation of anger, shame, sadness, happiness, envy, guilt, contempt, love, or hatred without imagining other people as cause, target, or third-party observer of these emotions. In other words, the social constitution of emotions is beyond doubt -- @fischerWhereHaveAll2010, p. 208.

Keltner and Haidt [-@keltnerSocialFunctionsEmotions1999] identify four level of analysis in which emotion play a social function: (1) the individual level, whenever the source of the emotion is of a social nature; (2) the dyadic level, such as in direct dialogue or collaboration; (3) the group-level, in which people share common identities and goals to attain; and (4) the cultural level, where the analysis focus on macro-elements such as history and tradition. An overarching synthesis on the social functions of emotions across all levels is proposed by Fischer and Manstead [-@fischerSocialFunctionsEmotion2016], who identify two complementary, but distinct, functions performed by emotions: affiliation and distancing. The affiliation function serves to form and maintain positive social relationship with others, whereas the distancing function helps in establishing and maintaining a social position relative to others (*ibid.*). Other social functions of emotion comprise the social sharing of emotion [@rimeEmotionElicitsSocial2009; @rimePartageSocialEmotions2005], the use of emotional information to better understand behavior and appropriate conduct in social situations [@parkinsonEmotionsInterpersonalInteractions2010; @vankleefHowEmotionsRegulate2009; @hareliEmotionsSignalsNormative2013; @hareliWhatEmotionalReactions2010],

Van Kleef [-@vankleefEmergingViewEmotion2010; -@vankleefHowEmotionsRegulate2009; -@vankleefInterpersonalDynamicsEmotion2018] proposes on overarching framework, the Emotion As Social Information (EASI) model, which attempts at integrating the many inter-personal functions of emotion in terms of two classes of mutually influential, but conceptually distinct and empirically separable mechanisms:

1.  *Emotional expressions trigger affective reactions in observers*. The first mechanism implies that the emotion of a person may be the eliciting stimulus of an emotion in the observer. The affective reaction in the observer may be the same as the one expressed by the person, as in the case in emotional contagion [@barsadeRippleEffectsEmotional2002] or empathy [@bloomEmpathyItsDiscontents2016; @wondraAppraisalTheoryEmpathy2015]. For instance, boredom may propagate between learners as the result of the first person pausing and puffing. At the same time, the emotion in one person can trigger a different affective reaction in the observer, but independently of inferential processes about the situation. For instance, the amusement of a colleague during a collaborative task may trigger anger in the observer who wants to stay focused, independently of the reasons why the colleague is amused. Whether the observer affective reaction mirrors or complements that of the person who has expressed it, the reaction will have consequences on the observer's behavior (*e.g.*, deciding to ignore the amused colleague) and/or both the observer and the person who expressed the *original* emotion (*e.g.*, both learners decide to take a break).
2.  *Emotional expressions elicit inferential processes in observers*. The second mechanism implies a more complex and deliberate act of information-processing by which the observer attributes to the expressed emotion potential causes of and consequences on behavior. Especially in appraisal theories of emotion [@moorsAppraisalTheoriesEmotion2013; @moorsFlavorsAppraisalTheories2014; @rosemanAppraisalTheoryOverview2001], which will be more thoroughly depicted in the next chapter, the evaluation a person makes of the situation is pivotal in determining what emotion she may feel. By a form of *reverse engineering* [@hareliWhatEmotionalReactions2010; @schererFacialExpressionsAllow2007], the specific emotion a person is feeling may be used to infer how that emotion came to be elicited.

The EASI model (*ibid.*) also specify two preconditions and two moderator factors that determine the inter-personal dynamics of emotion. The preconditions concern (1) the ability of the person experiencing the emotion to *encode* it in a form that can be communicated to the observer, which broadly refers to the concept of *emotion expressivity* [@kringIndividualDifferencesDispositional1994; @scarantinoHowThingsEmotional2017]; and (2) the observer's ability to *decode* the emotional information and make sense from it using emotional knowledge and understanding, which closely relates to the principle of socio-emotional competences depicted in the previous chapter [@brackettRULERTheoryDrivenSystemic2019; @chernissEmotionalIntelligenceClarification2010; @matthewsScienceEmotionalIntelligence2007]. The two preconditions reflect the displaying and the monitoring functions of awareness tools depicted in the previous chapter [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. As stated by Van Kleef [-@vankleefInterpersonalDynamicsEmotion2018]:

> No matter how informative emotions may be, and however critical their social-regulatory functions, if emotions are not expressed and/or fail to be perceived by others, their signaling function is evidently lost and social interaction may be jeopardized -- @vankleefInterpersonalDynamicsEmotion2018, p.52

The EASI model also proposes two classes of moderator factors that may influence to what extent an expressed emotion may result in an affective reaction and/or an inferential process by the observer (*ibid*.). For instance, it may be the case that the *confusion* expressed by a colleague may trigger an affective reaction of *anger* in the observer, because this slows down the collaboration. On the other hand, the same expression may also push the observer to understand the causes of the colleague's *confusion*, for then proposing some form of learning co-regulation. According to the EASI model, the engagement of the observer in deliberate and more cognitively demanding inferential processes may be facilitated by (1) the observer's motivation and capacity to allocate resources to make inferences about causes and consequences on the sender's behavior; and (2) the perceived *appropriateness* of the expressed emotions according to the observer's criteria of evaluation, for instance in relation to social norms [@fischerSocialFunctionsEmotion2016; @hareliWhatEmotionalReactions2010]. In other words, the observer may try to understand the colleague's confusion if (a) the observer has some *free* resources to dedicate, which may be difficult if the learning context is demanding, and (b) the observer evaluates confusion as an appropriate reaction to the situation, for instance by reckoning that the learning task is difficult.

The EASI theory (*ibid.*) provides an integrative framework of the social-function of emotion, which is founded on the assumption that one's own emotions may guide thoughts, actions and feelings in another person. Using emotion as social information may, for instance, be a first step in the social regulation of emotion, that is, the attempt to voluntary act upon other people's emotion [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013\]. According to Reecks and colleagues \[-@reeckSocialRegulationEmotion2016, p.48\] , "\[t\]he goal-driven nature of social regulation distinguishes it from related phenomena, such as social sharing, empathy, or emotional contagion, where one person's actions are not strategically directed towards influencing another's emotions". The authors propose a cross-disciplinary model that implements the process model of emotion regulation \[@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], illustrated in the intra-personal section, from an inter-personal perspective. In what they call the Social Regulatory Cycle, depicted in Figure \@ref(fig:tf-social-regulation-model), the authors identify the regulator and the target person upon which the emotion regulation is intended. The regulator must proceed, first, by identifying the target emotion. Second, the regulator must assess whether the identified emotion corresponds to a suitable emotion or not. Third, in case of a discrepancy between the two, the regulator must plan a strategy aiming at producing in the target the suitable emotion. Last, this strategy must be implemented. At this point, the strategy may target a different stage of the process in the target (see the description of the individual model above for more details): the selection or modification of the situation; the orientation of the target's attention; the possibility of changing the way the target appraises the situation; or the modulation of the behavioral, physiological and experiential manifestation of the emotion. The cycle may start again either from an individual standpoint (*e.g.*, the target is dissatisfied with the new emotion induced by the regulator) or inter-individual perspective (*e.g.*, the regulator did not obtain the suitable emotion or identify an even more adapted emotion for the target).

(ref:tf-social-regulation-model-caption) The Social Regulatory Cycle at the inter-personal level, fromm the original Figure 2 in @reeckSocialRegulationEmotion2016, p. 51.

```{r tf-social-regulation-model, fig.align="center", out.width="80%", fig.cap="(ref:tf-social-regulation-model-caption)"}
knitr::include_graphics(here("./figure/theory/social-regulation-emotion.png"))
```

TO SUM UP

### Related Works

Eligio, Ainsworth and Crook [-@eligioEmotionUnderstandingPerformance2012] carried out two intertwined experiments aiming at exploring "what collaborators understand about each other's emotions and the implications of sharing information about them" (*ibid*, p. 2046). In the first experiment, the authors asked pairs of same-sex, unacquainted participants to play a collaborative game in a co-located environment, where they shared the same computer equipment. At the end of the collaboration, participants were asked to fill in two questionnaires where they had to rate the intensity of 15 emotions: *happy*, *angry*, *sad*, *fearful*, *angry*, *bored*, *challenged*, *interested*, *hopeful*, *frustrated*, *contempt*, *disgusted*, *surprised*, *proud*, *ashamed* and *guilty*. In the *own* version of the questionnaire, they rated the intensity of each emotion as they have perceived it during the task. In the *partner* version of the questionnaire, participants had to project the intensity by which their partner in the dyad had experienced each of the listed emotion. The administration of the questionnaire was made individually, so that participants could not discuss the matter with their partner, and up until that moment, participants were not informed of the rating, so that they had no particular reason to pay attention neither to their own, nor to their partners' emotions. By comparing the responses to the two questionnaires, the authors concluded that, despite collaborating side-by-side, participants had little understanding of their partner's emotions. On the contrary, consistently with previous findings in the literature [@kruegerTrulyFalseConsensus1994; @tomaAnticipatedCooperationVs2010], participants tended to project their own emotional experience onto their partners. Eligio and colleagues (*ibid*) provides two possible explanations of these results. On the one hand, participants could simply not care about the emotions of their partner. On the other hand, participants could genuinely care about them, but lack the means to focus on them, for instance because the task was too demanding, and they decided to prioritize other mental states perceived as *more* instrumental to the task at hand. In both cases, the attribution of their own emotional experiences to that of the partner is caused by a lack of information available, but the reasons for this shortcoming are not the same depending on the intentions.

In the second experiment, Eligio and colleagues (*ibid*) therefore decided to intervene by providing (or not) participants with explicit awareness of their partner's emotions using a *scripting* strategy, that is, by interrupting the collaboration at specific moments for participants to express their emotions and projecting that of their partner. After filling the *own* and *partner* questionnaire, if participants disposed of awareness, they could look at the emotions of their partner's before resuming the task, otherwise they just continued with the collaboration. Furthermore, dyads were also assigned either in a co-located or a remote collaborative setting, resulting in a 2x2 factorial design with awareness vs. no-awareness, and co-located vs. remote conditions. Using experimental settings similar to the first experiment (but with a slightly different collaborative task and with only women as participants), the authors report evidence suggesting that participants benefited of emotional awareness in terms of performance both in the co-located and remote conditions. Furthermore, participants in the remote condition were more accurate in understanding emotions of their partner and also experienced more *positive affect*, computed by averaging the intensity of *happy*, *interested*, *hopeful*, *excited* and *challenged*.

From the two experiments, Eligio and colleagues (*ibid.*) concluded that participants do not have an accurate understanding of the emotion of their partner if their attention is not explicitly driven to it. Providing emotional awareness seems thus a promising way to increase mutual understanding, since participants with emotional awareness showed higher accuracy in estimating their partner's emotions, as well as better performance to the task at hand.

Following Eligio and colleagues (*ibid.*) findings, Molinari and colleagues [@molinariEmotionFeedbackComputermediated2013] conducted a study in which 30 dyads of same-sex participants (16 dyads of women, 14 dyads of men) performed a collaborative task in remote conditions, with the possibility of audio but not video connection. The aim of the collaborative task was to conceive a slogan against violence in schools using an argument graphic tool [@lundHowArgumentationDiagrams2007]. The task therefore implied some form of negotiation for deciding the final outcome of the collaboration, which was intended to solicit socio-cognitive tensions [@andriessenSociocognitiveTensionCollaborative2011] in participants as a means to elicit emotions. Half of the dyads were randomly assigned to a control condition, in which participants did not dispose of emotional awareness. The other half of the dyads were provided with emotional awareness through the use of a persistent EAT available on the right-side of the screen, with the argument graphic tool occupying the left-hand side of the screen (see Figure \@ref(fig:tf-eatmint-eat)). Contrary to Eligio and colleagues (*ibid*.), in which participants shared emotions at specific moments during the task following a *scripting* strategy, in this case the setting was congruent with an *awareness* strategy, since participants could express and have access to their partner's emotions at any time during the task. A message also showed up after 5 minutes from the last expressed emotion to remind participants to express how they were feeling.

(ref:tf-eatmint-eat-caption) The argument graphic tool and the EAT adopted in @molinariEmotionFeedbackComputermediated2013, from Figure 1 in the original article.

```{r tf-eatmint-eat, fig.align="center", fig.cap="(ref:tf-eatmint-eat-caption)", out.width="80%"}
knitr::include_graphics(here("figure/theory/eatmint-eat-and-task.png"))
```

As shown in Figure \@ref(fig:tf-eatmint-eat), the EAT was horizontally divided in two areas. On top, the monitoring-perceiving area consisted in the last three discrete emotions expressed by the participant (green boxes) and the partner (blue boxes). The box on top of each pile, representing the very last emotion expressed, was highlighted with a paler color. The lighter green box was also editable by the participant, who could write in an emotion not available through the buttons in the lower part of the screen. These buttons represent the displaying-expressing function and are organized in two columns: the right one with 10 *positive* emotions, and the left side with 10 *negative* emotions. According to a private communication with the authors, the list of discrete emotions were defined mixing a previous study in the field [@dmelloDynamicsAffectiveStates2012] and 2 pre-experiments that aimed to identify the most frequent and intense emotions felt during a situation of collaboration, real or imagined. The EAT adopted by Molinari and colleagues (*ibid*.) will be further discussed in Chapter 4.

At the end of the collaborative task, participants individually filled-in a questionnaire aimed at gathering information about (1) the kind and intensity of the emotions of the participant at the end of the task, as well as the emotions the participant attributed to the partner, particularly with respect to the 20 discrete emotions available as buttons on the EAT; and (2) the quality of the perceived interaction based on the frequency by which the participant and the partner provided/imposed their own points of view, defended and argued their ideas, understood their partner's points of view, built up on their partner's ideas, as well as managed emotion during interaction. As for Eligio and colleagues (*ibid.*), also Molinari and colleagues (*ibid*.) report that their result support the hypothesis of beneficial effect from the presence of an EAT, but only in dyads of women and with the presence of mixed results with respect to their initial hypothesis.

The tests upon which Eligio and colleagues (*ibid*.) and Molinari and colleagues' (*ibid*.) result are based, though, do consider the hierarchical structure of the dyads, which may inflate the rate of type I error since the non-independence of observations is not taken into account [@brownIntroductionLinearMixedEffects2021; @singmannIntroductionMixedModels2020; @westLinearMixedModels2015]. As a consequence, the evidence provided by the two studies should be taken with caution. The experimental settings, on the other hand, are interesting and directly compare a *scripting* and an *awareness* perspective, with the latter presenting the advantage of persistent emotional awareness. The two studies also highlight the many methodological challenges in determining the effect of an EAT in computer-mediated learning environments, which concern how emotional awareness is provided, but also how the potential benefit are measured and analyzed.

A different approach was taken by Avry and colleagues [-@avryAchievementAppraisalsEmotions2020], who, rather than providing awareness through discrete emotions, took a dimensional approach adopting Pekrun's Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006] introduced in Chapter 1. In their study, the authors asked 28 dyads of same-sex participants to play a remote collaborative game. Beside the game window, participants disposed of smaller window on the screen which reported two feedbacks: (1) a feedback on how well the dyds was mastering the game, representing the Control dimension; and (2) how well they were performing in a standing compared to other dyads, representing the Value dimension. The two feedbacks were manipulated by the authors as to create a 2x2 factorial design combining high vs. low Control on the one hand, and high vs. low Value on the other. After the collaborative task, participants filled in a questionnaire on which they evaluated the overall collaboration in terms of (a) affective dimensions (Valence, Dominance, and Activation) and 16 discrete achievement emotions, and (b) six computer-supported collaborative exchanges (sustaining mutual understanding, information pooling, transactivity, reaching consensus, task management, and time management). For both category of measures, the rating was performed for the participant herself, as well as what the participant thought her partner would have experienced. The results obtained by Avry and colleagues (*ibid.*), who took into account the hierarchical structure of dyds by checking the intraclass correlation of dyads, corroborate an effect of the manipulation of the Control-Value appraisals on both category of measures (affective and socio-cognitive). These results are particularly interesting considering that in this study, a form of emotional awareness was (1) manipulated, and (2) conveyed through a feedback aimed at eliciting a certain kind of appraisal of the situation, which influenced the kind of discrete achievement emotions experienced.

The three studies outlined in this section provide very different approaches to the study of emotional awareness in computer-mediated learning environments. Since this is a recent and cross-disciplinary field of inquiry, a fragmented stage of research is inevitable [@fiedlerCycleTheoryFormation2004]. On the other hand, efforts should also be dedicated to the possibility of direct comparison and incremental building of knowledge. In this regard, the adoption of a prototypical EAT, whose eventual differences can be more easily defined through objective parameters, could be beneficial.

## Emotion in Computer-Mediated Learning Environments

The third assumption underlying emotion awareness tools concerns the possibility to create emotional awareness in computer-mediated learning environments.

### Theoretical Underpinnings

For emotional awareness to emerge and be available, it is necessary that emotion may be fruitfully *encoded* and *decoded* in a computer-mediated learning environment. In this regard, the computer-mediated environment is somehow ambivalent with respect to affect-related phenomena. On the one hand, the combined presence of a learning task and a technological device captures learner's attention and thus, even in the case of co-located interaction or seamless audio/video connection in remote settings, diminishes the possibility to pay attention to efferent cues of affective experiences -- such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009] -- that are more readily available in face-to-face interaction [@baltesComputerMediatedCommunicationGroup2002; @lundHowArgumentationDiagrams2007]. On the other hand, the same presence of a technological device provides alternative or complementary means to create emotional awareness and even extend it over time [@derksRoleEmotionComputermediated2008; @gliksonDarkSideSmiley2018; @hegartyCognitiveScienceVisualspatial2011; @leonyProvisionAwarenessLearners2013; @derickEvaluatingEmotionVisualizations2017].

The presence of emotion in computer-mediated learningn environments even without seamless audio/video connection can be sustained by a corollary to the EASI model, illustrated in the previous section, which Van Kleef [-@vankleefSocialEffectsEmotions2017] identifies as the *functional equivalence hypothesis*. The hypothesis posits that the role emotion play at the inter-personal level is equivalent regardless of the specific way it is communicated, as long as the emotional information passes from the sender to the receiver. In the words of the author:

> If one accepts the notion that emotional expressions can influence social interactions by providing information about what is on the expresser's mind, it follows that emotions can have such effects regardless of how they are expressed, as long as the expressions convey the relevant information. Consequently, EASI theory posits that expressions of the same emotion that are emitted via different expressive modalities (i.e., in the face, through the voice, by means of bodily postures, with words, or via symbols such as emoticons) have comparable effects, provided that the emotional expressions can be perceived by others --- @vankleefSocialEffectsEmotions2017, p. 213

On the other hand, the fact that various forms of emotional expression are equivalent with respect to their social function does not mean that each way of expressing an emotion is equivalent [@vankleefInterpersonalDynamicsEmotion2018]. A colleague's anger expressed by shouting on your face how badly your part of a collaborative task has been handled has not the same intensity of *I am very angry with you* written in an email. A diminished *veracity*, though, may be compensated by more favorable conditions to engage in inferential processes rather than affective reactions as described by the EASI model (*ibid.*). As stated in Chapter 1, for awareness tools more generally, even in the case of emotional awareness, face-to-face interaction may not be the golden standard to aim for [@buderGroupAwarenessTools2011; @boehnerHowEmotionMade2007]. An EAT whose aim is to make learners' aware of their own and/or their colleague's emotions must therefore face the challenges of *encoding* and *decoding* emotional information, so that it maximized the preservation of its *functional* meaning.

In this regard, the *encoding* of emotional information has received so far greater attention than *decoding*, since it is tightly related to the action of detecting, representing or measuring emotions [@fuentesSystematicLiteratureReview2017; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @silvaComparativeStudyUsers2020]. In their review of technologies for emotion-enhanced interaction already cited in Chapter 1, Cernea and Kern [-@cerneaSurveyTechnologiesRise2015] distinguishes between three types of techniques commonly adopted to estimate emotions: (1) perception-based estimation, derived from efferent manifestations of emotion such as facial expressions, vocal prosody or body posture [@banzigerEmotionRecognitionExpressions2009; @martinezContributionsFacialExpressions2016]; (2) physiological estimation, based on the detection of physiological patterns such as heart rate, blood pressure, or skin conductance [@ragotEmotionRecognitionUsing2018; @shuReviewEmotionRecognition2018]; and (3) subjective feelings, based on the person's self-report of her own emotional experience [@lavoueEmotionalDataCollection2017; @ritchieEvolutionSelfreportingMethods2016; @silvaComparativeStudyUsers2020]. As stated in the introduction, the thesis focuses on this last category. In fact, physiological estimation requires dedicated hardware and software, which would be difficult to provide at scale. Perception-based estimation may use more widely available hardware and software, for instance through a webcam or keyboard stroke, but the accuracy and usefulness of this kind of measure is still lively debated in the literature [@barrettEmotionalExpressionsReconsidered2019; @bahreiniMultimodalEmotionRecognition2016; @nahinIdentifyingEmotionKeystroke2014]. Barrett and colleagues [-@barrettEmotionalExpressionsReconsidered2019], for instance, argue that automatic recognition from facial expression are still limited in reliability, lack in specificity, and does not take sufficiently into account effects of context and culture. Voluntary self-report is therefore retained as the most parsimonious, portable and reliable way to provide emotional awareness both at the intra-personal and inter-personal levels, especially when the aim is to stimulate voluntary emotional introspection and/or inferential processes [@boehnerHowEmotionMade2007; @vankleefInterpersonalDynamicsEmotion2018].

Emotional self-report is tightly related to the underlying concept of *what an emotion is*, which will be discussed in Chapter 3. From a technical standpoint, though, emotion self-report is traditionally characterized by the dimensional or the discrete emotion approaches, and less frequently by a combination of the two [@bradleyMeasuringEmotionSelfAssessment1994; @cowenSelfreportCaptures272017; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005\]. In the dimensional approach, emotion is conceptualized as a point in a space determined by one, more often two, and more rarely three or more dimensions. The most widely adopted dimensional approach to emotion consists in the Valence x Arousal circumplex \[@russell1980; @stanley2009\]. More recent findings (see Chapter 4) suggest, though, that two dimensions are not enough to account for the sheer variety fo emotional experiences that a person can have \[@fontaineWorldEmotionsNot2007; @fontaine2013\]. In the discrete approach, emotion is conceptualized as a phenomenon with distinctive features compared to other emotions, which may consist in different facial expressions when emotion is represented graphically, or semanting meaning when emotion is represented by natural language words or idioms \[@torrePuttingFeelingsWords2018; @desmet2005; @fontaineComponentsEmotionalMeaning2013\]. The dimensional and discrete approaches can be combined by providing discrete emotions a precise collocation on the dimensions. The result is often referred to as an affective space \[@shuman2014; @schererWhatDeterminesFeeling2006; @gilliozMappingEmotionTerms2016].

Emotional *decoding* in a computer-mediated environment is tightly linked on how emotion is *encoded* and has received so far limited attention [@bersetVisualisationDonneesRecherche2018; @derickEvaluatingEmotionVisualizations2017; @leonyProvisionAwarenessLearners2013]. Furthermore, visualization of emotion in this context is usually derived from affective information available in students' productions or communication exchanges, for instance through sentiment analysis rather than a dedicated tool [@leonyProvisionAwarenessLearners2013]. In the absence of specific representations for emotional awareness, more general guidelines about the visuo-spatial representation of data should be applied [@hegartyCognitiveScienceVisualspatial2011; @hehmanDoingBetterData2021]. Emotional *decoding* is also influenced by the objective of the visualization. Learners' emotion may be represented individually or collectively, as a single unit in time or grouped through short or long time spans. They may also be partially or fully available to all other students or only to a selection of colleagues (e.g. in a group work). In other words, if the aim of emotional *encoding* is rather straightforward, emotional *decoding* depends on a larger number of factors.

### Related Works

There is scant work that has been dedicated to instruments that combine both emotion *encoding* and *decoding* in a computer-mediated learning environment, especially in real-time [@lavoueEmotionalDataCollection2017; @ez-zaouiaEmodashDashboardSupporting2020\]. As pointed out by Lavoué and colleagues \[@lavoueEmotionalDataCollection2017\], the few attemps that have been made are mostly ad-hoc solutions that do not aim at a general use. As a result, in learning settings, emotion self-report is often provided through questionnaires \[@pekrunMeasuringEmotionsEpistemic2016; @pekrun2011\] or adaptation of experience sampling methods \[@scollonExperienceSamplingPromises2003; @csikszentmihalyiValidityReliabilityExperienceSampling2014\] -- as in Molinari and colleagues discussed above \[@molinariEMORELOutilReporting2016\] -- whereas emotion representation is even less frequent and therefore less developed \[@ez-zaouiaEmodashDashboardSupporting2020; @bersetVisualisationDonneesRecherche2018]. Ez-zaouia and colleagues, for instance, developed EMODASH, a dashboard for visualizing retrospective emotions for tutors in an online learning environment. To the best of my knowledge, there is only a hanful of tools reported in the literature that come close to the purpose of an EAT as it is consireded in the preent contribution.

## Abstract Model of the Functions of an Emotion Awareness Tool

The information illustrated above, as well as in the previous chapter, can be integrated in an abstract model that depicts the mechanisms allegedly carried out by an EAT. The proposed abstract model does not have the pretension to establish an absolute reference; on the contrary, it aims at visually implementing the different passages that are (or may be) assumed in the instrumentality of an EAT as a support to learning activities in computer-mediated environments. The model, depicted in Figure \@ref(fig:intro-thesis-scm-model-figure), take the perspective of a learner that disposes of an EAT in its computer-mediated learning environment. The model comprises boxes for four conceptual elements: (a) the learning activity, which may refer broadly to any computer-mediated environment implementing an instructional design; (b) intra-personal emotion, representing a single event or a set of events corresponding to the learners' expressed-displayed emotions; (c) inter-personal emotion, representing a single event or set of events corresponding to emotions expressed-displayed by other learners sharing the same environment, that the learner herself can therefore perceive-monitor; and (d) an overarching *meaning-making* process, which encompasses learner's effort to extrapolate instrumental information from the emotional information available. The boxes are connected with directional arrows, numbered from 1 to 7, representing a series of processes that are (or may be) implicated in each passage. The numbers are used for identifying the passages, but do not imply a fixed order, even though some processes may be built upon previous passages. The next sections provides a discussion of each passage (*i.e.*, each numbered arrow) more specifically.

(ref:thesis-scm-caption) The EAT determines how an intra-personal emotion becomes inter-personal, and how the inter-personal emotions contribute to improve the shared understanding of the situation at hand. The situation may then trigger other intra-personal emotions.

```{r intro-thesis-scm-model-figure, fig.cap="(ref:thesis-scm-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/intro/thesis-scm-model.png"))
```

### From the Learning Activity to Intra-Personal Emotion

Passage #1 goes from the learning activity to an intra-personal emotion, implicating that the learning activity elicits emotions in oneself. An EAT may intervene in this passage in several ways, grouped here in a non exhaustive lists of X phenomena labeled for illustration purpose as *emotion alertness* and *emotion conceptualization*.

Emotion alertness broadly refers to the fact that the presence of the EAT represents a general reminder about paying attention to emotion during the learning activity, and therefore may increase the *sensitivity* about emotional experience. In other words, the presence of the EAT may push learners to bestow to emotional self-awareness more attention that they would normally do without the presence of the EAT [@brackettRULERTheoryDrivenSystemic2019; @lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013]. Emotion alertness may be induced to different degrees, depending on, for instance, whether the EAT is always available or not, and whether explicit prompts are frequently sent to learners [@csikszentmihalyiValidityReliabilityExperienceSampling2014; @shiffmanEcologicalMomentaryAssessment2008]. As an example, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented an EAT into a computer-mediated collaborative task that reminded participants to self-report their emotions after five minutes since the last emotion expressed.

Emotion alertness is not necessarily and automatically linked to concrete action of emotion display, that is, self-reporting the emotion into the system. In fact, even in the case of an increased alertness, learners could genuinely not feel any emotion, or decide not to express their emotion through the EAT for dispositional (*e.g.*, emotional expressivity, see @kringIndividualDifferencesDispositional1994) or contingent reasons (*e.g.*, bestow priority to the learning task at hand).

Emotion conceptualization broadly refers to the mechanisms by which learners are planned to self-report their emotions, which may vary greatly according to a number of theoretical and technical aspects. For instance, emotion self-report tools or questionnaires are very often divided in two categories, depending on whether they adopt a dimensional or a discrete perspective [@broekensAffectButtonMethodReliable2013; @fuentesSystematicLiteratureReview2017; @ritchieEvolutionSelfreportingMethods2016; @schererWhatAreEmotions2005].

In a dimensional approach, an emotion is conceptualized as a rating on a number of continuous criteria, among which the most frequently adopted are *valence* (also called *pleasure*) and *arousal* (also called *activation*) [*e.g.*, @russellCircumplexModelAffect1980; @stanleyTwodimensionalAffectiveSpace2009], but may also comprise different or additional dimensions. It is the case, for instance, of the Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] or the AffectButton [@broekensAffectButtonMethodReliable2013], which both use the *valence*, *arousal*, and *dominance* dimensions, even if in different formats. The Self-Assessment Manikin [@bradleyMeasuringEmotionSelfAssessment1994] uses three rows of figures (*i.e.*, the *manikin*), where each rows represent one of the three dimensions. For each row, 5 figures, progressively modified in some features, represent the increasing or decreasing value on the specific dimension. The AffectButton [@broekensAffectButtonMethodReliable2013], on the other hand, uses a single iconic facial expression that changes according to the user's coordinates of the mouse on the surface of the icon: the horizontal movement determines the pleasure dimensions; the vertical movement the dominance dimension; and the arousal dimension is calculated according to the distance of the mouse from the central point of the image.

In a discrete approach, an emotion is conceptualized as a distinctive experience that can be identified through a verbal and/or graphical representation [@mortillaroEmotionsMethodsAssessment2015]. The number, kind and organization of the representations varies depending on several aspects. In the case of verbal representations that adopts natural language nouns or adjectives, for example, the list may be determined according to emotion theories -- as in the case of basic emotion theory [@ekmanArgumentBasicEmotions1992] -- or determined empirically according to previous (or pilot) studies, often with the aim of retaining a list of the most frequently experienced or expressed items. For instance, Molinari and colleagues [-@molinariEmotionFeedbackComputermediated2013] implemented in a self-report interface 20 buttons, 10 labeled with negative and 10 with positive emotion adjectives retrieved among the most frequently expressed during a computer-mediated collaborative task in a pilot study. The interface also provided -- as it is considered a good practice [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005] -- the possibility to provide another emotion not in the list. When emotion is conceptualized graphically, representations usually attempt at maintaining some degree of analogy with *reality*, for instance with respect to facial expressions [@desmet2003measuring; @lauransNewDirectionsNonVerbal2012].

Less frequently, self-report tools attempt to combine the two approaches, as it is the case in the Geneva Emotion Wheel [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005], depicted in more details in the next chapter, or the Mood Meter application associated with the RULER approach [@brackettRULERTheoryDrivenSystemic2019] mentioned in the previous chapter. When the dimensional and discrete approaches coalesces, they create an underlying structure, which is often referred to as an *affective space* [*e.g.*, @davidsonParsingAffectiveSpace19940101; @gilliozMappingEmotionTerms2016; @schererWhatDeterminesFeeling2006; @stanleyTwodimensionalAffectiveSpace2009]

The Mood Meter application ...

The emot-control system implemented by Feidakis and colleagues [-@feidakisProvidingEmotionAwareness2014] shows on the same interface (a) a circumplex of 14 discrete affective states, represented by stylized, and colored faces similar to smileys/emoticons, organized according to the valence x activation orthogonal axis; (b) 5 additional images of similar kind, but representing moods in a sort of 5-point Likert scale (*sad*, *unhappy*, *neutral*, *happy*, and *very happy*); (c) a free text input to type-in another emotion not listed; and (d) another free text input introduced by the field legend *Write more about your feelings*.

Depending on how emotion is conceptualized and, by extension, planned for self-report, the intra-personal awareness of emotion may be more or less guided, inducing learners to pay attention to some elements of their emotional experience that they will not necessarily consider in the same way, or with the same weight, depending on the specifics of the EAT at hand. At the same time, emotion self-report also depends on the learner herself, for instance in terms of her ability to discriminate between emotion ... ...

### From Intra-Personal Emotion to Meaning-Making

Once an emotion or a set of emotions are self-reported, it is assumed that learners may extrapolate meaning from the emotional information available. In this regard, information may be punctual, for example the last emotion or set of emotions displayed concurrently. Conversely, the system may also implement some form of data persistence, which allows to observe the accumulation and the evolution of the emotional experience over a varying time span. Depending on the way emotion information is also graphically represented, emotional meaning-making can be guided by the system, which may privilege some form of representation (*e.g.*, changes over time) over another (*e.g.*, cumulative frequency of the same emotion expressed).

Meaning-making is also tightly related to the way emotion is displayed through the system (passage #1), not only from a technical standpoint -- since the information available is determined by what information is inserted into the system -- but also conceptually. If emotion is considered from a dimensional standpoint, learners will extrapolate meaning from the extant criteria through which dimensions are rated. On the contrary, in a discrete emotion approach, learners extrapolate meaning from an *information unity* that is supposed to provide unique meaning compared to other possible choices. Finally, if the system combines dimensional and discrete approach, learners can extrapolate meaning from both.

The system may also influence meaning-making by adding contextual information to the way emotion is displayed. For instance,

### From the Learning Activity to Inter-Personal Emotion

Passage #3 starts a complementary path with respect to passages #1 and #2, but from an inter-personal perspective, that is, concerning emotion in others learners sharing the same computer-mediated learning environment with the learner herself. This path may in fact be optional when the EAT is exclusively concerned with emotion self-awareness [*e.g.*, @lavoueEmotionAwarenessTools2020], whereas it becomes an integral part when the EAT is inspired from a Group Awareness Tool perspective [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @feidakisProvidingEmotionAwareness2014; @molinariEmotionFeedbackComputermediated2013].

### From Inter-Personal Emotion to Meaning-Making

Passage #4 consists in extrapolating meaning from the emotion of other learners sharing the same computer-mediated learning environment and whose emotions have been monitored by the learner herself.

### From Intra-Personal Emotion to Inter-Personal Emotion

Expression of emotion: emotion is made available to others depending on learners willingness to disclose their emotional experience.

Emotion influence.

Emotion sharing.

### From Inter-Personal Emotion to Intra-Personal Emotion

Emotion comparison (e.g. affiliation or differentiation). Emotion contagion.

### From Meaning-Making to the Learning Activity

Emotion is integrated into the learning activity, it modifies behavior (self-regulation, co-regulation, socially shared regulation).

Modification of learning activity can trigger another emotion, creating cycle.

## Methodological Issues

For instance, Pekrun and Linnerbrink-Garcia \[@pekrunIntroductionEmotionsEducation2014, p. 2\] point out that "\[i\]n the broader education literature, affect is often used to denote a broad variety of noncognitive constructs including emotion but also including self-concept, beliefs, motivation, and so on \[...\]. In contrast, in emotion research, affect refers to emotions and moods more specifically".

## Synthesis

<!--chapter:end:22-emotion-awareness-tools.Rmd-->

# Defining the *Unit* of Measure to Self-Report and Convey Emotional Awareness

```{r emotion-definition-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```



## What is an Emotion?

*What is an emotion?* is a question that literally appears in a multitude of scientific contributions. For instance, it is the title of a seminal article by William James [-@jamesWhatEmotion1884], which, despite being more than a century old, is still source of lively debate [@ellsworthWilliamJamesEmotion1994; @schererWhatAreEmotions2005]. There are many iconic statements that testify to the difficulty of studying emotions from a scientific perspective. Fehr and Russel [-@fehrConceptEmotionViewed1984, p. 464] argued that "[e]veryone knows what an emotion is, until asked to give a definition", which resonates with Kleinginna and Kleinginna [-@kleinginnaCategorizedListEmotion1981], who, around the same time forty years ago, collected almost one hundred definitions. More recently, Frijda and Scherer [-@frijdaEmotionDefinitionsPsychological2009, p. 142] still confirmed that "emotion may be one of the fuzziest concepts in all of the sciences", which was empirically corroborated by Izard [-@izardManyMeaningsAspects2010], who attempted -- and failed -- to extrapolate a unitary definition from the responses of 35 leading researchers from different fields to 6 questions about emotion definition, function, activation and regulation.

In the last few decades, though, a growing consensus has emerged over a *minimal*, consensual definition of some characteristics of emotion that are shared by different scholars and models of emotion [@frijdaEmotionDefinitionsPsychological2009; @kleinginnaCategorizedListEmotion1981; @moorsTheoriesEmotionCausation2009; @sanderModelsEmotionAffective2013]. Sander (2013, p. 16) resumes this consensus in four points: (1) emotions are multicomponent phenomena, spanning affective, cognitive, behavioral and physiological processes ; (2) emotions are a two-step process involving *emotion elicitation* mechanisms that produce *emotional responses*; (3) emotions have relevant objects, being triggered by internal or external events/stimuli that are relevant for the organism; and (4) emotions have a brief duration compared with other affective phenomena such as moods, attitudes or preferences.

Within this consensual framework, though, different positions persist, which have both theoretical and applied consequences in the study of emotion *per se*, as well as in conjunction with other phenomena. These differences are often rooted in historical, cultural and epistemological contexts, within and across disciplines and approaches -- see @dixonPassionsEmotionsCreation2003 and @plamperHistoryEmotionsIntroduction2015 for overarching perspectives, or @sanderModelsEmotionAffective2013 for a more concise introduction. Some of the broader controversies regarding emotion can be encapsulated in onion layers that are highly interconnected and mutually influenced. From the outset to the inset, controversies may concern:

-   how emotion relates to, and differ from other phenomenon of the mind/body such as intelligence, rationality, consciousness or various forms of cognition [@broschImpactEmotionPerception2013; @leventhalRelationshipEmotionCognition1987; @liebermanBooConsciousnessProblem2019; @rollsEmotionDecisionmakingExplained2014; @becharaRoleEmotionDecisionmaking2004; @pessoaCognitiveemotionalBrainInteractions2013; @murphyIntelligenceInterpersonalSensitivity2011; @saloveyEmotionalIntelligence1990];
-   how emotion relates to, and differ from other affective phenomena such as moods, preferences, attitudes, motivation, desires, feelings, sentiments, or passions [@linnenbrink-garciaAdaptiveMotivationEmotion2016; @schererWhatAreEmotions2005; @dixonPassionsEmotionsCreation2003; @schwarz2003mood; @colombettiFeelingBodyAffective2014];
-   whether there are, and what are the building blocks and features covering the whole process, from causation to consequences, shared by all phenomena identified as emotion [@adolphsNeuroscienceEmotionNew2018; @grossEmotionRegulationAffective2002; @moorsTheoriesEmotionCausation2009; @russellCoreAffectPsychological2003; @smithPatternsCognitiveAppraisal1985];
-   whether there are a countable number of, and how to distinguish between discrete emotions -- such as *anger*, *fear*, or *happiness* -- and if is it possible to categorize them in taxonomies [@fehrConceptEmotionViewed1984; @fontaineGlobalMeaningStructure2013; @russellCoreAffectPsychological2003; @sanderModelsEmotionAffective2013; @pekrunInternationalHandbookEmotions2014; @scarantinoDonGiveBasic2011; @ekmanArgumentBasicEmotions1992]
-   how a specific emotion, assuming that a discrete emotion exists, should be referred to -- both from an everyday communication and scientific conceptualization points of view -- considering differences between cultures and languages [@ekmanWhatMeantCalling2011; @fontaineComponentsEmotionalMeaning2013; @gilliozMappingEmotionTerms2016; @torrePuttingFeelingsWords2018; @grandjeanConsciousEmotionalExperience2008; @schererMeasuringMeaningEmotion2013; @douglas-cowieEmotionalSpeechNew2003].

The implementation of an EAT, especially one that has a multipurpose vocation, must therefore face the complexity of defining the *unit* that it aims to make people aware of [@boehnerHowEmotionMade2007]. In this regard, a first, broad decision that shall be considered can be related to what Scarantino and de Sousa [-@sep-emotion] identify as the two *desiderata* of emotion definition. The first consists in shaping the definition of emotion in order to achieve compatibility with ordinary linguistic usage, which corresponds to a descriptive definition. The second aims at finding a definition of emotion that is theoretically fruitful, that is, a prescriptive definition. The two desiderata are not mutually exclusive, since even a prescriptive definition should try to maintain as much compatibility with ordinary linguistic usage as possible [@schererWhatAreEmotions2005]. But, in the case of incompatibility with ordinary intuition, a prescriptive definition should favor theoretical generalization [@scarantinoHowDefineEmotions2012]. Roughly translated in more concrete terms, a definition of emotion should guide the implementation of an EAT in deciding whether it shall convey what users consider an emotion to be from a *folk* perspective, or rather what an emotion should be considered according to a *scientific* perspective [@scarantinoHowDefineEmotions2012; @fontaineGlobalMeaningStructure2013; @schererWhatAreEmotions2005].

From a pure usability standpoint, that is, the perspective of a learner that must face a tool she is not used to have in a learning environment, a *folk* perspective should not be discarded as simplistic. For instance, an EAT could be based on the most frequent affective states encountered by learners [@dmelloSelectiveMetaanalysisRelative2013; @molinariEmotionFeedbackComputermediated2013; @reisAffectiveStatesComputersupported2018, see also previous Chapter 2], without any relationship with a prescriptive definition, and regardless of whether these states are more or less prototypical of emotion [@fehrConceptEmotionViewed1984; @scarantinoDonGiveBasic2011]. Even more bluntly, an EAT could avoid the problem altogether by prompting users to share *what they think an emotion is*, letting them construe the meaning of how they feel without any guidance [@barrettKnowingWhatYou2001; @barrettSolvingEmotionParadox2006; @russellCoreAffectPsychological2003]. On the other hand, from a multipurpose point of view -- which is in and on itself a form of generalization -- the link with a *scientific*, theory-driven perspective presents several advantages.

First, by mapping the tool to a prescriptive definition, emotion not only emerges according to what it *is*, but also according to what it *does* and why. As seen in the previous chapter, there is a growing body of literature focusing on the functional role emotion plays both at the intra- and inter-personal levels [@adolphsInvestigatingEmotionsFunctional2018; @fischerSocialFunctionsEmotion2016; @keltnerSocialFunctionsEmotions1999; @levensonIntrapersonalFunctionsEmotion1999; @leventhalRelationshipEmotionCognition1987]. Thus, a theory-driven perspective may contribute to better assemble these functions and take full advantage of them in the implementation of the EAT [@broschAppraisingBrainNeuroCognitive2013; @pekrunProgressOpenProblems2005; @vankleefInterpersonalDynamicsEmotion2018].

Second, tailoring an EAT on emotion theory may contribute to guide learners in discovering and using it, since they can be both interested and reassured by identifying underlying mechanisms that are thought to be an integral part of the phenomenon [see for instance @tranEmotionsDecisionMakingProcesses2012 for an applied example]. The fact that a dedicated tool is provided in the environment for some *corollary* activities does not automatically mean that it will be adopted [@kreijnsIdentifyingPitfallsSocial2003].

Third, from a research perspective, linking the EAT to a theory facilitates the assessment and comparison of measures collected through or about it, and therefore foster common ground in the contributions that shall decide to adopt it [@izardManyMeaningsAspects2010; @schererWhatAreEmotions2005]. In the meantime, the EAT would also benefit from the upcoming theoretical and applied contributions in the literature that relate to the shared emotion theory, ensuring the tool can progress over time, or otherwise be dismissed if some competing theory provides a better alternative [@fergusonVastGraveyardUndead2012; @meehlAppraisingAmendingTheories1990].

These reasons advocate the adoption of a theory-driven approach to emotion definition. The next section will thus address the topic of emotion theories.

## Theories of Emotion

As one may expect from the complexity in defining an emotion outlined in the previous section, there are a multitude of emotion theories and taxonomies that try to categorize them [see for example @moorsTheoriesEmotionCausation2009; @sep-emotion; @sanderModelsEmotionAffective2013]. Adopting a perspective spanning philosophy and affective sciences, Scarantino and de Sousa [-@sep-emotion] identify three traditions in which most of emotion theories may belong: (1) the Feeling Tradition, which considers the conscious experience of an emotion as the fundamental characteristic; (2) the Evaluative Tradition, which focuses on how emotion consists in, or is crucially defined by the way eliciting events are evaluated; and (3) the Motivational Tradition, which identifies emotion with specific internal states driven by want or need to satisfy a goal.

In psychology more specifically, the last decades have been characterized mainly by three major families of emotion theories that account for the *whole* emotional phenomenon (*i.e.*, elicitation and response): (1) basic emotion theories [@ekmanArgumentBasicEmotions1992; @ekmanWhatMeantCalling2011; @huttoNewBetterBET2018; @keltnerEmotionalExpressionAdvances2019], stemming from the assumption that there exist a fixed number of discrete, *primary* emotions, each of them presenting a fixed pattern of elicitation and response  (2) constructivist/dimensional emotion theories [@russellCoreAffectPsychological2003; @russellEmotionCoreAffect2009; @stanleyTwodimensionalAffectiveSpace2009], driven from a dimensional approach to emotion, in which a persistent core affect is determined by a bi-polar combination of physiological arousal (*e.g.*, activating vs. deactivating) and psychological assessment of the perceived valence of events (*e.g.*. pleasant vs. unpleasant); and (3) appraisal emotion theories [@moorsAppraisalTheoriesEmotion2013; @schererDynamicArchitectureEmotion2009; @smithPatternsCognitiveAppraisal1985], resulting from a combination of dimensional and discrete approaches, in which discrete emotion are determined by the ongoing, cognitive evaluation of events on a number of (dimensional) criteria. These three families emerged from different historical backgrounds, and their foundation are often rooted in philosophical perspectives with secular traditions. In *psychological* era, we can trace back seminal works on the three families in the second half of the last century [@arnoldEmotionPersonalityPsychological1960; @lazarusPsychologicalStressCoping1966; @russellCircumplexModelAffect1980; @schererEmotionProcessFunction1982]. Since then, theories have influenced each other, converging on some points -- most notably the fact that emotion is not unitary but comprises different components -- and maintaining different positions on other points, such as eliciting and differentiation mechanisms in discrete emotions [@frijdaEmotionDefinitionsPsychological2009; @sanderModelsEmotionAffective2013]. New and often interdisciplinary theories that have emerged lately resume and extends some ideas of one or more of these families. It is the case, for instance,  of the *enacting* approach proposed by Colombetti [-@colombettiFeelingBodyAffective2014], which bestow to emotion, from a phenomenological perspective, a more *active* role compared to the *passive* response to events that is sometimes implied in emotion theories. Another example is represented by the *theory of affective pragmatics* proposed by Scarantino [-@scarantinoHowThingsEmotional2017], which pursue the idea of the existence of emotion as natural kinds, as in basic emotion theories. In other cases, such as in the *theory of constructed emotion* advocated by Feldman Barrett [@barrettHowEmotionsAre2018; @barrettSolvingEmotionParadox2006; @barrettTheoryConstructedEmotion2017], there is considerable discontinuity with previous work, challenging the principle according to which emotions are responses or exist as natural kinds. For instance, Feldman Barret [-@barrettHowEmotionsAre2018, p. vii-viii] argues that "[e]motion are real, but not in the objective sense that molecules or neurons are real. They are real in the same sense that money is real -- that is, hardly an illusion, but a product of human agreement". Some contributions have also attempted to adopt a theory-independent perspective and rather focus on empirical contributions by investigating the accuracy in making predictions about the emotion experienced by a person in a controlled environment [@schererEmotionProcessEvent2019].

A second important choice in the implementation of a theory-driven EAT is therefore to define which emotion theory or set of theories it refers to. At present, no emotion theory provides clear and undisputed supremacy in terms of explanatory power of what an emotion is [@sep-emotion]. As a consequence, the choice of the theory may be assessed rather in functional terms: which tradition/theory may provide the best framework to foster emotional awareness? The present contribution makes the opinionated assumption that the Evaluative Tradition, and more specifically appraisal theories of emotion, may best fit this purpose. It is posited by this choice that an EAT may convey instrumental information to learners, if that information is crucially determined by how learners construe emotion meaning from the situation at hand, an assumption also shared by other scholars investigating the relationship between emotion and learning [@lavoueEmotionAwarenessTools2020; @molinariEmotionFeedbackComputermediated2013; @pekrunControlValueTheoryAchievement2006; @shumanConceptsStructuresEmotions2014]. The next section offers an overview of this family of emotion theories.

## Appraisal Theories of Emotion

Appraisal theories of emotion originated in the 1960's with the work of Arnold [-@arnoldEmotionPersonalityPsychological1960] and Lazarus [-@lazarusPsychologicalStressCoping1966] and were then formalized in the 1980's by independent scholars reaching similar conclusions about the need to explain how different emotions may be elicited by the same event, for different persons and in different situations [@schererEmotionProcessFunction1982; @smithPatternsCognitiveAppraisal1985; @schorrAppraisalEvolutionIdea2001; @rosemanAppraisalTheoryOverview2001; @siemerSameSituationdifferentEmotions2007]. The core element shared by theories belonging to the appraisal family is resumed by the premise that emotions "are adaptive responses which reflect appraisals of features of the environment that are significant for the organism's well-being" [@moorsAppraisalTheoriesEmotion2013, p. 119], where well-being roughly encompasses the satisfaction or obstruction of someone's wants, needs or goals that are considered of major concern [@frijdaEmotions1986; @rosemanAppraisalTheoryOverview2001; @schererWhatAreEmotions2005]. This feature is nevertheless not unique to appraisal theories, since other theories reckon adaptive functions to emotion or motivational implications [@adolphsNeuroscienceEmotionNew2018; @ekmanArgumentBasicEmotions1992; @scarantinoHowThingsEmotional2017; @sep-emotion]. Appraisal theories are therefore more thoroughly characterized by the dynamic, recurrent and ongoing nature they confer to emotion in sustaining the adaptive response [@broschAppraisingBrainNeuroCognitive2013; @oatleyCognitiveApproachesEmotions2014; @rosemanAppraisalTheoryOverview2001; @schererDynamicArchitectureEmotion2009; @schorrAppraisalEvolutionIdea2001; @siemerSameSituationdifferentEmotions2007]. It is in fact common among appraisal theories to consider emotion as a process, rather than a state (*ibid.*). The bounded process unfolds over brief periods of time and can be modified in light of new information coming from any of the organismic subsystems or components, which include "an appraisal component with evaluations of the environment and the person-environment interaction; a motivational component with action tendencies or other forms of action readiness; a somatic component with peripheral physiological responses; a motor component with expressive and instrumental behavior; and a feeling component with subjective experience or feelings" [@moorsAppraisalTheoriesEmotion2013, pp. 119-120].

An example of a concrete theory may contribute to better frame the mechanism, which is here limited to the appraisal component. The example is taken from Pekrun's Control-Value theory of achievement emotion [@pekrunControlValueTheoryAchievement2006], already sketched in Chapter 1, which provides three key elements from an illustrative purpose. First, it directly relates to learning, which fits with the thesis's applied context. Second, it proposes a reduced number of appraisal criteria compared to other theories (see below). Third, the appraisals are not *valence* and *arousal*, which are ubiquitous dimensions in constructivist/dimensional theories [@russellCircumplexModelAffect1980; @russellEmotionCoreAffect2009; @stanleyTwodimensionalAffectiveSpace2009], whereas in appraisal theories may have important, but not mandatory status [@fontaineWorldEmotionsNot2007; @gilliozMappingEmotionTerms2016; @schererWhatAreEmotions2005]. In Pekrun's own words (*ibid*, p. 317):

> The control-value theory described here posits that two groups of appraisals are of specific relevance for achievement emotions: (1) *subjective control* over achievement activities and their outcomes (e.g., expectations that persistence at studying can be enacted, and that it will lead to success); and (2) the *subjective values* of these activities and outcomes (e.g., the perceived importance of success).

What emotion a learner will feel in an achievement context, such as studying for a test, will depend on the specific pattern emerging from the combination of appraising the subjective control and the subjective value. For instance, "[i]f an achievement activity (e.g., studying) and the material to which it relates (e.g., learning material) are positively valued, and if the activity is perceived as being sufficiently controllable by the self, *enjoyment* is assumed to be instigated" (*ibid.*, p. 323). Conversely, "[i]f the activity is perceived as being controllable, but is negatively valued (e.g., when effort required by the activity is experienced as aversive), *anger* is expected to be aroused. If the activity is not sufficiently controllable, *frustration* will be experienced" (*ibid*, p. 323, all italics in the text).

According to the dynamic, recursive and ongoing nature of emotion posited by appraisal theories, then, the evaluation of the activity may change instantly. For instance, the learning material may be perceived as too difficult at first, lowering learners' subjective control, but become clearer after a synthesis or a figure. Likewise, the subjective value may suddenly arise if, after reading a few pages useless to one's needs, the learner finds a passage she can directly apply to solve a problem that has been bothering her ever since. Conversely, the very same passage may be considered of scarce value from her colleague, who has already solved the problem beforehand.

The Control-Value theory of achievement emotions may therefore seem to be a valid candidate to guide the implementation of an appraisal-driven EAT, but it only partially comply with the multipurpose vocation. In fact, learning is characterized by different *kinds* of emotion, which comprise, but are not limited to, achievement emotions. As illustrated in Chapter 1, Pekrun and Linnerbrink-Garcia [-@pekrunInternationalHandbookEmotions2014] reckon that learning also triggers epistemic emotions, topic emotions, and social emotions. An overarching appraisal theory would therefore be more suitable to a multipurpose EAT. In this regard, appraisal theories, even if they share considerable overlapping, differ on several key points, which are enumerated in a state of the art proposed by @moorsAppraisalTheoriesEmotion2013. Most of the points are too detailed for the present purpose and the complete list would lack instrumental information to be fully explained. Only the points concerning appraisal criteria are considered to be of primary relevance and will thus be extrapolated by the aforementioned source.

As a reminder, appraisal is considered a process that scans events outside or inside the organism and evaluates them against a set of criteria related to the organism's major concern. One of the main source of divergence in appraisal theories refers, precisely, to the number of appraisal criteria the event is evaluated against, the order of evaluation, as well as the possible outcomes of every criterion. An appraisal criterion can have dimensional, ordinal or categorical values. For instance, the Control-Value theory described above [@pekrunControlValueTheoryAchievement2006] considers that the two appraisals control and value can assume ordinal values *none*, *low*, *medium* and *high*. Other appraisal theories postulate dimensional criteria, with potentially infinite values along a continuum, or categorical, with a predefined set of possible values. Pekrun could have hypothesized, for instance, that subjective control ranges from *entirely uncontrollable* to *entirely controllable*, or categorized the appraisal in a dichotomous *not controllable* and *controllable*. As stated by @moorsAppraisalTheoriesEmotion2013:

> The number and nature of the appraisal variables and/or values is closely related to the number and nature of the emotions that one can or wishes to explain. In general, more emotions require more appraisal variables and/or more appraisal values. Turning it around, more appraisal variables and/or more appraisal values allow more variety in emotions. Two appraisal variables with two values each can account for four emotions. Seven appraisal variables with an infinite number of values each can account for an infinite number of emotions. The number and nature of the emotions that one wishes to explain can be traced back to metatheoretical choices such as whether one strives for parsimony and/or a focus on natural language descriptors of emotions, on the one hand, or exhaustiveness and/or a focus on variety, on the other hand. --- @moorsAppraisalTheoriesEmotion2013, p. 121-122

It is posited here that a multipurpose EAT may benefit from the greatest possible variation. Standing at information provided by the cited article, the appraisal theory which provides the greatest variety is the Component Process Model theoretical framework [@schererAppraisalConsideredProcess2001; @schererDynamicArchitectureEmotion2009; @schererNatureDynamicsRelevance2013; @schererWhatAreEmotions2005], which will thus be adopted as primary reference and is illustrated in the next subsection.

## The Component Process Model Theoretical Framework

The Component Process Model (CPM) is a theoretical framework bridging appraisal and componential approaches to emotion, first proposed by Scherer [-@schererEmotionProcessFunction1982] in the 1980's and then refined over the years [-@schererNatureDynamicsRelevance2013; -@schererWhatAreEmotions2005; -@schererAppraisalConsideredProcess2001; -@schererDynamicArchitectureEmotion2009]. It has been applied to different contexts: the reader may find discussions of the model in relation with learning and education in @shumanConceptsStructuresEmotions2014, with affective computing in @schererBlueprintAffectiveComputing2010, with emotional meaning-making in @fontaineComponentsEmotionalMeaning2013, with emotional competence/intelligence in @schererComponentialEmotionTheory2007, and with neuroscience in @sanderAppraisalDrivenComponentialApproach2018. An overarching and accessible description of the model can be found in @schererWhatAreEmotions2005, whereas @schererAppraisalConsideredProcess2001 and @schererDynamicArchitectureEmotion2009 are more focused on the dynamic, recurrent and ongoing features of the model. Finally, recent developments are discussed in @schererNatureDynamicsRelevance2013. 

This section first proposes an overview of model and its components according to the objectives of the thesis by combining a computational [@schererComponentProcessModel2010], emotion meaning-making [@schererMeasuringMeaningEmotion2013; @schererSemanticStructureEmotion2018] and voluntary self-report [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005] perspectives.

### Overall Description of the Model

The CPM adopts a functional perspective according to which "[e]motions have developed in the course of evolution to replace rigid instincts or stimulus-response chains by a mechanism that allows flexible adaptation to environmental contingencies by decoupling stimulus and response, creating a latency time for response optimization" [@schererComponentProcessModel2010, p. 48]. Response optimization emerges as the result from the synchronization of five organismic subsystems performing specific functions. The state of each subsystem during an emotion episode corresponds to one of the five components of the model, namely: (1) the cognitive, (2) the neurophysiological, (3) the motivational, (4) the motor expression, and (5) the subjective feeling components. Table \@ref(tab:tf-cpm-components-table) lists the full association between functions, organismic subsystems, and emotion component in the model. According to the CPM, thus, an emotion is defined as "*an episode of interrelated, synchronized changes in the states of all or most of the five organismic subsystems in response to the evaluation of an external or internal stimulus event as relevant to major concerns of the organism*" [@schererWhatAreEmotions2005, p. 697, italic in the text]. 

(ref:tf-cpm-subcomponents-caption) Relationships between organismic subsystems and the functions and components of emotion. Reproduced from @schererWhatAreEmotions2005, original table 1, p. 698.

```{r tf-cpm-components-table, eval=FALSE}
tf.cpm_components <- tibble(
  `Emotion function` = c("Evaluation of objects and events", "System regulation", "Preparation and direction of action", "Communication of reaction and behavioral intention", "Monitoring of internal state and orgnism-environment interaction"),
  `Organismic subsystem and major substrata` = c("Information processing (CNS)", "Support (CNS, NES, ANS)", "Executive (CNS)", "Action (SNS)", "Monitor (CNS)"),
  `Emotion component` = c("Cognitive component (appraisal)", "Neurophysiological component (bodily symptoms)", "Motivational component (action tendencies)", "Motor expression component (facial and vocal expression)", "Subjective feeling component (emotional experience)")
)

tf.cpm_components %>% 
  kable(
    caption = "(ref:tf-cpm-subcomponents-caption)\\label{tab:tf-cpm-components-table}",
    caption.short = "CPM components and functions",
    longtable = TRUE,
    booktabs = TRUE,
    linesep = c("\\addlinespace", "\\addlinespace", "\\addlinespace", "\\addlinespace", "\\addlinespace", "\\addlinespace")
  ) %>% 
  kable_styling(full_width = TRUE) %>%
  column_spec(1, width = "6cm") %>% 
  column_spec(2, width = "4.5cm") %>% 
  footnote(
    general = "Abbreviations: CNS-Central Nervous System; NES-Neuroendocrine System; ANS-Autonomic Nervous System; SNS-Somatic Nervous System.", threeparttable = TRUE
  )
```

The CPM belongs to the appraisal family of emotion theories because it states that the coordination and synchronization of the components during an emotion episode is driven by the cognitive evaluation of the situation, that is, by how the situation is subjectively appraised by the person [@schererWhatAreEmotions2005]. The five components intertwine so that a variation in one fo them may lead to modifications in others, modifying thus the net effect of the coordination and synchronization. As stated by Scherer [-@schererMeasuringMeaningEmotion2013]:

>Briefly put, the CPM suggests that the event and its consequences are appraised with a set of criteria on multiple levels of processing, producing a motivational effect or action tendency that often changes or at least modifies the status quo. Specifically, the appraisal results and the concomitant motivational changes will produce efferent effects in the autonomic nervous system (in the form of somatovisceral changes), and motor expressions are centrally represented and constantly fused in a multimodal integration area (with continuous updating as events and appraisals change). Parts of this central integrated representation may then become conscious and subject to assignment to fuzzy emotion categories, as well as being labeled with emotion words, expressions, or metaphors --- @schererMeasuringMeaningEmotion2013, p. 13

The process can be understood in terms of input/output when the model is organized in three modules, as graphically represented in Figure \@ref(fig:tf-cpm-general-model-adapted-caption), adapted from @schererComponentProcessModel2010: (1) the appraisal module, (2) the response-patterning module, and (3) the integration/categorization module. From the left-hand side of the model, an event, behavior or situation (the input) is subject to a multilevel appraisal, that is, cognitively evaluated against a number of appraisal criteria. This evaluation leads to the patterning of a response, which comprises three out of the five organismic subsystems: (a) the motivational component, which prepares the organism to action, even if this action is not necessarily executed; (b) the neurophysiological component, which orchestrates the allocation of resources according to the action tendency; and (c) the motor expression component, which is responsible of the emergence of the underlying process to the *surface* via efferent manifestations, providing communicative cues, such as facial expressions, vocal prosody, or body postures. Finally, in the integration/categorization module on the right-hand side, the whole process -- that is, the net effect of the appraisal and response-patterning -- is integrated in a central representation, the feeling, of which the person may or may not be conscious. If she is conscious, the person may recur to her emotional lexicon in an effort to label the feeling, which in that case becomes the subjective feeling, that is, the conscious output of the whole process. 

(ref:tf-cpm-general-model-adapted-caption) The dynamic architecture of the component process model. Adapted, with minor labeling and graphical modifications, from the original figure 2.1.1 in @schererComponentProcessModel2010, p. 50.

```{r tf-cpm-general-model-adapted, fig.align="center", fig.cap="(ref:tf-cpm-general-model-adapted-caption)", out.width="100%"}
knitr::include_graphics(here("figure/theory/cpm-model-adapted.png"))
```

After a brief overview of the overall model, the next subsections focus on the three modules respectively, with particular attention to the appraisal and the integration/categorization modules, for they are more compatible with the overall assumption in the thesis of decoupling emotional awareness from the face-to-face golden standard.

### The Appraisal Module: Appraisals as Sequential Evaluation Checks

The appraisal module may be considered the most characteristic of the CPM for two intertwined reasons. First, it bridges cognition and emotion in a functional perspective [@leventhalRelationshipEmotionCognition1987], avoiding thus the pitfalls of the opposition between affective and cognitive, which recent developments in affective science are progressively dismissing in favor of interacting or even integrating perspectives [@dukesRiseAffectivism2021; @pessoaCognitiveemotionalBrainInteractions2013]. Second, it plays a pivotal role in differentiating the CPM from other appraisal theories (see above), for the CPM makes bold assumptions about how distinctive appraisal patterns lead to distinctive emotional episodes [@schererDynamicArchitectureEmotion2009; @schererHumanEmotionExperiences2013; @schererStudyingEmotionantecedentAppraisal1993; @schererEmotionProcessEvent2019]. In other words, the model explicitly assumes a causal mechanism instantiated by the appraisal module, which determines the unfolding emotional episode in terms of response-patterning and integration/categorization.

The CPM's appraisal module is built around the central notion of Sequential Evaluation Checks (SECs): a set of appraisal criteria the event, situation or behavior is evaluated against [@schererAppraisalConsideredProcess2001; @schererDynamicArchitectureEmotion2009; @schererNatureDynamicsRelevance2013]. According to the CPM, these appraisal criteria can not only be enumerated, but also determined with respect to their timing in achieving *preliminary closure* in information processing, that is, before the result of the evaluation can be integrated by other SECs or components of the model in the unfolding emotional process [see @schererNatureDynamicsRelevance2013 for a recent discussion]. The CPM identifies a total of 13 SECs, listed in Table \@ref(tab:tf-cpm-secs), which can be grouped in correspondence with four major appraisal objectives in adapting to a given event [@schererDynamicArchitectureEmotion2009; @schererNatureDynamicsRelevance2013]:

* *Relevance*: how relevant is this event for me?
* *Implications/Consequences*: what are the implications or consequences of this event and how do they affect my well-being and my immediate or long-term goals?
* *Coping potential*: how well can I cope with or adjust to these consequences?
* *Normative significance*: what is the significance of this event for my self-concept and for social norms and value?

(ref:tf-cpm-secs-caption) Stimulus evaluation checks, organized in four groups, illustrated with typical features describing the event or the effects on the person. Retrieved from original Table 1 in @schererNatureDynamicsRelevance2013, p. 151
```{r tf-cpm-secs}
tf.secs <- tibble(
  `Stimulus evaluation checks` = c("Novelty", "Intrinsic pleasantness", "Goal/need pertinence", "Causal attribution", "Outcome probability", "Discrepancy from expectation", "Goal/need conduciveness", "Urgency", "Control", "Power", "Adjustment", "Internal standards", "External standards"),
  `Event or behavior/person` = c(
    "Event is sudden, familiar, unpredictable",
    "Event is in itself un/pleasant for the person",
    "Event is important and relevant for person’s goals or needs",
    "Event was caused by the person’s own/somebody else’s behavior/chance; caused un/intentionally",
    "Consequences of the event are predictable",
    "Event confirmed/is inconsistent with expectations",
    "Consequences of the event are positive/negative for person",
    "Event required an immediate response",
    "Person can control the consequences of the event",
    "Person has power over the consequences of the event",
    "Person can live with the consequences of the event",
    "Event incongruent with own standards and self-ideals",
    "Event violated laws or socially accepted norms"
  )
)

tf.secs %>% 
  kable(
    caption = "(ref:tf-cpm-secs-caption)\\label{tab:tf-cpm-secs}",
    caption.short = "CPM list of SECs",
    longtable = FALSE,
    booktabs = TRUE,
  ) %>% 
  kable_styling(full_width = TRUE) %>%
  pack_rows("Relevance", 1, 3) %>% 
  pack_rows("Implications/consequences", 4, 8) %>% 
  pack_rows("Coping potential", 9, 11) %>% 
  pack_rows("Norm compatibility", 12, 13) %>% 
  column_spec(1, width = "5.5cm")
  
```

The evaluation of SECs can be executed at various levels of processing [@leventhalRelationshipEmotionCognition1987], identified by Scherer [-@schererNatureDynamicsRelevance2013] in four levels: 

>I have further developed the notion of levels of processing and now postulate four such levels (entailing different neural structures and circuits): (a) a low sensorimotor level with a pattern-matching mechanism that is largely genetically determined, using criteria consisting of appropriate templates; (b) a schematic level, based on memory traces from social learning processes and occurring in a fairly automatic, unconscious fashion; (c) an association level, involving various cortical association areas, which may occur automatically and unconsciously or in a deliberate, conscious fashion; and (d) a conceputal level, involving propositional knowledge and underlying cultural meaning systems, requiring consciousness and effortful calculations in prefrontal cortical areas. --- @schererNatureDynamicsRelevance2013, p.151

Furthermore, appraisal criteria are also bidirectionally linked with cognitive functions such as attention, memory, and reasoning, as well as motivational factors and self-concepts [@sanderSystemsApproachAppraisal2005], as illustrated in \@ref(fig:tf-cpm-full-process). As a consequence, the specific appraisal pattern that emerges is determined by situational factors, but also by more stable structures that can lead to certain patterns to emerge more or less frequently (*e.g.*, the tendency of a student to avoid public speaking as a result of past experiences). In the meantime, the fact that many elements concur in determining the specific appraisal profile implies that an evaluation may cause the emotional episode to derail from adaptive binaries and lead to inefficient or even harming responses to events, as in the case of excessive anxiety or fear in taking an exam [@pekrunControlValueTheoryAchievement2006; @shumanConceptsStructuresEmotions2014]. The CPM also implies that the emotion components have links with cognitive processes (*i.e.*, the red *backward* arrows in Figure \@ref(fig:tf-cpm-full-process), which contributes to explain why emotion influence these processes [*e.g.*, @broschImpactEmotionPerception2013].

(ref:tf-cpm-full-process-caption) Comprehensive illustration of the component process model of emotion. Retrieved from the original Figure 1 in @sanderSystemsApproachAppraisal2005, p. 321. Please note that the *conducivness* SEC does not appear in the more recent list provided by Scherer [-@schererNatureDynamicsRelevance2013] and illustrated above.

```{r tf-cpm-full-process, fig.align="center", fig.cap="(ref:tf-cpm-full-process-caption))", out.width="90%"}
knitr::include_graphics(here("figure/theory/cpm-full-process.png"))
```

To sum up, in the CPM appraisal criteria play a prominent role in emotion elicitation and differentiation. As a consequence, an EAT could leverage on the pivotal role of SECs to foster emotional awareness, for instance advocating the cognitive evaluation of the salient event. In other words, the EAT could contribute to make sure that emotion is assessed at the conceptual level -- the one implicating consciousness and effortful calculations -- as a means to extrapolate meaning-making from the adaptive objectives of appraisal criteria.

### The Response-Patterning Module

The response-patterning module build upon information provided by sequential evaluation checks described in previous subsection, since "the fundamental assumption of the CPM is that the appraisal results drive the response patterning in all other components by triggering efferent outputs designed to produce adaptive reactions that are in line with the current appraisal results (often mediated by motivational changes)" [@schererComponentProcessModel2010, p. 58]. Adaptive reactions can be designated at two levels: (1) intra-individual, with the regulation of the organism's functions (neurophysiological component) and preparedness to action (motivational component); and (2) inter-individual, through the motor expression component, with efferent manifestations such as facial expression, vocal prosody or body posture playing an important communicative role [see @hyniewskaNaturalisticEmotionDecoding2019; @schererFacialExpressionsAllow2007; @shumanEmotionPerceptionComponential2017 from a componential perspective]. 

From the point of view of an EAT, emotional awareness from the response-patterning would require dedicated hardware/software both at the intra-individual (*e.g.*, heart-rate detection, skin conductance, ...) and inter-individual level (audio/video connection, autonomic emotion recognition from facial expression, ...). As stated in the introduction, even though these techniques exist and are currently deployed in emotional awareness research in computer-mediated environments [*e.g.*, @ez-zaouiaEmodashDashboardSupporting2020], they are not considered for the purpose of the present contribution. This is an opinionated choice, dictated primarily by the desire to provide a parsimonious tool, requiring minimal hardware configuration to maximize portability and adaptability to different contexts.

### The Integration/Categorization Module: Sbjective Feeling As Emotion Meaning Making

The integration/categorization module may be considered the *output* of the CPM, because it coalesces information from all the other components into a conscious emotional experience. In fact, the CPM considers the subjective feeling "a holistic cognitive representation that integrates the temporarily coordinated changes of the other components [...], allowing the individual to reach awareness of his/her state and label it –- stating that he/she ‘has’ or ‘feels’ a particular emotion" [@schererGRIDMeetsWheel2013, p. 281]. Contrary to other interpretation of what an emotion is that requires consciousness for emotion to exist [@ledouxHigherorderTheoryEmotional2017; @liebermanBooConsciousnessProblem2019], according to the CPM the subjective feeling does not necessarily emerge to consciousness, nor it fully represents the emotional episode. Figure 

(ref:tf-cpm-subjective-feeling-caption) LABEL
```{r tf-cpm-subjective-feeling-caption, fig.align="center", fig.cap="(ref:tf-cpm-subjective-feeling-caption)", out.width="50%"}
knitr::include_graphics(here("figure/theory/cpm-subjective-feeling.jpg"))
```

* Explain the threeparti
* GRID

### Synthesis



>if one knows the results of an individual’s event appraisal on major checks, one can approximately predict what kind of emotion he or she will most likely experience (or more precisely, what label the person is likely to use to refer to the experience). -- @schererDynamicArchitectureEmotion2009, p. 1326

## Corolollaries to the Component Process Model

### Emotion as a Distinct Affective Phenomenon

### Emotional Competence from a Componential Perspective

### Emotion Meaning-Making: the GRID Instrument

### Emotion Self-Report: the Geneva Emotion Wheel

The complexity and diversity in the field of emotion theories is reflected in different approaches on how emotions should be assessed and measured [@boehnerHowEmotionMade2007; @cerneaSurveyTechnologiesRise2015; @maussMeasuresEmotionReview2009; @mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005]. In a componential approach to emotion, ideally, all aspects of the synchronized and unfolding process should be taken into account [@schererWhatAreEmotions2005]. Measuring all these aspects at once, though, requires dedicated material, some of which is difficult or impossible to deploy outside a laboratory. Self-report remains therefore a widely adopted method to ask a person about her emotional experience, either through *paper-and-pencil-like* questionnaire  [e.g. @harmon-jonesDiscreteEmotionsQuestionnaire2016; @pekrunMeasuringEmotionsEpistemic2016], or more elaborated self-report tools [see among others @broekensAffectButtonMethodReliable2013; @desmet2003measuring; @schererWhatAreEmotions2005;  @siegertAppropriateEmotionalLabelling2011; @bradleyMeasuringEmotionSelfAssessment1994; @caicedoHowYouFeel2006; @isomursuExperimentalEvaluationFive2007; @molinariEMORELOutilReporting2016].  With respect to more elaborate tools in particular, Scherer [-@schererWhatAreEmotions2005] advocates some of the characteristics an emotion self-report tool should provide.

>The design characteristics for the instrument to be developed are as follows:
>
>* concentrating on the feeling component of emotion, in the sense of qualia, rather than asking respondents to judge concrete response characteristics such as sympathetic arousal;
* going beyond a simple valence-arousal space in order to be better able to differentiate qualitatively different states that share the same region in this space;
* relying on standard emotion labels in natural languages in order to capitalize on respondents’ intuitive understanding of the semantic field;
* allowing systematic assessment of the intensity of the feeling;
* going beyond the arbitrariness of choosing different sets of emotion terms and presenting them in very unsystematic fashion by building some emotion structure into the instrument;
* presenting the instrument in a graphical form that is userfriendly, allowing the respondent to rapidly understand the principle and use the instrument in a reliable fashion. --- @schererWhatAreEmotions2005, p. 721

Driven by these tenets, Scherer [-@schererWhatAreEmotions2005] developed and refined over the years [@schererGRIDMeetsWheel2013] one of the most widely adopted emotion self-report tool: the Geneva Emotion Wheel (GEW). For instance, the GEW is also presented in an overarching chapter [@shumanConceptsStructuresEmotions2014] about emotion in the *International Handbook of Emotions in Education* [@pekrunInternationalHandbookEmotions2014], which is, to the best of my knowledge, the most up-to-date and comprehensive resource about emotion in education. The GEW is an instrumental implementation of the Component Process Model theoretical framework illustrated above. It combines the two most widely adopted approaches to emotion self-report -- the dimensional and discrete emotions approaches [@schererWhatAreEmotions2005] -- by establishing a causal link between the appraisal [@schererAppraisalConsideredProcess2001] and the subjective feeling [@grandjeanConsciousEmotionalExperience2008] components -- that is, between the cognitive evaluation of the situation and the holistic experience that usually emerges to consciousness as a natural language word or idiom. This direct link is also corroborated by recent empirical contributions investigating whether the emotional meaning of words often adopted, in different cultures, to express emotional experiences can be traced back to specific evaluations of the eliciting event [@fontaineGlobalMeaningStructure2013; @fontaineWhyWhatHow2013; @gentschEffectsAchievementContexts2017; @gilliozMappingEmotionTerms2016; @schererSemanticStructureEmotion2018]. 

(ref:gew-version-3-caption) The third version of the Geneva Emotion Wheel [@schererGRIDMeetsWheel2013]
```{r gew-version-3-figure, fig.cap="(ref:gew-version-3-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/dew/gew-v3.png"))
```

The interface of the GEW, depicted in its third version in Figure \@ref(fig:gew-version-3-figure) [@schererGRIDMeetsWheel2013], presents 20 subjective feelings -- chosen with respect to their frequency in emotion literature and everyday language use -- organized in a circumplex, which is determined by two appraisal dimensions as axes: *Valence* on the abscissa and *Control/Power* on the ordinate. Each feeling is positioned according to a value along both axes and belongs thus to one of the four quadrants of the circumplex, determined by the combination of positive or negative Valence, and positive or negative Control/Power. From the origin of the axes stem 5 circles for each of the subjective feelings. The circles grow in size as they get closer to the edge of the circumplex, so that the first circle is the smallest, and the last one the biggest. Users can therefore choose, first, the *row* of circles that corresponds to the subjective feeling they are experiencing and, second, the intensity of the feeling as a function of the circle's size.

## A Working Definition of Emotion

*x* is an emotion only *if*

1. *x* is an affective *episode*
2. x has the property of intentionality (i.e., of being directed)
3. x contains bodily changes (arousal, expression, etc.) that are felt
4. x contains a perceptual or intellectual episode, y, which has the property of intentionality 
5. the intentionality of x is inherited from the intentionality of y
6. x is triggered by at least one appraisal
7. x is guided by at least one appraisal

<!--chapter:end:23-defining-emotion-unit.Rmd-->

# The Dynamic Emotion Wheel

```{r dew-chapter-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

The thesis builds on and extends previous work on Emotion Awareness Tools carried out in the Emotion Awareness Tool for Computer-Mediated Interactions (EATMINT) project [@avryAchievementAppraisalsEmotions2020; @avrySharingEmotionsContributes2020; @cereghettiSharingEmotionsComputermediated2015; @Chanel2013; @chanelGrandChallengeProblem2016; @fritzDynamicEmotionWheel2015; @fritzReinventingWheelEmotional2015; @molinariEmotionFeedbackComputermediated2013]. The project, which was part of the National Center of Competence in Research (NCCR) *Affective Sciences*, had two intertwined objectives:

1. Designing Emotion Awareness Tools, which allow either the explicit or the autonomic sharing of 
emotions in computer-mediated interaction; 
2. Studying the impact of emotional awareness on collaboration both at intra- and inter-personal 
levels, with respect to the emergence of strategies that improve collaboration such as monitoring, 
regulating and reflecting on emotions.

**Voluntary Self-Report**. Considerable attention in the last decades has been directed towards automatic recognition of emotion through individual or combined sources such as facial expressions, body posture, vocal prosody, or text analysis [@jordivallverduHandbookResearchSynthesizing2015; @schererBlueprintAffectiveComputing2010; @calvoAffectDetectionInterdisciplinary2010]. Some of these techniques have been applied in computer-mediated learning environments, notably in relation to virtual agents or intelligent tutors, which adapt or react based on learners' detected affective states [@calvoFrontiersAffectAwareLearning2012; @cerneaGroupAffectiveTone2014; @robisonEvaluatingConsequencesAffective2009]. 

Notwithstanding the interest to detect affective information without learners' explicit action, ...

**Implementing an Emotion Structure**. Despite a widespread agreement about the pivotal role of *affectivism* [@dukesRiseAffectivism2021], fundamental issues across affective phenomena (*e.g.* are emotions distinct from moods?) and within the same phenomenon (*e.g.*, what is an emotion?) are still unresolved [@izardManyMeaningsAspects2010; @sanderModelsEmotionAffective2013]. As a consequence, when endowing learners with emotional awareness, one has several options as to what constitute and is representative of *emotion*. For reasons that will be developed later in the text, this contribution adopts an appraisal-driven approach to emotion, according to which emotions are determined by an ongoing cognitive evaluation of events against a series of criteria [@moorsAppraisalTheoriesEmotion2013; @rosemanAppraisalTheoryOverview2001; @schererWhatAreEmotions2005].   

**Moment-to-Moment Availability of Emotional Awareness**.

<!--chapter:end:24-dew-chapter.Rmd-->

# (PART\*) Proof of Concept {.unnumbered}

<!--chapter:end:30-method.Rmd-->

# Foreword to the Methodological Part of the Thesis {.unnumbered}

In Part I of the thesis, the network of theoretical foundations in which the concept of an Emotion Awareness Tool (EAT) is intertwined has been set forth with respect both to the overall *affective milieu* in which the use of this type of tool is currently investigated, and the challenges in the conceptualization of each of the terms composing the acronym. *Emotion* has been illustrated through an overarching framework, the Component Process Model, from which a causal mechanisms as been retained: appraisals criteria as determinants of elicitation and differentiation in the emotional experience in the form of a holistic subjective feeling. *Awareness* has been substantiated in a double perspective: self-awareness and group-awareness, depending on the target of the instrumental information provided by emotional awareness. Finally, the technical part, the *Tool*, has been circumscribed to a moment-to-moment and self-report perspective. In a nutshell, these are the main premises to two intertwined methodological objectives: 

1. Endowing computer-mediated learning environments with an EAT that implements an emotional structure as an abstract and parsimonious computational model that links appraisals and subjective feeling as a means to foster emotional self-awareness and/or emotional group-awareness; 
2. Leveraging on this mechanism to provide researchers and instructional designers with a multi-purpose, moment-to-moment, and self-report EAT, which may be adapted to their needs in an open-ended perspective, fostering comparison, sharing, and cumulative knowledge on the subject of emotional awareness in computer-mediated learning environments. 

The next two chapters of Part II will detail each objective more specifically. Before that, though, it is important for the reader to be informed that, even though the methodological part is illustrated before empirical contributions, the overall process has not been straightforward, but rather iterative, following human-computer interaction principles [@mackenzieHumancomputerInteractionEmpirical2013; @rogersInterctionDesignHumanComputer2011]. The technical and methodological implementation of the EAT has both informed, and been informed by, the empirical contributions of Part III. This means that Part II presents the result of the last iterative process, the one that implements data and experience from the empirical contributions. At the same time, some of the technical and methodological elements resulting from the last iteration were not available for the empirical contributions. As a consequences, empirical contributions do not necessarily take full advantage or explore thoroughly all the available features. 

The iterative process also determines the use of the term *proof-of-concept*, since the overall process is to be intended as an attempt to assess to what extent an EAT may emerge with the desired characteristics and fulfill the desired objectives. At the same time, the *proof-of-concept* perspective highlights the fact that the result is mainly focused on conceptual features, whereas technical and programming issues (*e.g.*, debugging, error-handling, ...) have been considered insofar as they were determinant in the development of a minimum viable product. 

Finally, to keep this part of the contribution to a manageable length and accessible to non-technical readers, programming information will not be illustrated thoroughly. Interested readers in the technical aspects are encouraged to explore the code and documentation available in the following public repositories:

* ...
* ...

<!--chapter:end:31-methodological-objectives.Rmd-->

# A Parsimonious Computational Model Linking Appraisal and Subjective Feeling

```{r computational-model-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

# Load the relevant data and graphs
source(here("data", "dew-rt", "dew-rt-export.R"))

```

This chapter illustrates a parsimonious computational model inspired by the Component Process Model (CPM) theoretical framework that links appraisals criteria and subjective feeling. As a reminder, appraisals criteria in appraisal theories of emotions -- and the CPM more specifically -- are cognitive evaluation of an event, which determine emotion elicitation and differentiation. The subjective feeling, on the other hand, is the result of a highly dynamic and cumulative process, instantiated by appraisal, that coalesces in an overall emotional experience, from which the person may extrapolate meaning by recurring to natural language words or idioms that best convey its ineffable nature. In previous works, I have set forth the foundations of a prototype of an EAT, named *Dynamic Emotion Wheel* for its relationship with the Geneva Emotion Wheel illustrated in Part I of the thesis, which already explored the possibility to use active ratings of appraisal criteria to determine a set of subjective feelings most likely to occur, given the specific evaluation of the appraisal criteria. Previous work was nevertheless mainly driven by usability concerns emerged in empirical contributions using a provisional EAT [@molinariEmotionFeedbackComputermediated2013]. As it is often the case with usability issues, though, they were only the tip of the iceberg of more fundamental issues to be addressed, which initialized the quest for a better concept of an EAT. This chapter therefore builds on previous work in an attempt to bring an EAT even closer to appraisal theories of emotion, and the CPM more specifically.

The chapter starts by briefly describing previous work on the Dynamic Emotion Wheel. Then, it extends on it by implementing a *k-nearest neighbor* approach to the relationship between appraisals and subjective feeling. Harnessing the abstract concept of an *N-dimensional* affective space, a parsimonious computational model can link one or more appraisal criteria with a set of subjective feelings that ar most likely to correspond with the specific appraisal profile. While presenting the abstract model, existing affective spaces will be used to illustrate the concept in more concrete terms. At the end of the chapter, the core principle of an appraisal-driven EAT will emerge, paving the way to the possibility of configuring it according to theoretical or practical needs.

## The Dynamic Emotion Wheel

During the Master of Science in Learning and Teaching Technologies at Geneva University between 2013 and 2015, I had the chance to collaborate to the *Emotion Awareness Tool for Computer-Mediated Interactions* (EATMINT) project, which at the time had just concluded empirical investigation on the use of Emotional Awareness Tools in computer-mediated collaboration -- which then led to several publications [*e.g.*, @cereghettiSharingEmotionsComputermediated2015; @Chanel2013; @molinariEmotionFeedbackComputermediated2013]. The project had two intertwined main objectives:

1. Designing Emotion Awareness Tools, which allow either the explicit or the autonomic sharing of
emotions among participants in a computer-mediated task;
2. Studying the impact of emotional awareness on collaboration both at intra- and inter-personal
levels, with respect to the emergence of strategies that improve collaboration such as monitoring,
regulating and reflecting on emotions.

Description of the DEW.


## A Parsimonious Computational Model Based on a *K-Nearest Neighbor* Approach

The bi-dimensional mechanism of the DEW can be abstracted according to the working definition of emotion provided by @mulliganWorkingDefinitionEmotion2012, specifically the points about appraisals: (a) an emotion *is triggered by at least one appraisal*, and (2) an emotion *is guided by at least one appraisal*. To ease the illustration of the parsimonious computational model, the section adopts the following notation:

* $E$ stands for *Appraisal **E**valuation*, that is, the hypothetical value that a cognitive evaluation can have on a given set of appraisal criteria $E_1$, $E_2$, ..., $E_n$. In concrete terms, $E$ represents the rating of a person asked to evaluate one or more appraisal dimensions. For instance, if there was only one appraisal, say, *Novelty*, than $E$ assumes a value determined by the extent to which the person evaluates the event as sudden, familiar or unpredictable on $E_1$.
* $P$ stands for *Subjective Feeling **P**redicted Appraisal*, that is, the predicted appraisal profile associated with a given subjective feeling, with subscripts $S_1$, $S_2$, ..., $S_n$ representing a set of $n$ predicted appraisal criteria In concrete terms, $P$ represents the appraisal profile that is supposed to elicit the given subjective feeling. For instance, the CPM predicts that a subjective feeling represented by the word *Happiness* may be related to a higher evaluation on appraisal *Novelty* compared to a subjective feeling represented by the word *Sadness* [@schererComponentProcessModel2010].
* As a consequence, $d(E, P)$ represents the distance between the Appraisal Evaluation and the Subjective Feeling Predicted Appraisal, with the *k-nearest neighbors* being the *k* subjective feelings with the lowest $d(E, P)$. In other words, if $k=5$, then, among all the subjective feelings in the affective space, the 5 whose appraisal profile $P$ is closer to the appraisal evaluation $E$ represent the subset of most likely subjective feelings to occur, given the *mysterious* point $E$, which is provided by the person cognitive evaluation. 

The following subsections illustrates how the distance $d(E, P)$ is calculated by the model depending on the number of appraisal criteria/dimensions composing the underlying affective space: one-dimensional, two-dimensional, or multi-dimensional. The presence of a specific section on a two-dimensional affective space, which is in fact the first multi-dimensional condition, is due to the possibility of interpreting the affective space either as a circumplex or as a Cartesian plane, which require a slightly different computation.

### One-Dimensional Affective Space

In a one-dimensional affective space, that is, where there is only a single appraisal criterion, the nearest neighbors are determined by the absolute distance between $E$ and $P$:

$$
d(E, P) = |E_1 - S_1|
$$

One-dimensional affective space are not usually defined as such in the literature, even though the possibility to conceptualize emotion on a single dimension is not infrequent, especially when this dimension represents an omnibus *valence* that often divides emotion in positive and negative. The possibility to use a one-dimensional affective space is therefore warranted by the working definition itself (*i.e.*, *at least one appraisal*), as well as by researchers and practitioners habit to recur to a dichotomous affective space, even though the actual rating may be implicit rather than explicit, for instance in grouping forced choices [@molinariEmotionFeedbackComputermediated2013].

The possibility to restrain the affective space to a single dimension may also be useful in situations where researchers or practitioners are interested specifically in the assessment of only one criterion, avoiding thus the mediation or moderation effects of additional criterion. For instance, if one is interested in the *normative significance*, more specifically with respect to *external standards* -- whether the event violated laws or socially accepted norms [@schererWhatAreEmotions2005] -- the presence of another appraisal criteria such as *valence*, may influence that evaluation. For example, negatively valenced emotion may be considered less socially acceptable [@vankleefInterpersonalDynamicsEmotion2018]. The presence of an explicit reminder of the valence could therefore mediate or moderate the rating of the external standard of normative significance.

### Two-Dimensional Affective Space

Two-dimensional, or bi-dimensional, affective spaces are ubiquitous in research, especially in the form of the dimensional *valence x arousal* circumplex [@russellCircumplexModelAffect1980], but also in appraisal-driven perspectives, as in Pekrun's control-value theory of achievement emotions [@pekrunControlValueTheoryAchievement2006; @pekrunControlValueTheoryAchievement2014] or the *valence x control/power* structure of the Geneva Emotion Wheel [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005]. Several scholars have theoretically adopted two-dimensional affective spaces in their research, but also as graphical representations for self-report/awareness tools [*e.g.*, @feidakisEndowingElearningSystems2011]. Consequently, it is important that an overarching computational model can manage to incorporate different kinds of two-dimensional affective spaces. In this regard, an affective space based on two dimensions can be interpreted as the sum of a pair of one-dimensional continuous spaces, in which case each subjective feeling is determined by a precise coordinate in a Cartesian plane. Alternatively, it may also be interpreted, less stringently, as a circumplex where each subjective feeling is positioned at an equal distance from the origin. The two interpretation of a two-dimensional affective space requires different computational models from a *k-nearest neighbors* perspectives. This subsection first illustrates the computational model for a Cartesian plane, and then for a circumplex. Last, it compares the two computational models in a simulation using the same affective space depicted either as a Cartesian plane or a circumplex.

#### Cartesian Plane Affective Space

When a second appraisal criteria is added, the resulting affective space can be conceptualized as a Cartesian plane, in which $P$ is represented by the coordinates $x = P_1$, and $y = P_2$. At the same time, the appraisal evaluation is also determined by coordinates $x = E_1$ and $y = E_2$. Consequently, the *k-nearest neighbors* may be determined using the Euclidean (or Pythagorean) distance, with the nearest subjective feelings having the smallest hypotenuse of a triangle with the length of the sides determined by the difference between $E_1$ and $P_1$, and $E_2$ and $P_2$:

$$
d(E, P) = \sqrt{(E_1 - P_1)^2 + (E_2 - P_2)^2}
$$

#### Circumplex Affective Space

An alternative for two-dimensional affective spaces consists in the use of a circumplex, a representation that is mainly linked with the concept of core-affect emerging from a *valence x arousal* space in constructivist/dimensional emotion theories [@russellCoreAffectPsychological2003; @russellEmotionCoreAffect2009; @stanleyTwodimensionalAffectiveSpace2009]. The structure itself, though, may also be adopted in other contexts, since a circumplex allows researchers to dispose multiple variables on a circle, so that the similarity between two items is inversely proportional to their distance on the circumference: the less the distance, the more the two elements are similar or correlated. The circumplex is determined by an x-axis and a y-axis, usually delimited by opposite poles of the same constructs (*e.g.*, pleasant/unpleasant) or two constructs in antithesis (*e.g.* sadness/enthusiasm). The combination of the two axes creates four quadrants, usually crossing over the negative and positive poles of the two axes. The top-right quadrant is x-positive and y-positive; the bottom-right quadrant is x-positive and y-negative; the bottom-left quadrant is x-negative and y-negative; and the top-left quadrant is x-negative and y-positive. 

Given its practical structure, a circumplex ca be used to dispose subjective feelings determined by two appraisals criteria in a more *relaxed* way compared to a Cartesian plane. In fact, researchers can tentatively dispose items at an equal distance from each other, assuming the order of disposition on the circle as a sufficient connotation of the affective space. Since the computational model, though, does not use the circumplex as a graphical representation, but rather as a *database* to retrieve suggestions, the fact that subjective feelings are not equally spaced on the circumference is irrelevant from a usability standpoint. As a consequence, it is also possible to adopt circumplexes with overlapping subjective feelings, or where the distance between any two items on the circle is not necessarily the same, but rather a function of some theoretical or practical mechanism.

On the other hand, the use of a circumplex as the underlying affective space requires a slightly different approach in computing the distance between $E$ and $P$, because $P$ in a circumplex is best conveyed as an angle, so that $P_{angle} \in \mathbb{R} : 0 \le P_{angle} \le 360$. A possible solution is to use $E_1$ and $E_2$ as points in a Cartesian plane, and compute the slope from the origin [0,0] of the plane. The slope can then be used to retrieve the angle from the x-axis, using the inverse tangent, or arctangent, trigonometric function, and -- with some correction skipped here, see Fritz (2015) or the code repository for more details -- also obtain an $E_{angle} \in \mathbb{R} : 0 \le E_{angle} \le 360$:

$$
E_{angle} = arctan(\frac{E_1 - 0}{E_2 - 0}) + correction = arctan(\frac{E_1}{E_2}) + correction
$$

Once retrieved the $E_{angle}$, the *k-nearest neighbors* can be determined using the same logic as in a uni-dimensional condition, by computing the absolute distance between $E_{angle}$ and $P_{angle}$:

$$
d(E, P) = |E_{angle} - P_{angle}|
$$
Conceptually, with this solution, it is worth considering the case when both $E_1$ and $E_2$ are at the neutral point 0, since $d(E, P)$ is exactly the same for every subjective feeling, since any point on a circle is at the same distance from the origin. In other words, any subjective feeling is as likely to represent the neutral point as any other. When both $E_1$ and $E_2$ are 0, thus, the computational model attributes a virtual random value both to $E_1$ and $E_2$ only with the purpose to shuffle the set of subjective feelings and avoid thus that some subjective feelings are arbitrarily suggested more frequently with the neutral point.

#### Comparison

Taking advantage of the fact that the disposition of the 20 subjective feelings in the third version of the Geneva Emotion Wheel [@schererGRIDMeetsWheel2013] has been determined using the GRID instrument, the same affective space is available as a circumplex and as Cartesian plane. As a consequence, it is possible to simulate the behavior of the computational model. For the purpose of the simulation, the following parameters will be adopted:

* The two appraisal dimensions (Valence and Control/Power) are implemented on a continuum ranging from -100 to 100, divied in steps of 1 unit;
* The *k*-nearest neighbors is set to $k = 3$ and to $k = 10$ to assess how the radial and the vector algorithms behave when the number of suggested feelings varies;
* The subjective feelings in the circumplex affective space are disposed so that each is equally distant from the previous and the following item, with a rotation of half the distance between each subjective feeling to respect the graphical representation of the circumplex, see the righ side of Figure \@ref(fig:pof-gew-radial-vs-vector);
* The subjective feelings in the Cartesian plan are disposed according to the values retrieved from data retrieved in @schererGRIDMeetsWheel2013, see the left side of Figure \@ref(fig:pof-gew-radial-vs-vector). The original value on a range from -1 to 1 has been multiplied by 100 to map the range of the simulation.

(ref:pof-gew-radial-vs-vector-caption) Disposition of the Geneva Emotion Wheel's 20 subjective feeling on a circumplex (left) and Cartesian plane (right) affective space. Reconstructed from @schererGRIDMeetsWheel2013.
```{r pof-gew-radial-vs-vector, fig.align="center", fig.cap="(ref:pof-gew-radial-vs-vector-caption)", fig.align="center", out.width="100%"}
knitr::include_graphics(here("figure/dew-rt/gew-radial-vs-vector.png"))
```

The simulation is computed by looping the $E_1$ and $E_2$ ranges so that each possible combination between the two -- that is, [-100, -100], [-100, -99], ..., [0, 0], ..., [100, 100] -- generates a subset of $k = 3$ or $k = 10$ subset of subjective feelings. Figure \@ref(fig:pf-simulation-3) and  \@ref(fig:pf-simulation-3) show the results of the simulation. With a $k = 3$ subset, the radial distance of the circumplex shows a more homogeneous number of appearances of each subjective feeling in the subset compared to the vector distance of the Cartesian plane. The opposite happens with $k = 10$, where the vector distance results in more homogeneous number of appearances compared to the radial distance.

(ref:pof-simulation-3-caption) Results of the simulation comparing distance computed as a radial or vector distance with a subset of $k = 3$ subjective feelings.
```{r pf-simulation-3, fig.align="center", fig.cap="(ref:pof-simulation-3-caption)", fig.align="center", out.width="70%"}
dew_rt.sim_gew_graph_3
```

(ref:pof-simulation-10-caption) Results of the simulation comparing distance computed as a radial or vector distance with a subset of $k = 10$ subjective feelings.
```{r pof-simulation-10, fig.align="center", fig.cap="(ref:pof-simulation-10-caption)", fig.align="center", out.width="70%"}
dew_rt.sim_gew_graph_10
```

This simulation has only illustrative purpose, because the two affective spaces are only examples of an infinite number of spaces that can be conceived and therefore results may be very different with other configuration. Nevertheless, this kind of simulation may be interesting to carry out before adopting an affective space (see next chapter) to assess whether there are subjective feelings that are more likely to be suggested by the system, because that could influence learners in choosing them.

### Multi-Dimensional Affective Space

There is growing empirical evidence in suggesting that one- and two-dimensional affective spaces are not enough to account for the variety of emotional experiences in semantic terms [@fontaineWorldEmotionsNot2007; @gilliozMappingEmotionTerms2016]. As a consequence, an overarching computational model should be able to integrate higher-order dimensional spaces, where the number of dimensions can be determined by researchers and practitioners with respect to theoretical, practical, or usability criteria (*e.g.*, if the moment-to-moment temporal dimension is particularly important, asking learners to repeatedly rate a high number of criteria would be very time consuming).

Extending the model to more than two dimensions is relatively straightforward when the Euclidean distance is maintained as the algorithm to determine the *k-nearest neighbors*. In fact, the it is possible to determine the distance between any two points in a multidimensional space, simply by adding additional dimensions to the Pythagorean formula:

$$
d(E, P) = \sqrt{(E_1- P_1)^2 + (E_2 - P_2)^2+\cdots+(E_n - P_n)^2}
$$

The model therefore remains fairly limited in term of computational power required, even if the number of dimensions increases. The difficulty with a higher-order affective spaces is in fact more theoretical than practical, since it is more difficult to imagine a space with three or more dimensions. Beyond three dimensions, it is even difficult to represent the space graphically, which contrasts with the easy-to-understand, and easy-to-implement disposition of subjective feelings in a circumplex. At the same time, the *k-nearest neighbor* approach can precisely contribute to overcome this issue, once a multi-dimensional affective space is established, since the dimensions are *flattened* on uni-dimensional ratings. The computational model is then in charge of mapping uni-dimensional ratings into multi-dimensional distances, as vectors between points in spaces the human mind can hardly imagine from a spatial point of view. 

For instance, Gillioz and colleagues [-@gilliozMappingEmotionTerms2016] adopted the GRID instrument -- in its shorter CoreGRID version [@schererCoreGRIDMiniGRIDDevelopment2013] -- to assess the mapping of 80 emotion terms in an affective space. The authors found evidence, corroborating previous findings on a smaller set of emotion terms [@fontaineComponentsEmotionalMeaning2013; @fontaineWorldEmotionsNot2007], attesting the presence of a four-dimensional affective space, composed by *valence*, *power*, *arousal*, and *novelty*. The number of dimensions and of emotion terms composing the affective space empirically established by the authors is a representative example of how a parsimonious computational model could be deployed in a self-report instrument, allowing users to benefit of all the dimensions and all the suggested emotion terms.

## Synthesis

This chapter illustrated a parsimonious computational model that allows to regroup affective spaces expressed on multiple dimension into the same abstract functioning, harnessing a *k*-neares neihbors approach.


<!--chapter:end:32-appraisal-to-feeling.Rmd-->

# Building a Toolbox Around the Parsimonious Computational Model

```{r eat-dashboard-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

```

In the previous chapter illustrated the parsimonious computational model linking appraisal dimensions and subjective feelings as a representation of an emotion structure that can be *injected* into a graphical user interface to self-report emotions. The model represents the *core* of the proof of concept proposed by the thesis, but is clearly not sufficient to provide emotional awareness in computer-mediated learning environments. In this regard, this chapter introduces a toolbox built around the computational model that allows the configuration of the expressing-displaying as well as the perceiving-monitoring functions of an Emotion Awareness Tool (EAT) according to researchers and practitioners needs. The implementation of a toolbox is driven by open science tenets, which advocate the need for investing in the development of open material that can foster transparency, sharing, replicability and reproducibility of research practices and results. In this intent, the toolbox pays particular attention in incorporating these tenets. 


<!--chapter:end:33-configuration-open-science.Rmd-->

# (PART\*) Empirical Contribution {.unnumbered}




<!--chapter:end:40-empirical.Rmd-->

# Foreward to the Empirical Part of the Thesis {.unnumbered}

Part II of the thesis outlined the features of an multipurpose EAT with specific characteristics, which aim at providing learners with a tool that implements an emotion structure in terms of the cognitive evaluation of the situation, from which learners may benefit both in terms of self- and group-awareness. In the meantime, the proposed EAT also aim at providing a viable research tool, that scholars may implement in their research. The empirical part of the thesis therefore attempts to combine the two levels of analysis by (1) investigating the EAT in computer-mediated learning environments with respect to concrete research questions derived from the extant literature in the field; and (2) providing an overall assessment of the EAT with respect to its most characteristics features (appraisal-driven, moment-to-moment, and based on self-report). 

As an attempt to combine the two objectives, Part III of the thesis is organized in three chapters. The next chapter illustrates the first empirical contribution, in which the EAT is implemented in a synchronous and collaborative computer-mediated learning environments, therefore within coordinates that are closer to the use of group-awareness tools in Computer-Supported Collaborative Learning. The following chapter, on the other hand, implements the EAT in an asynchronous and individual computer-mediated learning environment, in conditions that are closer to *mainstream* e-learning settings. The last chapter of Part III takes advantage from the use of the *same* EAT -- even though in different configurations that will be illustrated in the material sections of each chapter -- to propose an overall and comparative assessment of the tool. Alongside specific research questions, thus, there is an overall objective: 

>investigate some factors -- intrinsic to the tool, emerging from the interaction between the learners and the tool, or between the learners themselves -- which may determine the adoption, use and perception of the EAT in computer-mediated learning environments.

It is worth mentioning in this foreword that the empirical contributions of the thesis had to be adapted due to the COVID19 pandemic, which hindered and delayed research at various levels. In particular, a third experiment was planned but could not be carried out. It aimed at determining whether differences in the emotional information available through the EAT modified the perception of the colleague in a computer-mediated collaborative environment above and beyond the available information in the content space of the collaboration. It was meant as a follow-up of the empirical contribution of the next chapter, and was planned with $N = 160$ participants as a result of a power analysis to maximize its informative potential. The experiment will eventually be performed outside the scope of the thesis. Unfortunately, though, the two empirical contributions presented hereafter have less informative potential, and the overall empirical scope of the thesis is therefore limited by this shortcoming. 

<!--chapter:end:41-research-questions.Rmd-->

# Emotion Awareness in Synchronous and Collaborative Settings

```{r s1-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

# Load the relevant data and graphs from the study-1 folder
source(here("data", "study-1", "s1-export.R"))
```

General introduction to the study:

* Collaborative learning advantages and pitfalls
* Mutual-modeling in CSCL
* Description of empirical studies on emotional awareness in CMC/CSCL (Eligio, EATMINT)
* Follow-up to EATMINT "*Finally, "one main limitation of the study is the difficulty in disentangling the effect of reflecting upon one’s own emotions from the effect of sharing one’s emotions with the partner" [@molinariEmotionFeedbackComputermediated2013, p. 342]

## Study Overview

The moment-to-moment use of a voluntary self-report EAT implies that both the expressing-displaying and the perceiving-monitoring functions of an awareness tool [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002] require an effort that is not directly implemented in the content space, but is limited to the relational space of the task [@janssenCoordinatedComputerSupportedCollaborative2013; @dillenbourgSymmetryPartnerModelling2016]. As pointed out in the general theoretical framework of the thesis, expressing and perceiving emotions require cognitive effort, especially when (1) emotions are expressed deliberately and at a conceptual level, as stated by the Component Process Model theoretical framework [@sanderAppraisalDrivenComponentialApproach2018; @schererDynamicArchitectureEmotion2009], and (2) perceived emotions are linked to inferential processes about causes and consequences of behavior, as stated by the Emotion As Social Information (EASI) model [@vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018]. One of the pivotal aspects of voluntarily, self-reported emotional awareness is thus whether learners are keen to make this effort and under which conditions they are more prone to dedicate some of their cognitive resources for an activity outside the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013; @pashlerDualtaskInterferenceSimple1994].

More broadly, this phenomenon relates to one of the fundamental questions in theory of emotion: *why do people express and share emotions* [e.g., @darwinExpressionEmotionsMan1872; @fischerSocialFunctionsEmotion2016; @vankleefInterpersonalDynamicsEmotion2018]? Two main positions on the matter are relevant to the context of emotional awareness provided during a computer-mediated collaborative task.

On the one hand, there is an intra-personal perspective which is, for instance, represented by the concept of *affect labeling* [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018], according to which putting feelings into words is a form of emotion self-regulation [@grossEmotionRegulationAffective2002; @grossEmotionRegulationCurrent2015], which may even be implicit, since the person may not realize the regulation is taking place. Similarly, one of the main tenet of the RULER approach [@brackettRULERTheoryDrivenSystemic2019; @nathansonCreatingEmotionallyIntelligent2016] -- and emotional intelligence [@mayerAbilityModelEmotional2016; @saloveyEmotionalIntelligence1990] or competence [@schererComponentialEmotionTheory2007] more broadly -- resides in the individual ability to appraise the situation and recognize her own emotional episodes to better cope with the situation. Expressing emotions would therefore help learners to ask themselves how they are feeling by evaluating the situation [@erbasRoleValenceFocus2015; @hoffmannTeachingEmotionRegulation2020; @lavoueEmotionAwarenessTools2019] and finding the corresponding subjective feeling that best depict their emotional state [@grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009]. Applied to the computer-mediated collaborative context, learners could therefore use an EAT because it provides an affordance to differentiate and reflect about how they are feeling in a self-centered process [@boehnerHowEmotionMade2007], aimed at regulating the emotional episodes elicited by the socio-cognitive conflicts of the learning task [@andriessenSociocognitiveTensionCollaborative2011; @arguedasAnalyzingHowEmotion2016]. As a result, learners can benefit from emotional self-awareness, even in the absence of communication with the partner.

On the other hand -- even if the two positions are not mutually exclusive -- many researchers advocate that emotions have pivotal inter-personal functions [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018]. For instance, Rimé [-@rimeEmotionElicitsSocial2009] posits that an individualistic view of emotion and regulation is untenable, and that the usefulness of emotions resides in the social sharing of them. According to this view, a person shares her emotions to make others aware of her emotional state as a way to gain social attention, arouse empathy, stimulate bonding or strengthen social ties (ibid.). Parkinson [-@parkinsonInterpersonalEmotionTransfer2011] introduces the concept of *social appraisal*, according to which emotions in others can serve as useful information to modify our own behavior. An integrated framework of the inter-personal function of emotions is posited by Van Kleef in the Emotion As Social Information (EASI) model [@vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], according to which emotional expression can modify behavior in others through affective reactions or inferential processes about elicited emotions' causes and consequences. For instance, the emotion of a person can represent a trigger for an emotional episode in another person, a phenomenon also known as meta-emotion [@miceliMetaemotionsComplexityHuman2019]. Applied to the computer-mediated collaborative context, thus, learners can therefore use the EAT to sustain the mutual-modeling activity to foster collaborative learning [@dillenbourgSymmetryPartnerModelling2016], especially in the absence of para-verbal cues that are usually available in face-to-face settings [@derksRoleEmotionComputermediated2008]. Learners can benefit from emotional awareness by adapting their behavior according to the crossed-over emotional information available, for instance by engaging in inter-personal emotion regulation [@netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013].

In the few studies that have investigated emotional awareness in computer-mediated collaboration so far [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013], participants who disposed of emotional awareness (*e.g.*, those in the *treatment* group) had access both to their own emotions and that of the partner, knowing beforehand that the emotional information would be crossed-over. This ecological situation is consistent with the mutual-modeling perspective [@dillenbourgSymmetryPartnerModelling2016], according to which the symmetry of the information available to learners is instrumental to build and update a holistic representation of the partner, upon which the collaborative effort can strive. It is nevertheless possible to break down this *full* use of an EAT by varying (1) the sender of emotional information provided, and (2) the recipient of the emotional information expressed [@vankleefInterpersonalDynamicsEmotion2018]. As suggested by Buder [-@buderGroupAwarenessTools2011], varying the characteristics of the same awareness tool -- rather than comparing a *control* group without the awareness tool and a *treatment* group with the tool -- allows a better assessment of its contribution. For instance, by varying the sender and the recipient of the emotional information, three different interfaces of the tool can be obtained:

-   *Self-Centered*: the learner can access only her own emotions, and the emotions she expresses are not conveyed to the partner;
-   *Partner-Oriented*: the learner can access only the partner's emotions and the emotions she expresses are conveyed to the partner, even though she can't access them herself;
-   *Mutual-Modeling*: the learner can access both her own and the partner's emotions, which provide direct and persistent comparison fostering symmetry in the emotional information shared by both partners.

The three versions of the EAT imply different reasons why a learner may be keen to use the EAT to express her own emotions, as well as to seek and process the emotional information provided by the tool. In fact, the interfaces mobilize different theoretical concepts associated with an intra- or inter-personal use of emotional awareness (see Figure \@ref(fig:s1-theory-diagram-figure) for a graphic representation). The *Self-Centered*, linked to the intra-personal function of emotions, activates concepts such as affect labeling, appraisal and self-regulation; the *Partner-Oriented*, related more towards an inter-personal perspective, implies meta-emotions, social-regulation, and social-sharing; and, finally, the *Mutual-Modeling* combines both perspectives by adding symmetry in the emotional information available to both partners. Comparing the three versions of the EAT will thus contribute to determine whether the more *socially-oriented* interface yields a more thorough use of emotional awareness through the EAT.

(ref:s1-theory-diagram-caption) Theoretical concepts mobilized by versions of the EAT differing in the use of, and access to emotional information.

```{r s1-theory-diagram-figure, fig.cap="(ref:s1-theory-diagram-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-theory-diagram.png"))
```

## Research Question and Hypothesis

The phenomenon under scrutiny in the present study is therefore whether a different use of, and access to emotional information elicit a different use of the EAT in terms of its expressing-displaying and the perceiving-monitoring functions [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. More specific hypotheses are stated with respect to each function.

### Use in Expressing Emotions

For expressing-displaying emotions, the three interfaces provide the learner with different reasons to express how she feels, as well as different affective triggers that may elicit emotional episodes (see also below the hypothesis about the use in perceiving emotions). More precisely:

-   With the *Self-Centered* interface, the learner knows she is the only recipient of the emotions she expresses. Therefore, if she decides to use the EAT to express an emotion, she probably does it out of self-interest, possibly linked to self-regulation as stated by the *intra-personal* perspective. In the meantime, the EAT does not provide any additional information about the partner's emotions that may serve as trigger for emotional episodes in the learner herself.
-   With the *Partner-Oriented* interface, the learner knows she will not have access to her own emotions once she has expressed them, but that these are conveyed to the partner. Therefore, if she decides to use the EAT to express an emotion, one can assume that she does it from an *inter-personal* perspective (even if the possibility that she does it exclusively in a *Self-Centered* perspective cannot be excluded). In the meantime, the learner can also access the partner's emotions, which may represent additional triggers for meta-emotional episodes (*e.g.*, *Jane expresses guilt because she thinks Paul has just expressed anger as a result of something she has done*).
-   With the *Mutual-Modeling* interface, the learner knows the emotions she expresses are available both to her and the partner. Therefore, if she decides to use the EAT to express an emotion, she does in a *Self-Centered*, *Partner-Oriented*, or a combined perspective. In the meantime, the learner also disposes of direct and persistent comparison between her own emotions and that of the partner, which may also represent an additional trigger for emotional episodes compared to the *Partner-Oriented* interface (*e.g.*, *Jane expresses relief because she saw from the interface that in the last few minutes both she and Paul were often confused*).

Hypothesis (*H1*) is therefore stated in the following terms: there will be an overall difference in the use of the EAT for expressing-displaying emotions depending on the interface the learner has at disposal. More specifically, in comparing the interfaces, a greater use of the expressing-displaying function of the EAT in the *Partner-Oriented* and *Mutual-Modeling* interfaces compared to the *Self-Centered* interface would corroborate an *inter-personal* interest in expressing emotions. Furthermore, a greater number of emotions expressed in the *Mutual-Modeling* interface compared to the *Partner-Oriented* condition would suggest that the possibility of direct and persistent comparison between one's own and the partner's emotions results in a *surplus* of expression-displaying of emotions. Translated in concrete use:

-   Jane will express more emotions when she knows she is sharing her emotions with Paul compared to when she is expressing emotions only for herself;
-   Jane will express even more emotions when she knows she is sharing her emotions with Paul *and* they can mutually dispose of direct and persistent comparison between their respective emotional episodes.

### Use in Perceiving Emotions

With respect to perceiving emotions, the three interfaces differ in the quality and quantity of the emotional information available on screen. The three interfaces will provide the learner with different reasons to seek and process the emotions expressed during the collaboration:

-   With the *Self-Centered* interface, the learner has access only to the emotions she has expressed over time during the collaboration. This may be interpreted as a *control* condition: what is the interest of having emotional information that the learner is already supposed to know? Seeking and processing the learner's own emotions may be explained by the interest of reflecting on the evolution of her own affective states during the task.
-   With the *Partner-Oriented* interface, the learner has access only to the emotions expressed by the partner, that is, information she does not already know. Seeking and processing the partner's emotions may be explained by the interest in knowing how the other is feeling and/or the evolution of the affective states of the partner during the collaboration.
-   With the *Mutual-Modeling* interface, the learner has access both to her own and the partner's emotions. This condition inserts an additional interest to the previous ones: the possibility of direct and persistent comparison between the learner's own emotions and that of the partner.

Hypothesis (*H2*) is therefore posited as follows: there will be an overall difference in the use of the EAT for seeking and processing the expressed emotions depending on the interface the learner has at disposal. More specifically, the *Partner-Oriented* and *Mutual-Modeling* interfaces will elicit a greater use in perceiving-monitoring emotions compared with the *Self-Centered* interface. Furthermore, greater information seeking and processing in the *Mutual-Modeling* interface compared to the *Partner-Oriented* interface would suggest an accrued interest due to direct and persistent comparison. Translated in concrete use:

-   Jane will seek more often and process the emotional information longer when she can access Paul's rather then her own emotions;
-   Jane will seek even more often and process the emotional information even longer when she can access, for direct and persistent comparison, both her own and Paul's emotions at the same time.

## Methods

I report how I determined the sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

### Participants and Design

48 participants (29 women, 19 men), aged 18 to 55 ($M=37.3, SD=10.01$), voluntarily participated to the study. The sample size was determined by time constraints, since data had to be collected in 15 days. 23 participants were university students from different faculties, both at undergraduate and graduate levels. 25 participants were professionals working for a company adopting distance learning practices. No remuneration was provided for taking part in the study. Participants were randomly assigned to one of the three conditions/interfaces (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*) in order to produce a balanced design with 16 participants per condition.

### Material

#### Overall Interface of the CSCL Task

The experimental material comprises different components; I therefore provide an overview before specifying the various details. Figure \@ref(fig:s1-task-interface-figure) shows the disposition of the screen during the experimental task. It comprises the EAT on the left-side of the screen, outlined in blue, and the simulated, joint-problem solving task on the right. The image indicates what part of the interface was simulated for the *Mutual-Modeling* condition. The interfaces of the other conditions are illustrated below. Some parts of the interface have been translated in English for the current contribution. In the experiment, though, the french language was used consistently for every condition.

(ref:s1-task-interface-caption) Overview of the interface that presents the various part of the material adopted for the CSCL task.

```{r s1-task-interface-figure, fig.cap="(ref:s1-task-interface-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-task-interface.png"))
```

#### Problem-Solving Task

The joint problem-solving task comprised four enigmas taken from a game. The first three enigmas had a clear response that could be inferred, whereas the last one was more of a non-nonsensical type. The same enigmas have been used in a previous study [@fritzReinventingWheelEmotional2015], where they elicited different emotions both in number and kind in a population similar to that of the current sample. Each enigma was subdivided in three phases:

-   40 seconds during which the text of enigma was showed on the interface. At this stage, participants could only express their emotions, but could not write their reasoning or the reply;
-   3 minutes and 20 seconds during which participants could write their reasoning and reply to the enigma, as well as see the *playback* reasoning (but not answer) of the simulated partner;
-   1 minute in which the given answers from the participant and the partner were displayed on screen with the expected solution to the enigma. At this stage, once again, the reasoning and reply fields were not available on screen.

#### Configuration of the Emotion Awareness Tool

The version of the DEW was composed as follows. The Valence and Control/Power dimensions [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005] determined the appraisals for the expression-displaying of emotional episodes. Valence was prompted with the question "*Is the situation pleasant?*", whereas Control/Power with the question "*Is the situation under your control?*". Both evaluations were determined with the extreme negative pole *Not at all*, and extreme positive pole *Yes, absolutely*. The underlying affective space was configured using a radial circumplex with the 20 EATMINT emotions also used in Fritz [-@fritzReinventingWheelEmotional2015]. The composition of the circumplex has been detailed in the previous chapter (add reference here).

For the monitoring/perceiving function of the EAT, each condition differed in the following ways (depicted in Figure \@ref(fig:s1-eat-interfaces-figure)):

-   *Self-Centered*: the interface comprises an emotion time-line, then a graphic line chart that depicts the evolution of the appraisal dimensions over time, and finally a tag cloud where the size of each subjective feelings is proportional to the frequency with which it has been expressed. The information provided is based only on the emotions expressed by the participant herself.
-   *Partner-Oriented*: the interface is the same as in the *Self-Centered* condition, but the information provided is based only on the emotions expressed by the simulated partner (see below).
-   *Mutual-Modeling*: the interface comprises an emotion time-line, but with both the participant and the simulated partner's subjective feelings organized in two different rows. Two line charts complete the interface, one with the appraisal dimensions of the participant, and the other of the partner.

The *Self-Centered* and *Partner-Oriented* conditions presents a tag cloud in order to balance the surface of the EAT that contains information. In this way, the EAT occupies more or less the same amount of the screen.

(ref:s1-eat-interfaces-caption) The three different interfaces used in the experiment. In order from left to right: the *Self-Centered*, the *Partner-Oriented*, and the *Mutual-Modeling* versions.

```{r s1-eat-interfaces-figure, fig.cap="(ref:s1-eat-interfaces-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-eat-interfaces.png"))
```

#### Simulated partner

```{r s1-simulated-partner}
s1.simulated_partner <- s1.dew_configuration$task$simulation %>%
  select(-user) %>%
  filter(time <= 1200) %>%
  left_join(s1.feelings_translation, by = c("feeling" = "label"))
```

The *playback* manipulations displayed on the interface were recorded in a pilot test with 4 confederates: 2 men and 2 women performed the same joint problem-solving task, but in a synchronous situation. The *playback* is thus comprised by: (1) all the emotional episodes expressed, with both the evaluation on the appraisal dimensions and the related subjective feeling expressed; and (2) what confederates have typed, at the very same moment, into the reasoning field, as well as the answer to each of the 4 enigmas. In this regard, confederates were explicitly asked not to communicate directly through the text fields, but limit their typing to the reasoning for solving the problem. One of the *playback* was then randomly chosen for the task and *injected* into the experimental task interface. The simulated partner expresses `r s1.simulated_partner %>% nrow()` emotions and finds the solution to 2 out of 4 enigmas. The full list of emotions -- comprising the time of expression, the associated Valence and Control/Power appraisals, and the subjective feeling -- are depicted in Table \@ref(tab:s1-simulated-emotions-table).

(ref:s1-simulated-emotions-caption) List of the emotions of the simulated partner. Time is expressed in seconds.

```{r s1-simulated-partner-table}
kable(s1.simulated_partner,
  col.names = c("Time", "Valence", "Control", "Feeling (FR)", "Feeling (EN)"),
  caption = "(ref:s1-simulated-emotions-caption)\\label{tab:s1-simulated-emotions-table}",
  caption.short = "Study 1: Simulated Partner's Emotions",
  longtable = FALSE,
  booktabs = TRUE
)
```

#### Eye-tracking

A Tobii T120 eye-tracker with Tobii Studio Pro v3.4.8 software [@tobiiabTobiiStudio2015] was used for eye-tracking measures. Areas Of Interest (AOI) were disposed on the EAT as a whole (left side of the screen) and the task (right side of the screen). The AOI of the EAT was further divided in the displaying/expressing upper zone, and the monitoring/perceiving lower zone.

### Procedure

Participants were given a specific time to come to the test, which was performed at Geneva University, and were reminded of the importance to be on time since another participant was performing the test in the meantime. The experimenter welcomed the participant in the room with the eye-tracking equipment. Once installed, the experimenter proceeded to explain the outline of the study:

-   Introduction and explication (10 minutes)
-   Warm-up session with the DEW and instructions for the task (5 minutes)
-   Collaborative task (20 minutes)
-   Debriefing (10 minutes)

#### Introduction and explication

The general aim of the study was explained. The experimenter reassured participants about the fact that the data would be anonymous, and that they could stop the experiment at any time without any reason. A first consent form was then signed if the participant agreed to take part in the study.

At this point, the experimenter explained how the collaborative task would take place. She first showed a demo about the functioning of the DEW. Since in a previous study [@fritzReinventingWheelEmotional2015], whose aim was to observe the spontaneous use of the tool, participants were confused about the dimension of Control/Power, in this study the experimenter proposed a more thorough explanation of what the two sliders of the DEW stand for. The explication also aimed at reducing the risk that participants will move the cursors for the Valence and Control/Power dimensions until they found the *right* subjective feeling. Next, the experimenter showed the perceiving-monitoring part of the EAT, which was explained according to the experimental condition the participant was attributed to. In this way, participants were informed about both what the emotional information they provided would be used for (*Self-Centered* vs *Partner-oriented/Mutual-modeling*), and to which emotional information they would have access (*Self-Centered* vs *Partner-Oriented* vs *Mutual-Modeling*).

Finally, the experimenter explained the right-hand side of the screen, which implemented the joint problem-solving task. Participants were informed about the three parts (reading, solving and solution) that composed each of the 4 enigmas to solve. They were also prompted to write their reasoning to solve the problem in the appropriate field, but to avoid direct communication with the partner.

#### Warm-up session with the DEW

Participants were placed in front of the screen used for the test and could practice with a simplified version of the interface for the task. Participants could familiarize with the expression of emotions through the DEW, and random emotions were also injected in the interface at short intervals to emulate the emotion of the partner. The side of the screen devoted to the task was filled in with generic texts explaining what participants will see in the actual task (*e.g.*, *here will appear the text of the enigma*, *here you must write your reasoning*, ...)

#### Experimental task

Once the participant was ready for the test, the experimenter simulated to check-in with another confederate to simulate that the other participant was ready to start the experiment as well. Then the experimenter proceeded to calibrate the eye-tracker equipment. After being reminded about the general functioning of the eye-tracker and the importance of not moving during the task, the participant would then proceed with the task. At first, she had to fill a sort of *log-in form*, providing a random ID and an identifier for the pair. Once the task properly started, the participant had access to the overall interface depicted above, including the *playback* of all the manipulations made by a confederate.

#### Post-test debriefing

After the task, participants were asked to fill in a survey with information that will not be used in the present contribution, but have been analyzed by Perrier [-@perrierCollaborationEnvironnementMediatise2017]. At the end of the study, participants were informed about the manipulation of the simulated partner and the experimental reasons behind it. A second consent form was therefore submitted to participants, for them to confirm they understood the reason of the manipulation, and that they accepted the use of the data.

### A Priori Exclusion Criteria

Exclusion criteria determined beforehand concerned only technical issues that could jeopardize the task, especially with respect to the simulated partner. Any interruption of the task or technical failure would make the trial not recoverable. Exclusion caused for low quality of eye-tracking measures, due for instance to the participant moving too much, were also foreseen, but not yet quantified due the lack of a precise benchmark.

### Data analysis

For hypothesis *H1*, concerning differences in expression of emotions, a omnibus one-way ANOVA with pairwise comparison between all conditions was planned beforehand. The number of emotions expressed through the EAT represents the dependent variable, and the interface of the EAT the independent variable.

For hypothesis *H2*, concerning differences in perception of emotions, two indicators retrieved by eye-tracking measures [@blascheckVisualizationEyeTracking2017; @pooleEyeTrackingHumanComputer2005] are used as dependent variables. First, the total time (in seconds) that participants spent looking at the perceiving-monitoring zone of the interface. Such indicator is usually interpreted as a proxy for information processing and could account for interest (*i.e.*, people look at it longer because it is interesting) or complexity (*i.e.*, people look at it longer because they need more time to understand what it means). Second, the number of times participants sought information by orienting their gaze inside the perceiving-monitoring zone of the interface, which is usually interpreted as an indicator that the person intentionally seeks for information she may find useful or that she got lost and needs reorientation. Given the relative simplicity of the information provided -- even though people do not like graphs [@carpenterModelPerceptualConceptual1998; @pinkerTheoryGraphComprehension1990] -- and the use of a fixed interface of the EAT, both measures are used as indicators of interest. For both measures, a omnibus one-way ANOVA with pairwise comparison between all conditions were planned beforehand. A family-wise correction to account for inflation in Type I error has been planned for each pairwise comparison.

Within this experimental setting, a sensitivity analysis reveals that, given the planned sample $N = 48$ and conventional $\alpha$ = 0.05 and $\beta$ = 0.8 error rates control, each statistical test will be able to detect an effect size of Cohen's $d$ = `r printnum(s1.detectable_d)`. Taking a previous study using exactly the same task as benchmark, that would translate in practical terms as follows:

-   For expressing emotions, the statistical test will detect a difference of `r printnum(s1.delta_emotions)` expressed emotions or greater, given a mean of $M=`r printnum(mean(fritz2015_num_emotions))`$ ($SD = `r printnum(sd(fritz2015_num_emotions))`$) observed in Fritz (2015)
-   For information processing, the statistical test will detect a difference of `r printnum(s1.delta_processing)` seconds or greater, given a mean of $M=`r printnum(mean(fritz2015_eyetracking$Total_Fixation_Duration_Monitoring_Sum))`$ ($SD =`r printnum(sd(fritz2015_eyetracking$Total_Fixation_Duration_Monitoring_Sum))`$) observed in Fritz (2015);
-   For information seeking, the statistical test will detect a difference of `r printnum(s1.delta_seeking)` number of visits or greater, given a mean of $M=`r printnum(mean(fritz2015_eyetracking$Visit_Count_Monitoring_Sum))`$ ($SD =`r printnum(sd(fritz2015_eyetracking$Visit_Count_Monitoring_Sum))`$) observed in Fritz (2015).

All analysis are conducted using the statistical software R, version 4.0.5. Analysis of variance use the Afex package version `r packageVersion("afex")` [@singmannAfexAnalysisFactorial2020].

## Results

### Post-Hoc Exclusion

Results will be based on $N = 35$ participants. 10 participants were excluded due to technical issues during the task or low quality of eye-tracking measures. One participant was excluded for statistical reasons: the participant expressed 62 emotions during the task, against a mean of `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)`, that is, more than 8 standard deviations above the mean. Such a number, not even close to any participant to the same task in Fritz (2015), suggests a non representative use of the tool. The distribution of participants after post-hoc exclusions with respect to the experimental conditions is depicted in Table \@ref(tab:s1-observed-design-table).

(ref:s1-observed-design-capture) Number of participants retained for each experimental condition ($N = 35$).

```{r s1-observed-design}
s1.participants_repartition <- s1.et_data %>%
  group_by(group) %>%
  summarise(
    n = n()
  )

kable(s1.participants_repartition,
  col.names = c("Condition", "N"),
  caption = "(ref:s1-observed-design-capture)\\label{tab:s1-observed-design-table}",
  caption.short = "Study 1: Observed Experimental Design",
  longtable = FALSE,
  booktabs = TRUE,
) %>% 
  kable_styling(latex_options = "HOLD_position")
```

The resulting unbalanced design and overall small $N$, particularly low in the *Partner-Oriented* condition, decrease the power of the planned test and make it more exposed to violation of assumptions [@hoekstraAreAssumptionsWellKnown2012], which are checked in Appendix A. The interpretation of results must therefore take into account these limits.

### Differences in Expressing Emotions

Participants expressed a total of `r s1.dew_emotions %>% nrow()` emotions, which corresponds to a mean close to 14 emotions per participants (`r maf.print_m_sd(s1.aggregated_emotions$n, TRUE, FALSE)`). Participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions; `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Partner") %>% pull(n), FALSE, TRUE)` in the *Partner-Oriented* condition; and `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Mutual") %>% pull(n), FALSE, TRUE)` in the *Mutual-Modeling* condition (see Figure \@ref(fig:s1-expressed-emotions-graph)).

(ref:s1-expressed-emotions-graph-caption) Number of emotions expressed by experimental condition. Bars represent 95% confidence intervals.

```{r s1-expressed-emotions-graph, fig.cap="(ref:s1-expressed-emotions-graph-caption)", fig.align="center", out.width="60%"}

maf.plot_means_comparison(s1.aggregated_emotions, aes(x = group, y = n, color = group)) +
  labs(x = NULL, y = "Number of emotions") +
  theme(legend.position = "none")
```

No detectable difference, neither in the omnibus one-way ANOVA (`r apa_print(s1.anova.expressed_emotions)$full_result`), nor in the pairwise comparisons could be observed. Results of the comparisons are depicted in Table \@ref(tab:s1-expressed-emotions-comparison-table). Hypothesis (*H1*) is therefore rejected: a different access to and use of emotional information did not yield detectable differences in the number of emotion expressed.

(ref:s1-expressed-emotions-comparison-caption) Pairwise comparisons of the thee conditions with respect to the number of emotions expressed (*p*-values are adjusted with the Tukey method).

```{r s1-expressed-emotions-contrats}
s1.anova.expressed_emotions.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-expressed-emotions-comparison-caption)\\label{tab:s1-expressed-emotions-comparison-table}",
    caption.short = "Study 1: Number of Emotions Expressed per Condition",
    longtable = FALSE,
    booktabs = TRUE
  )
```

### Differences in Perceiving Emotions

The perception of the emotional information is divided in emotional information processing and emotional information seeking.

#### Processing Emotional Information

Participants spent on average `r maf.print_m_sd(s1.total_visit_duration$value, TRUE, TRUE)` seconds looking at any part of the perceiving-monitoring zone of the interface, which amounts to 4.28% of the total task time. Participants in the *Self-Centered* condition spent `r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)` seconds, whereas this time roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which time differed slightly (see Figure \@ref(fig:s1-total-visit-duration-graph)).

(ref:s1-total-visit-duration-graph-caption) Total time (in seconds) spent looking at the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-total-visit-duration-graph, fig.cap="(ref:s1-total-visit-duration-graph-caption)", out.width="60%", fig.align="center"}
maf.plot_means_comparison(s1.total_visit_duration, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Seconds") +
  theme(legend.position = "none")
```

An overall effect of the experimental condition on the time spent processing emotional information could be observed (`r apa_print(s1.anova.total_visit_duration)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-total-visit-duration-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-total-visit-duration-caption) Pairwise comparisons of the thee interfaces with respect to total time spent looking at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-total-visit-duration-contrats}
s1.anova.total_visit_duration.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-total-visit-duration-caption)\\label{tab:s1-total-visit-duration-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Total Visit Duration.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

#### Seeking Emotional Information

Participants' gaze entered the perceiving-monitoring zone of the interface on average `r maf.print_m_sd(s1.visit_count$value, TRUE, TRUE)` times. In the *Self-Centered* condition, the number of visits has been `r maf.print_m_sd(s1.visit_count %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)`, whereas the count roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which the count was similar (see Figure \@ref(fig:s1-visit-count-graph)).

(ref:s1-visit-count-graph-caption) Total visits count in the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-visit-count-graph, fig.cap="(ref:s1-visit-count-graph-caption)", out.width="60%", fig.align="center"}

maf.plot_means_comparison(s1.visit_count, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Number of visits") +
  theme(legend.position = "none")
```

An overall effect of the interface adopted on emotional information seeking could be detected (`r apa_print(s1.anova.visit_count)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-visit-count-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-visit-count-caption) Pairwise comparisons of the three interfaces with respect to the number of visits at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-visit-count-contrats}
s1.anova.visit_count.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-visit-count-caption)\\label{tab:s1-visit-count-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Visits Count.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

## Discussion

The planned analyses aimed at investigating whether a different use of, and access to, emotional information determine differences in use of an EAT during a computer-mediated collaborative task, which -- beside evident shortcomings in the *collaborative* nature -- may be considered representative of a CSCL application in distance learning. The reduced sample size upon which the analyses are based requires caution in interpreting the obtained results. The inter-individual differences in all measured dependent variables entail wide confidence intervals, whose source can be traced back to the many processes implicated in the task. Some of them may not be directly inherent to a genuine interest in emotional awareness, and may have potentially influenced participants' capacity in conveying and taking into account emotional awareness beyond their intentions. For instance, participants had to coordinate multiple functions, both cognitively and practically (e.g. writing on a keyboard, manipulating the EAT, etc.), under specific time constraints. Participants with less dexterity in writing at the keyboard or manipulating the interface may have found less time to dedicate to the EAT even if they were willing to. The small sample size cannot guarantee that these individual differences are sufficiently balanced by the randomized trial. Even in the presence of detectable effects, thus, the assessment of their relevance in terms of *practical* consequences is limited: their size is inherently high due to the small sample and they should not even be taken as reliable benchmarks for future studies [@albersWhenPowerAnalyses2018].

On the other hand, the controlled environment in which the *performance-based* measures have been obtained make them worth of interest in assessing to what extent the presence of an EAT serves as an affordance in conveying and taking notice of emotional awareness during a computer-mediated collaborative task. Eye-tracking measures, in particular, may be considered spontaneous reactions occurring to some extent even beyond participants' top-down control. The discussion of the obtained results may thus contribute to sketch a more defined outlook of the use of an EAT and provide cues for further hypotheses worth investigating or shortcomings to be taken into account in future studies.

### Expressing Emotions May not Depend Exclusively on Social Sharing

In the first hypothesis, it has been posited that learners expression of emotions through the EAT varies depending on what use would be made of them, and what emotional information they have access to through the interface. More precisely, it has been stated that participants in the *Self-Centered* condition would express fewer emotions, compared to the *Partner-Oriented* and *Mutual-Modeling* condition, because of the absence of social sharing. It has also been posited that participants in the *Mutual-Modeling* condition would express more emotions than in the *Partner-Oriented* condition by virtue of an additional prompt in social sharing due to the direct and persistent comparison between one's own emotions and that of the partner.

Results failed to corroborate any of these assumptions given that neither an overall effect, nor differences in the comparison between each condition could be detected. The effect size to yield significant results was already ambitious with the planned $N=48$ sample size, and was therefore even further undercut by post-hoc exclusions, for which even observed effect sizes above $d = 0.5$ (*Self-Centered* vs *Partner-Oriented* and *Self-Centered* vs *Mutual Modeling*) cannot yield a discernible difference. Hypothesis *H1* must therefore be provisionally rejected, even though the observed directions and size of the effects are, in part at lest, consistent with an increased expression of the emotions with a more social-oriented interface.

Notwithstanding the limits of the sample, it is also worth reversing the perspective and, rather, highlight how participants in all conditions expressed on average `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)` emotions, that is more than 1 emotion every 2 minutes of task. In particular, participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions despite knowing they were the only recipient of the information. This result could be considered, in principle at least, as support for *not* ruling out the interest of *intra-personal* interest in expressing emotions: the presence of an EAT could indeed serve as an *individual affordance* for emotional expression as a support for (implicit) emotion regulation. On the other hand, though, this result may also be explained by side effects of the experimental task. For instance, this number may be inflated by task compliance, since the overall experimental setting was overtly aimed at expressing emotions. Furthermore, the characteristics of the experimental task, whose timing is fixed and not determined by the participants' actions, may also have pushed participants to express emotions to *fill-in* idle time between enigmas or part of the task within each enigma, rather than for an urge to express and regulate their emotions. A more fine-grained analysis of the time of expression should therefore be taken into account.

All things considering, thus, the present contribution conveys limited and mixed evidence with respect to the interest for expressing emotions during a computer-mediated collaborative task. Nevertheless, the experimental settings elicited considerable variation in the number of emotions expressed: with an increased sample size, the experimental plan could potentially contribute to assess the matter more thoroughly, for instance using a planned equivalence test with the aim to rule out differences, rather than detecting ones [@fidlerEpistemicImportanceEstablishing; @lakensEquivalenceTestsPractical2017].

### Emotional Seeking and Processing Seem Related to Social Sharing

In the second hypothesis, it has been posited that learners would seek and process the emotional information available on screen depending on the source and the comparison it facilitates. More precisely, it has been stated that participants in the *Self-Centered* condition would seek less often and process for shorter time the information available through the perceiving-monitoring part of the interface, compared to the *Partner-Oriented* and *Mutual-Modeling* conditions. It has also been posited that participants in the *Mutual-Modeling* condition would seek information more often, and process it longer compared to the *Partner-Oriented* condition due to the increased interest enhanced by direct and persistent comparison of emotional information.

Results corroborate the presence of an overall effect of the interface both on emotional information seeking and processing. For information seeking, the experimental condition yielded a generalized effect size [@olejnikGeneralizedEtaOmega2003] of `r apa_print(s1.anova.visit_count)$estimate$group`, and of `r apa_print(s1.anova.total_visit_duration)$estimate$group` for information processing. In both cases, thus, the experimental condition seems to account for a considerable amount of variation in the perceiving-monitoring function of emotional awareness.

On a more fine-grained level, though, the differences between conditions only partially corroborated the directional hypothesis; differences were detected, both for information seeking and processing, only in pairwise comparisons between *Self-Centered* vs. *Partner-Oriented* (Cohen's $d$ \> 1 for both measures) and *Self-Centered* vs. *Mutual-Modeling* (again Cohen's $d$ \> 1 for both measures), but not between *Partner-Oriented* vs. *Mutual-Modeling*, which was actually a more severe test [@mayoStatisticalInferenceSevere2018].

The lack of a detectable difference between the *Self-Centered* and the more social-oriented interfaces would have undermined the usefulness of an EAT, whereas its presence may be explained as a *simple* novelty effect: the *Partner-Oriented* and *Mutual-Modeling* condition convey information that the learner does not know, whereas in the *Self-Centered* condition the emotions are just a reminder of what the learner should already know. Nonetheless, taking the raw measures as benchmark, it is reassuring to observe that in a task of 20 minutes, the time spent looking at emotional information is around 1 minute when information about the partner is available, compared to 30 seconds when it is not. As it is the case for the expression of emotions, though, information processing and seeking in the *Self-Centered* measures must be considered as support for an intra-personal interest for the use of an EAT, even in the absence of communication with the partner.

The comparison between the *Partner-Oriented* and *Mutual-Modeling* interfaces has deeper implications with respect to the *raison d'être* of an EAT. The lack of a discernible effect between the two social-oriented conditions may suggest that there is no additional value conveyed by direct and persistent comparison available on screen. But the phenomenon could also be explained by the fact that the additional value is gained without the need for further information seeking and processing. That is, participants in the *Mutual-Modeling* condition were able to compare their own and the partner emotion thorough the interface without having to look at the perceiving-monitoring zone of the EAT more often or longer, because they could get more information for the same effort. The eye-tracking measures alone cannot unravel whether the lack of a discernible effect is a positive or negative outcome with respect to the social-oriented hypothesis. In future studies, the hypothesis should therefore also be assessed with the aid of self-reported measures about the perceived usefulness of direct and persistent comparison of learners' emotional states.

## Post-Hoc Corollary Analyses

In this section, I provide the results of additional analysis that have not been planned before the study. First, I extended the analysis of eye-tracking measures using transitions between Areas Of Interest as an interesting measure of the use of an EAT. Second, I provide indications of the use of the EAT in real-time with respect to the appraisals and subjective feeling measures collected through the task. And finally, I take advantage of the use of the same task as in @fritzReinventingWheelEmotional2015 to conduct a small, internal meta-analysis that can be of interest for the use of the same task in future studies.

### Transitions Between Areas of Interest

The eye-tracking measures used in the planned analyses of variance treated each zone of the interface as a separated element. Given the importance of dynamic, real-time phenomena in the overall thesis, it is worth investigating also transitions between the three main Areas of Interest (AOI) of the experimental task, that is (1) the expressing zone, which is common to all conditions; (2) the perceiving zone, which varies according to the experimental condition; and (3) the area dedicated to the *main* task, which is also common to all conditions. Given that transitions can go in either direction between AOI, there are 6 possible combinations of transitions: (1) Expressing to Perceiving and (2) Perceiving to Expressing; (3) Expressing to Task and (4) Task to Expressing; and finally (5) Perceiving to Task and (6) Task to Perceiving. An exploratory analysis of the transitions may reveal whether specific transitions are more frequent than others depending on the interface at disposal, and thus contribute to better assess the perceiving-monitoring function of an EAT.

The number of transitions between AOI was computed by searching for subsequent rows in the eye-tracking logs for each of the $N = 35$ participants in which the first row had a certain AOI activated, and the following row had another AOI activated. Is it worth noting, though, that this method is sub-optimal because the experimental task included the use of a keyboard. Therefore some transitions may have been lost due to the fact that each gaze-path may have been interrupted by a *detour* to the keyboard. Nevertheless, it is safe to assume that participants directed their gaze into the AOI they were interested in acting upon -- for instance, in order to focus the pointer into the text area -- before turning the gaze away from the screen if they needed to look at the keyboard for typing. All things considering, thus, this method can be of interest at least as an exploratory method, even though it lacks external validity and should be revisited before being deployed in a substantial analysis.

After seeing the data, one participant was excluded for having a number of transitions from Expressing to Perceiving and from Perceiving to Expressing much higher than all other participants regardless of the group: more than 100 against a mean of `r maf.print_m_sd(s1.transitions %>% filter(transition == "Expressing to Perceiving" || transition == "Perceiving to Expressing") %>% pull(num_transitions), FALSE, TRUE)` for the other participants regardless of the condition. Results are therefore based on $N = 34$ participants.

Participants made on average `r maf.print_m_sd(s1.transitions %>% group_by(ParticipantName) %>% summarise(total_transitions = sum(num_transitions)) %>% pull(total_transitions), TRUE, TRUE)` transitions between any two AOI. Figure \@ref(fig:s1-transitions-graph) reports the number of transitions stratified by experimental condition between the 6 AOI organized in three rows such as each row displays the transitions between the same two AOI in both directions.

(ref:s1-transitions-graph-caption) Number of transitions between Areas of Interest (AOI) on the interface. Transitions aggregated for $N = 34$ participants. Bars represent 95% confidence intervals.

```{r s1-transitions-graph, fig.cap="(ref:s1-transitions-graph-caption)", fig.height=6, out.width="100%"}
s1.transitions.graph
```

Data suggest that there are differences that may be accounted for by the type of interface the participants have access to. In particular, participants in the *Mutual-Modeling* condition seem to be more prone to make transitions between the two AOI that are more directly related to the social sharing of emotions. Transitions between the *expressing-displaying* and the *perceiving-monitoring* zone (first row in the graphic) may indicate that the possibility of direct and persistent comparison of one's own emotions with that of the partner could serve as *social reference* before expressing one's own emotions, or *social comparison* after having expressed them. Furthermore, transitions between the *perceiving-monitoring* zone and the *task* zone (last row in the graphic) may indicate that the emotional information is taken into account as instrumental information to the task at hand.

Participants in the *Self-Centered* condition seem to privilege the paths between the *expressing-displaying* zone and the *task* zone, which is consistent with the fact that the *perceiving-monitoring* zone has only information about their own emotions. It is interesting to notice, though, that in the *Self-Centered* condition, transitions from the *expressing-displaying* zone to the *perceiving-monitoring* zone (first row, graph on the left) do not seem to be more frequent compared to the *Partner-Oriented* condition. This may be relevant because it could rule out the possibility that a difference between the *Partner-Oriented* and *Mutual-Modeling* condition may be due simply to the fact that, in the *Mutual-Modeling* condition, participants only seek confirmation of what they have expressed, since this confirmation is not available in the *Partner-Oriented* condition.

Finally, the *Partner-Oriented* condition seems once again *stuck in the middle*, and results for this group are difficult to assess due to the greater inter-individual variance that is also present in the other planned analysis. As a rule of thumb interpretation, the *Partner-Oriented* condition seems to go hand-in-hand with the *Self-Centered* condition in transitions between the *expressing-displaying* zone and the *perceiving-monitoring* zone (first row); and with the *Mutual-Modeling* condition in the other transitions (second and third rows).

In an attempt to figure out whether this kind of analysis may be used in a more structured manner, a multilevel linear model, also known as mixed linear model [@batesFittingLinearMixedEffects2014; @kuznetsovaLmerTestPackageTests2017; @westLinearMixedModels2015], was fitted to the data at hand using the mixed function of the Afex [@singmannAfexAnalysisFactorial2020; @singmannIntroductionMixedModels2020] R package version `r packageVersion("afex")`. The model was fitted in the following terms: the aggregated number of transitions per participant for each possible path represented the outcome variable; the type of transition and the interface of the EAT (*i.e.* the experimental condition) were considered as fixed factors, with an interaction between the two; the participant was used as a random intercept to account for the non-independence of observations. A more complex model could have been more interesting, but hardly feasible due to the small number of participants [@batesParsimoniousMixedModels2018].

A Type III Analysis of Variance of the multilevel linear model confirms effects of both individual factors and the interaction. Results are depicted in Table \@ref(tab:s1-transitions-anova-table) using Kenward-Roger approximation for computing the *p*-value [@lukeEvaluatingSignificanceLinear2017].

(ref:s1-transitions-anova-caption) Results of a Type III ANOVA on the fitted multilevel linear model

```{r s1-transitions-anova-table}
s1.transitions.lmm.anova_table %>%
  kable(
    caption = "(ref:s1-transitions-anova-caption)\\label{tab:s1-transitions-anova-table}",
    caption.short = "Study 1: Transitions between AOI",
    booktabs = TRUE,
    longtable = FALSE
  )
```

Table \@ref(tab:s1-transitions-comparisons-table) reports the pairwise comparisons between the three experimental conditions stratified by the bi-directional path of the transition. Detectable differences in the pairwise comparisons can be observed between *Self-Centered* vs *Mutual-Modeling* and *Partner-Oriented* vs *Mutual-Modeling* in the transitions between expressing-displaying and perceiving-monitoring. In the four comparisons, the more *social-oriented* interface obtained more transitions, in both directions, than the less *social-oriented* one, corroborating the assumption that participants make use of the emotional information about the partner as a reference.

In the transitions between expressing-monitoring and the task, only the path going from the task to the expression-displaying zone yielded a detectable difference with more transitions in the *Partner-Oriented* than in the *Self-Centered* interface. The effect is nevertheless not corroborated by any other comparison in the same transition path.

Finally, in the transitions between perceiving and the task, detectable differences were observed between the *Self-Centered* and the *Mutual-Modeling* interfaces, with the *Mutual-Modeling* interface yielding more transitions in both directions. These results may support the role of emotional awareness as instrumental information directly related to the task at hand, but are not corroborated by a difference between the *Self-Centered* and the *Partner-Oriented* interfaces.

(ref:s1-transitions-comparisons-caption) Comparisons between the groups stratified by the path of the transitions. The Kenward-Roger approximation for the degrees of freedom is adopted and *p*-values are adjusted using the Tukey method for comparing a family of 3 estimates.

```{r s1-transitions-comparisons-table}
s1.transitions.contrasts$contrasts %>%
  as_tibble() %>%
  select(-transition) %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    caption.short = "Study 1: Pairwise comparison between transitions",
    longtable = FALSE,
    booktabs = TRUE,
    linesep = c("", "", "\\addlinespace"),
    col.names = c("Comparison", "Est.", "SE", "df", "t.ratio", "p.value"),
    caption = "(ref:s1-transitions-comparisons-caption)\\label{tab:s1-transitions-comparisons-table}",
  ) %>%
  kable_styling(
    latex_options = c("repeat_header")
  ) %>%
  pack_rows("Expressing to Perceiving", 1, 3) %>%
  pack_rows("Perceiving to Expressing", 4, 6) %>%
  pack_rows("Expressing to Task", 7, 9) %>%
  pack_rows("Task to Expressing", 10, 12) %>%
  pack_rows("Perceiving to Task", 13, 15) %>%
  pack_rows("Task to Perceiving", 16, 18)
```

All things considering, transitions may represent a more interesting measure of the perceiving-monitoring function of emotional awareness compared to the information seeking and processing measures adopted in the planned analyses. Even considering the shortcomings (e.g. transitions interrupted by the use of the keyboard), transitions provide a more *dynamic* outlook on how emotional information is integrated into the task. The concept of transition may even be pushed further by measuring at which moment the transition has occurred, which would provide useful information on the dual-task nature of emotional awareness. For instance, it would be possible to assess whether learners look at the emotions expressed by the partner as soon as they appear on the interface, or if they wait idle period in the task.

### Emotions and Time: Evaluating the Purpose of Real-Time Awareness

One of the main tenets of the present contribution is the advantage of *real-time* emotional awareness -- at least to the extent that participants do not have to wait predefined stops to share their emotions. Some exploratory analyses on the sample were performed in order to check to what extent the *real-time* feature has been exploited with respect to the expression of emotions.

#### Cognitive Evaluation Over Time

Congruently with appraisal theories of emotions -- which state that it is the evaluation one does of the situation and not the situation *per se* that elicits the emotion -- it is worth checking for the emergence of a pattern in the appraisals of Valence and Control/Power over time. Since all participants were exposed to the same stimuli (except participants in the *Self-Centered* condition, who did not see the emotions of the simulated partner), a clear pattern in the evaluation of the two criteria would not be congruent with appraisal theories. Figure \@ref(fig:s1-appraisal-evolution-graph) shows all the $N = `r nrow(s1.dew_emotions)`$ emotions that have been expressed by all the participants over the 20 minutes of the task, stratified by condition. For each observation, the value of Valence and Control/Power are displayed with respect to the elapsed time in the task. A Locally Estimated Scatterplot Smoothing (LOESS) -- that is, non-parametric curve that best fit the empirical data [@jacobyLoessNonparametricGraphical2000] -- is superposed to the raw data.

(ref:s1-appraisal-evolution-caption) Evolution of the appraisal dimensions over time with a LOESS smoother ($N = 35$, all emotions of participants are aggregated per condition).

```{r s1-appraisal-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-appraisal-evolution-caption)"}

grid.arrange(
  s1.appraisal_over_time_valence,
  s1.appraisal_over_time_control,
  nrow = 1
)
```

The graphic indicates that the smoother remain, overall, close to the neutral point, since data-points are evenly spread over the elapsed time in the task. It is worth noting, though, that in the *Mutual-Modeling* condition, both appraisals decreased as the task went on.

#### Subjective Feelings Over Time

The same analysis can be conducted with respect to the expression of subjective feelings over time. Figure \@ref(fig:s1-feelings-evolution-graph) below plots the evolution of the expression of the 20 subjective feelings -- which are part of the underlying affective space used in the study -- aggregated for all $N=35$ participants. The observations are stratified per condition. (In order to reduce the space of the graph, the legend for each condition has been omitted, but the colors are congruent with previous graphs.)

(ref:s1-feelings-evolution-caption) Expression of the subjective feelings. $N = `r nrow(filter(s1.dew_emotions, listed == TRUE))`$ emotions (out of `r nrow(s1.dew_emotions)`) whose subjective feeling belongs to the underlying affective space used by the DEW, aggregated for the $N = 35$ participants. (Legend omitted for reducing space, see previous graphs.)

```{r s1-feelings-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-feelings-evolution-caption)"}
s1.feelings_over_time_graph
```

Not considering the feelings that have been expressed only a few times (*e.g.*, *Envious* or *Disgusted*), most of the subjective feelings have been expressed rather uniformly over the 20 minutes of the task. Interesting exceptions are the feelings *Bored* and *Frustrated* that only starts around 5 minutes into the task -- that is, around the end of the first enigma -- which may be due to the repetitive nature of the task for boredom, and the increasing difficulty of the enigmas for frustration.

Finally, the overall small sample size combined with the unbalanced number of participants for experimental condition imply caution even on superficial interpretations about the effect of the interface. It is nevertheless worth noting how participants felt often *Relieved* or *Satisfied* in the *Mutual-Modeling* condition, but not in the *Self-Centered* or *Partner-Oriented* condition; or that the *Emphatic* feeling was expressed in the *Mutual-Modeling* and *Partner-Oriented* condition, but not in the *Self-Centered*. With a greater number of participants, it would be interesting to perform this kind of stratification in a more systematic way.

### Internal Meta-Analysis on Task Indicators

Taking advantage of the fact that the same task was adopted, under similar conditions, of a previous study [@fritzReinventingWheelEmotional2015], an internal meta-analysis was performed on the point-estimate means for the three dependent variables adopted in the current contribution. The interest of the meta-analyses is two-fold. On the one hand, they provide a better assessment of the point estimates about the performance-based indicators of the use of the EAT. On the other hand, those same indicators will be used as references in a subsequent study in this contribution.

Each meta-analysis has been conducted using the R meta package version `r packageVersion("meta")`, adopting the inverse of the variance weighting mechanism to account for differences in the sample size of the two internal studies. Results both for the fixed and the random models (using the DerSimonian-Laird estimator for $tau^2$) are provided.

#### Expressing Emotions Internal Meta-Analysis

The meta-analysis on the expression of emotions has been conducted on the whole sample size of both studies, since, even for participants in the *Self-Centered* condition, the overall situation in which participants have expressed their emotions are sufficiently close for an internal meta-analytic purpose. Consequently, the sample size are of $N = 16$ in Fritz (2015) and of $N = 35$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-expressing), assess an estimated mean of `r s1.meta_analysis_expressing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$fixed$upper %>% printnum()`] expressed emotions for the fixed effect model, and of `r s1.meta_analysis_expressing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis highlights the presence of considerable heterogeneity in the expression of emotions ($\tau^2$ = 10.29; $\tau$ = 3.21; I\^2 = 82.0% [23.9%; 95.7%]; H = 2.36 [1.15; 4.84]; $\chi^2$ = 5.55 *p* = .018). This may suggest that the different conditions of the two studies may have played a role in inflating the number of emotions expressed in Fritz (2015), where participants were explicitly asked, if possible, to express at least one emotion in each phase of the 4 enigmas (*i.e.* which would amount to 12), whereas in Perrier (2017) they did not receive any guidance. This may be interpreted as a warning about the importance of being careful in framing how the expression of emotional information is prompted, even if the inflation of the number of emotions may not be necessarily accounted by *forced* emotions, that is, emotional episodes that are not *really* felt, but nevertheless reported. It may also be the case that prompting for emotional expression may ease participants into expressing their emotions, something they could be less prone to do otherwise.

```{r s1-forest-expressing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of emotions expressed in the experimental task.", fig.height=2 }
forest.meta(s1.meta_analysis_expressing, hetstat = TRUE, xlim = c(5, 25), layout = "JAMA")
```

#### Information Processing Internal Meta-Analysis

The internal meta-analysis on information processing has been conducted using only the participants retained for the eye-tracking analysis ($N = 14$) in Fritz (2015), and only participants in the *Partner-Oriented* and *Mutual-Modeling* conditions ($N = 23$) in Perrier (2017), for the interface in these situations is identical or at least very similar with respect to the *social* information shared. Results, depicted in Figure \@ref(fig:s1-forest-processing), assess an estimated mean of `r s1.meta_analysis_processing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$fixed$upper %>% printnum()`] total visit duration, in seconds, for the fixed effect model, and of `r s1.meta_analysis_processing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 40.81; $\tau$ = 6.39; I\^2 = 45.2%; H = 1.35; $\chi^2$ = 1.82 *p* = .177), which may indicate that the time spent at looking at emotional information could be determined by a balance between the primary problem-solving activity and the sustaining emotional awareness. The point estimate of total visit duration being around 1 minute over the 20 minutes of the task, it corresponds to a proportion of 5% of the total time.

```{r s1-forest-processing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the time spent at processing emotional information available on screen.", fig.height=2}
forest.meta(s1.meta_analysis_processing, hetstat = TRUE, xlim = c(30, 80), layout = "JAMA")
```

#### Information Seeking Internal Meta-Analysis

With respect to emotional information seeking, the internal meta-analyses comprise the same samples as for information processing, that is $N = 14$ in Fritz (2015) and $N = 23$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-seeking), assess an estimated mean of `r s1.meta_analysis_seeking.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$fixed$upper %>% printnum()`] number of visits for the fixed effect model, and of `r s1.meta_analysis_seeking.summary$random$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$random$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 0; $\tau$ = 0; I\^2 = 0%; H = 1; $\chi^2$ = .56 *p* = .453), which is nevertheless rather due to the huge variability within studies rather than homogeneity between studies. In future studies, the number of transitions between AOI could represent a more informative measure for emotional information seeking.

```{r s1-forest-seeking, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of times emotional information has been visited on screen.", fig.height=2 }
forest.meta(s1.meta_analysis_seeking, hetstat = TRUE, xlim = c(50, 100), layout = "JAMA")
```

## Conclusion

This chapter presented a detailed illustration of an empirical study investigating whether a different use of, and access to emotional information expressed and available through an EAT had an effect on the actual use of the EAT itself. $N=48$ participants, then reduced to $N=35$ following exclusion criteria, were randomly assigned to three different interfaces of the EAT -- namely a *Self-Centered*, a *Partner-Oriented*, and a *Mutual-Modeling* interface -- which varied on how socially-oriented each interface was. The main assumption underlying the empirical investigation stated that the more socially oriented interface would yield a greater use of the EAT in terms of emotion expressed and emotional information seeking and processing. This assumption was not corroborated for the number of emotion expressed, since a detectable difference was not observed between the three conditions; and it was only partially corroborated for emotional information seeking and processing, for which participants in the *Partner-Oriented* and *Mutual-Modeling* conditions sought and processed emotional information more than in the *Self-Centered* condition, but no detectable difference was observed between the two more socially-oriented conditions.

Despite the hypotheses of the study being rejected or only partially corroborated, most of the performance-based indicators about the use of the EAT were congruent with the main assumption. These indicators included the number of emotions expressed through the tool, the number of visits and seconds spent looking at the perceiving-monitoring part of the EAT, as well as the transitions between the more socially-oriented parts of the EAT interface. In most of these cases, the *Mutual-Modeling* interface yielded the greater use of the EAT, followed by the *Partner-Oriented*. Congruently with a inter-personal perspective on emotional expression [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], these results seem to corroborate the usefulness of an EAT as an affordance to share emotion with a partner and take the partner's emotions into account during a computer-mediated collaborative task. Participants seemed to show a genuine interest in seeking and processing emotional information available about the partner, and knowing that their emotions would be conveyed to the partner did not stop participants to express them. The presence of the partner's emotions on the EAT interface also resulted in a more *dynamic* gaze-path, with more transitions between the part of the interface dedicated to the task, and that dedicated to the monitoring-perceiving function of awareness. This fact seems to corroborate the usefulness of providing real-time emotional awareness, since the emotional information may be truly integrated as instrumental information to the task at hand [@buderGroupAwarenessTools2011; @dourishAwarenessCoordinationShared1992]. A word of caution is nevertheless in order, since the method by which transitions have been computed is not externally validated yet. Whether and when it will, transitions could represent a more adequate measure of integrated and dynamic information seeking and processing compared to the *static* number of visits and seconds spent inside an Area of Interest used in the directional hypotheses of the study. All things considering, thus, a moderate optimism is warranted about the usefulness of an EAT during a computer-mediated collaborative task. Despite the fact that it does not directly provide information about the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013], sharing emotions could nevertheless sustain the mutual-modeling activity by which learners build and update a holistic representation of their partner in the collaboration [@dillenbourgSymmetryPartnerModelling2016].

At the same time, participants in the *Self-Centered* condition also seemed to harness the presence of the EAT, which is congruent with an intra-personal usefulness in expressing emotions [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018]. Even though participants in this condition knew beforehand that they were the exclusive sender and receiver of the emotional information, they still expressed emotions as well as sought and processed their own emotional information available on the EAT. This fact suggests that the presence of an EAT may prompt learners to inquiry about their own emotional state, appraising the situation and seeking for the congruent subjective feeling elicited by the circumstances [@boehnerHowEmotionMade2007; @grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009].

### Limitations and Future Development

The present contribution adopted a controlled environment in order to expose every participant to the same stimuli, except for the randomly assigned interface. The use of a simulated partner limited the inter-personal communication that would be normally available in a real collaborative setting. For the purposes of the study, a distinction between cooperation (roughly, doing the same thing with limited interdependence) and collaboration (integrating efforts into a common outcome) was not of primary concern. Nevertheless, this certainly represents a limitation to the generalization of the obtained results to a more articulated communication flow, which could overlap with the emotional information expressed through the EAT. For instance, learners could have used the text area dedicated to their reasoning to inject circumstantial information such as *I don't understand* or *I don't agree with you*, which would have conveyed social, cognitive and emotional information [@derksRoleEmotionComputermediated2008]. A focal question avoided by the present experimental setting is therefore whether participants would still have used the EAT the way they did, were they allowed to convey emotional information through the content space of the joint problem-solving activity. The task at hand, though, can be easily extended to a real collaboration between two participants, and therefore it would be possible to investigate the matter in a way that can be directly related to the results of this contribution. That could be obtained either by a direct replication of the current setting, but using pairs of participants, or by a split design in which part of the participants are randomly assigned to a simulated- or a real-partner. This second option would elicit a better and more reliable comparison, but its interest would be fairly limited to the *validation* of a simulated participant, which was an auxiliary instrument in the study.

A direct replication with real collaboration, on the other hand, would investigate the subject matter more thoroughly, even though the comparison with present results would have to take the difference in time and setting into account. In this regard, a possible solution is to retain a ratio between the duration of the joint-problem solving task and the performance-based measures of the use of the EAT. For example,participants in a dyad that solved the four enigmas in 12 minutes and expressed 18 emotions for one participant and 24 for the second would have a ratio of 1.5 and 2 respectively. Taking into account the temporal dimension would also add another element of interest: investigate whether the ration holds constant across different duration of the joint problem-solving task, or it yields a moderation effect.

Furthermore, the focal question of whether learners would use the EAT having a more thorough channel of communication could also be assessed by allowing learners to display or not the EAT on their screen. This choice may also be instrumental in investigating one of the main assumptions of the thesis, namely the interest for real-time emotional awareness: whether and when participants would decide to display the EAT may convey pivotal evidence about the usefulness of real-time emotional awareness compared, for instance, to a *scripting* strategy in which partners share their emotions at specific intervals outside the task.

### Acknowledgments

The experimental phase of the project has been carried out by Stéphanie Perrier as part of her Master thesis [@perrierCollaborationEnvironnementMediatise2017] that Mireille Bétrancourt and I co-directed.

<!--chapter:end:42-study-1.Rmd-->

# Emotion Awareness in Asynchronous and Individual Settings

```{r s2-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

Sys.setenv(LANG = "en")

# Load the relevant data and graphs from the study-1 folder
source(here("data", "study-2", "s2-export.R"))
```

General introduction to the study:

* Pitfalls in distance learning
* Increasing interest for enhancing social presence as a multi-faceted construct (isolation, belonging, authenticity, affective experience, ...)
* Social function of emotions (affiliation, distanciation, emotiona contagion, social reference in uncertain situations, ...)
* Description of some empirical contributions collecting emotions during distance learning situations (Lavoué and colleagues, 2020, Ruiz and colleagues 2016, ...)

## Study Overview

At the origin, awareness in Computer-Supported Cooperative Working (CSCW) was limited to cues about the presence of other co-workers: whether they were online and on what they were working on [@grudinComputerSupportedCooperativeWork1994; @gutwinDescriptiveFrameworkWorkspace2002; @gutwinWorkspaceAwarenessRealtime1996]. Progressively, awareness has assumed a broader perspective and, especially in computer-mediated collaboration and Computer-Supported Collaborative Learning (CSCL), there is nowadays a consensus about the need to provide -- through awareness tools -- information about others, which is inherently linked to social and relational phenomena [@arguedasOntologyEmotionAwareness2015; @buderGroupAwarenessTools2011; @janssenCoordinatedComputerSupportedCollaborative2013; @janssenGroupAwarenessTools2011]. An Emotion Awareness Tool (EAT) is deeply rooted in this perspective, since the information shared is not directly part of the collaborative task. In other words, using Janssen and Bodemer [-@janssenCoordinatedComputerSupportedCollaborative2013] division between the *content* and the *relation* spaces already mentioned also in the previous chapter, an EAT may be limited to the *relational* space if learners do not make the effort to integrate that information also in the *content* space. This fact may therefore limit the usefulness of Emotional Awareness to enhancing the social presence. That is, emotions shared through the EAT are used as a *proxy*, a strong reminder of the presence of other learners, but without integrating the emotional information into the task at hand. This phenomenon would not be inherently bad, since it is widely accepted that one of the main drawbacks in distance learning is the sentiment of loneliness and isolation [*e.g.*, @carswellDistanceEducationInternet2000; @conradEngagementExcitementAnxiety2002; @jacquinotApprivoiserDistanceSupprimer1993], but it would question whether the result is worth the effort: social presence may be sustained with cues that are closer to the *content space* compared to the dual-task [@pashlerDualtaskInterferenceSimple1994] imposed by the use of an EAT while performing the learning activities. Performance-based indicators of interest in emotional information sharing, seeking and processing -- obtained in the randomized trial with three different interfaces illustrated in the previous chapter -- seem to corroborate a genuine interest in emotional awareness as instrumental to the task at hand. That does not exclude the possibility, though, of an interest in using the EAT exclusively as a form of social presence. Therefore, it is worth investigating the usefulness of an EAT in an asynchronous and individual situation: can it still be useful?

In order to investigate the matter, the adoption of an experimental approach as the one used in the previous study is nevertheless inadequate for at least the following reasons. First, using a task of a relatively short time may fail to produce a sense of belonging to a group, especially in a computer-mediated settings. It is therefore more reliable to measure the usefulness of the EAT in an extended period of time, ideally in order to assess whether the perception of usefulness evolves in time. Second, if collaboration is removed, participants would be sharing emotions with, and seeing emotional episodes of an estranged person, without any connection in time, space or purpose. Thus, the social presence should rather be elicited in a group that has already a *raison d'être*, and whose members share as many elements as possible, even though they are not directly collaborating. Third, in an asynchronous and non collaborative situation, emotions may be elicited by a wide range of elements which are more difficult to pinpoint to something that has happened synchronously. It is thus important that the context in which emotional episodes emerge is as ecological as possible, for them to be representative as a *proxy* of the person.

For these reasons, the present study adopts a longitudinal plan [@fitzmauriceAppliedLongitudinalAnalysis2011] in which the use of the EAT is implemented in an ecological context of distance learning. The Master of Science in Learning and Teaching Technologies (MALTT) at Geneva University provides a blended learning program since more than 20 years. The planning divides each semester in three periods, in which a week of on-site classes is followed by 4-5 weeks of remote learning, during which students are often assigned a small project to submit before the following period begins. The use of the EAT will be implemented in one of the courses of the Master, named *Sciences et Technologies de l'Information et de la Communication I* (STIC I), which covers introductory web programming and computational thinking [@fritzPenseeComputationnelleAvec2019]. Students will use the EAT to express their emotions at any moment while they are working for the STIC I course, a mechanism comparable to the Experience Sample Method [ESM; @csikszentmihalyiValidityReliabilityExperienceSampling2014] or the Ecological Moment Assessment [EMA; @shiffmanEcologicalMomentaryAssessment2008] already used in distance learning situations (*e.g.*, @molinariEMORELOutilReporting2016, but see for instance @scollonExperienceSamplingPromises2003 for a critical assessment of the method). Contrary to some implementation of ESM or EMA, in which there is an external prompt that reminds participants of the recording activity, the use of the EAT is left to the spontaneous initiative of students, who can decide whether and when to use it based on the eliciting events during remote learning [@wheelerSelfRecordingEverydayLife1991].

Compared to the previous study, the emotional information of more than two students will be shown on the perceiving-monitoring part of the interface [@buderGroupAwarenessTools2011; @schmidtProblemAwareness2002]. As a consequence, the way emotional awareness is graphically represented on screen must also be adapted in order to convey a *grouped* representation of a whole class [@bersetVisualisationDonneesRecherche2018; @fritzRealTimeEmotionalAwareness2016]. This technical modification has consequences on a more abstract and theoretical level, since *individual* or *dyadic* emotions are replaced by *group* emotions [*e.g.*, @barsadeRippleEffectsEmotional2002; @keltnerSocialFunctionsEmotions1999; @parkinsonEmotionSocialRelations2005; @smithCanEmotionsBe2007; @vankleefEmotionalCollectivesHow2015; @vankleefEmotionalInfluenceGroups2017]. The use of the EAT may therefore be influenced by collective dynamics. For instance, Cheshin, Rafaeli and Bos [-@cheshinAngerHappinessVirtual2011] found supporting evidence that emotional contagion [@barsadeRippleEffectsEmotional2002; @hatfieldEmotionalContagion1993] can also happen in virtual teams, where computer-mediated communication is exclusively text-based.

In this regard, comparing two classes of the same course in different years may therefore also contribute to investigate to what extent the adoption, use and perception of the usefulness of an EAT is determined *mainly* by individual characteristics or is also influenced by *interactions* between the individual and the group [@boehnerHowEmotionMade2007; @dillenbourgSymmetryPartnerModelling2016]. Even though students would not be randomly assigned to the classes, it is difficult to figure out systematic factors at play, which would determine why particular students would apply in a specific year rather than another, especially in a Master that has always been very heterogeneous in the students' profile.

Finally, whereas the number of emotions expressed can be maintained as an indicator of the use of the EAT, the eye-tracking measures used to detect emotional information seeking and processing cannot be replicated in a longitudinal plan. For this reason, the study also implements a tentative survey aiming at measuring the perceived Emotion Awareness Usefulness (EAU). The survey -- thoroughly depicted in the material section -- combines 7 dimensions retrieved from the literature:

1.  *Frequency*. The frequency of use is a dimension used in different scales pertaining to Human-Computer Interaction and User Experience [@mackenzieHumanComputerInteractionEmpirical2013; @tullisMeasuringUserExperience2013], as it is the case in the System Usability Scale [@brookeSUSQuickDirty1996]. The more frequently a tool is used, the more useful it is perceived, especially when the use is voluntary.
2.  *Affordance*. Affordance in this context broadly refers to the actions available through the EAT and where they take place [@normanDesignEverydayThings2013]. The presence of an EAT may prompt users to share their emotions, something they would not do without the presence of the tool [*e.g.*, @parkinsonEmotionsDirectRemote2008; @rimeEmotionElicitsSocial2009; @vankleefInterpersonalDynamicsEmotion2018].
3.  *Social Presence*. Social presence is a pivotal dimension in remote learning, providing support for learners isolation and feeling of loneliness [*e.g.*, @gunawardenaSocialPresencePredictor1997; @tuRelationshipSocialPresence2002]. Perceiving the emotions of colleagues can help learners to remember there are others in the same condition as they are.
4.  *Self-Understanding*. Having emotions a strong influence on intra-personal functions [*e.g.*, @broschImpactEmotionPerception2013; @levensonIntrapersonalFunctionsEmotion1999; @schererWhatAreEmotions2005], the presence of an EAT may contribute to a better self-assessment of the situation and its consequences on learners' behavior.
5.  *Understanding Others*. The presence of the emotions of colleagues through the EAT may inform learners about what others are experiencing during the remote learning periods, providing information to build and update a mental model of the causes and consequences on their behavior [*e.g.*, @dillenbourgSymmetryPartnerModelling2016; @vankleefEmergingViewEmotion2010; @vankleefEmotionalInfluenceGroups2017; @vankleefInterpersonalDynamicsEmotion2018]
6.  *Self-Other Comparison*. Comparing one's own emotions with that of the colleagues can provide useful information, especially in situation of incertitude [*e.g.*, @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013; @vandevenEnvyAdmirationEmotion2017]. The presence of the EAT may facilitate and prompt this comparison.
7.  *Self-Regulation*. Emotion regulation is a pivotal phenomenon that allows learners to modify their emotional experience using different strategies, such as suppression or reappraisal, in order to maintain instrumental emotional states and modify disruptive ones [*e.g.*, @arguedasAnalyzingHowEmotion2016; @grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014; @jarvenojaRegulationEmotionsSocially2013]. The presence of the EAT, both in terms of learners own emotions and that of the colleagues, may facilitate regulatory processes.

## Research Questions

The present study aims at investigating four main topics. The first consists in the assessment of whether synchronous collaboration is a *necessary* condition for emotional awareness to be perceived as an integrated information into the task. If an EAT is perceived useful even when participants are not directly collaborating, it will rather suggest that emotional awareness is a *sufficient* condition for enacting social presence of others, regardless of the collaborative or individual nature of the task [@mackieCausesConditions1965]. In other words, emotional awareness could act as a *proxy* for students not to feel alone during distance learning and provide social reference as to how colleagues are feelings [@barsadeGroupAffect2015; @hareliEmotionsSignalsNormative2013; @vankleefEmotionalInfluenceGroups2017], as well as enhancing group belonging and cohesion during remote learning [@andersonEmotionalConvergencePeople2003; @keltnerSocialFunctionsEmotions1999; @salasMeasuringTeamCohesion2015; @vankleefEmotionalCollectivesHow2015]. The first research questions (*Q1*) thus investigates what is the use and the perception about the usefulness of emotional awareness in asynchronous and individual learning settings, and whether the use or the perception of usefulness change over time.

The second purpose of the study is to contribute to assess whether the use of an EAT depends *mainly* on the individual characteristics of the students or is *also* determined by the interaction between students using the tool at the same time [@dillenbourgSymmetryPartnerModelling2016]. Taking advantage of the fact that two classes will use the EAT under *more or less* equivalent conditions (same course, same program, same learning environment, ...) -- and assuming no systematic factor at play determining particular type of students in one class compared to the other -- differences in the use or perception of the tool between one class and the other should corroborate the effect of interaction rather than individual characteristics alone. The second research question (*Q2*) therefore investigates whether differences in the use or the perception of usefulness of the EAT can be detected between one class and the other.

The third aim of the study is to assess the perceived usability [@tullisMeasuringUserExperience2013] of an EAT in a longitudinal and asynchronous context. The previous assessment of the tool was conducted in a usability test, with the same fixed time and simulated collaborative task as in the previous chapter (Fritz, 2015). It is therefore worth investigating what is the perceived usability of the EAT in an ecological context.

The forth and last purpose of the study is to assess to what extent the moment-to-moment emotions expressed by learners are then recollected correctly after a period of time, both with respect to their own expressed emotions and that of their colleagues. There is in fact evidence in the literature suggesting that emotional episodes have a privileged access to memory [@broschImpactEmotionPerception2013; @kensingerRetrievalEmotionalEvents2020; @montagrinGoalConducivenessKey2013; @poolAttentionalBiasPositive2015; @rimePartageSocialEmotions2005], and it is therefore worth exploring whether the presence of an EAT contributes in *anchoring* the individual and collective affective experience during distance learning.

No hypothesis is nevertheless posited for any research question, because the setting is too unpredictable. The EAT has never been used before in longitudinal studies, which are by nature more prone to unforeseen events. For instance, it is not even warranted that the EAT will be used in the first place. Furthermore, the measure of emotion awareness usefulness is tentative and non-validated. Consequently, positing hypothesis without pre-registering them would expose any finding to well-founded doubts of questionable research practices [@johnMeasuringPrevalenceQuestionable2012; @makelQuestionableOpenResearch2019], especially Hypothesizing After the Results are Known (HARKing; @kerrHARKingHypothesizingResults1998). All research questions should therefore be interpreted as non-confirmatory [@scheelWhyHypothesisTesters2020].

## Methods

I report how I determined the sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

### Participants

`r s2.participants_aggregated_all %>% nrow()` students (`r (s2.participants_gender_age$gender == "F") %>% sum()` women, `r (s2.participants_gender_age$gender == "M") %>% sum()` men, and 5 missing values) of the course *Sciences et Technologies de l'Information et de la Communication I* [@fritzPenseeComputationnelleAvec2019] in the Master of Science in Learning and Teaching Technologies at Geneva University took part in the study ($M_{age} = `r s2.participants_gender_age$age %>% mean(na.rm = TRUE) %>% printnum()`$, $SD_{age} =`r s2.participants_gender_age$age %>% sd(na.rm = TRUE) %>% printnum()`$ with 7 missing values). Students belonged to two classes that took the one-semester course in two successive years during the period of the thesis (2015-2020): `r table(s2.participants_aggregated_all$group)[1]` students in the first class, and `r table(s2.participants_aggregated_all$group)[2]` students in the second, without overlapping. It is worth noting that a third cohort was originally planned to increase the sample size, but the study has not been implemented because this third cohort had to undergo an abrupt switch to an exclusive distance learning format due to the pandemic. Even though a completely online format would have been even more interesting for the purpose of the present study, the use of the EAT would have added up to an already complicated situation. Furthermore, the cohort would also differ with the other in the way social links could have been created, not disposing of the same weeks with on-site courses.

The use of the EAT was warranted as a pedagogical activity, since the use of technological tools in learning situations is an integral part of the Master's program. Given no participant can be forced to take part to a study, though, students had the choice, at the end of the course, either to sign a consent form allowing data to be used for research purposes, or to write on the same form only the ID they were given at the beginning of the study (without connection to their identity, see procedure below) so that all data associated with that ID could be erased. The overall sample size of the study is therefore determined by the number of students whose ID appears in at least one of the different sources of measure (see below) and has not been retracted via the *non-consent* form.

The population is clearly a convenience sample, but the choice has nevertheless both an explanation and a potential interest over and above the limitations. The explanation is preeminently of a technical nature. The EAT has never been adopted in a longitudinal study, where it must be seamlessly available even in the absence of the experimenter. Since the study implies comparison between two cohorts, it is therefore mandatory that technical problems should be signaled and repaired as soon as possible. In this regard, MALTT students possess the technical know-how to identify and accurately describe malfunctioning in a web application, as well as a quick access to the technical team.

The interest of the convenience sample is, ironically, its potential inconvenience. In fact, if the tool is adopted and considered as useful, results may be biased by the convenience sample, and therefore be taken with more than a grain of salt. On the other hand, if the tool is not adopted and considered of scarce usefulness, then the shortcomings are amplified, since even learners with an interest, habit, and technical know-how would not adopt it.

### Material

#### Configuration of the Emotion Awareness Tool

The Dynamic Emotion Wheel Research Toolbox (DEW-RT) was adopted for both cohorts. The configuration of the Dynamic Emotion Wheel (DEW), depicted in Figure \@ref(fig:s2-dew-interface-figure), is implemented as follows.

For the expressing-displaying function of awareness, the same EATMINT circumplex used in the study depicted in the previous chapter and also in Fritz (2015) was adopted. As a reminder, this affective space comprises 20 emotions organized over the two appraisal dimensions *Valence*, prompted with the question *is the situation pleasant?*, and *Control/Power*, prompted with the question *is the situation under your control?*. Both dimensions were characterized by two opposite poles, labeled *not at all* and *yes absolutely*. The choice and disposition of the subjective feelings is thoroughly depicted in the chapter about the DEW-RT, within the section dedicated to the EATMINT affective space.

For the perceiving-monitoring function of awareness, three *word clouds* were implemented. In a *word cloud*, words are depicted in a font whose size is proportional to the number of occurrences of that world in a specific context, such a text document or a categorization, so that the more frequently used words appear bigger than the less frequently used ones. The perceiving-monitoring interface of the EAT comprised the following elements:

1.  A *Self-Centered* word cloud, depicting the last 50 feelings expressed by the participant herself;
2.  A *Partners-Oriented* word cloud, depicting the last 100 feelings expressed by the other members of the class, that is all the feelings bar that of the participant herself;
3.  A *Collective-Oriented* word cloud, depicting all the emotions expressed by the whole class since the first use of the EAT, that is, the emotions of the participant herself, plus that of the other members.

(ref:s2-dew-interface-caption) Interface of the EAT for a participant, depicting the expressing-displaying and perceiving-monitoring parts of the tool. For the perceiving-monitoring parts, the word clouds refer to the social functions of emotions identified by Fischer and Mastead [-@fischerSocialFunctionsEmotion2016]: affiliation and distancing (details in the text).

```{r s2-dew-interface-figure, fig.cap="(ref:s2-dew-interface-caption)", fig.align="center", out.width="50%"}
include_graphics(here("figure/s2/dew-interface.png"))
```

The combination of the three word clouds sustain the two main social functions of emotions identified by Fischer and Mastead [-@fischerSocialFunctionsEmotion2016]: the affiliation and the distancing function. The top and center word clouds allow participant to compare her own emotions (top) with that of the class (center). In this way, she can position herself with respect to the group (distancing). The bottom word cloud groups all the emotions of the class and is therefore more related to the affiliation function of emotions.

The number of emotions proposed by each word cloud is arbitrary, since there is no previous benchmark about frequency of use in general, and expression in particular. The word cloud also present *a priori* shortcomings with respect to subjective feelings typed directly by participants, since for them to be grouped, they should be written in exactly the same way (*e.g.*, a small typo would isolate that expression). All things considering, though, word clouds are relatively known graphical representation, and convey an immediate and straightforward method of aggregation.

Compared with the interface used in the study illustrated in the previous chapter, thus, the differences concern only the perceiving-monitoring part of the EAT. First, all the emotional information was conveyed through the subjective feelings, with no trace of the cognitive evaluation. This choice is technically justified by the lack, at the time being, of a grouped representation of the appraisal dimensions, since the line charts used in the previous study are limited to individuals. It is also warranted by the fact that, in the usability test conducted on the prototype of the DEW (Fritz, 2015) adopting the very same interface as in the previous study, eye-tracking measures clearly showed that the subjective feelings in the emotion time line were sought more often than the line charts with the appraisal dimensions. Second, no temporal reference appeared at all, since the subjective feelings were categorized based on the frequency alone. This choice is justified by the fact that, in an asynchronous context, the temporal reference conveys limited information, especially considering that there is no manifest link between the time and a specific task or situation involved. Third, except for the participant's own emotions, it was not possible to discern what specific colleague has expressed a particular feeling or cluster of feelings. Once again, this choice is technically imposed by the lack, at the time being, of a grouped representation of feelings, which is able to maintain agency without overcrowding the interface. The choice is nevertheless also a theoretical influence, since in such a setting, the emotions of the colleagues are *truly* at a group level [@barsadeGroupAffect2015; @smithCanEmotionsBe2007; @vankleefEmotionalCollectivesHow2015]. More generally, without a previous benchmark about the number and frequency of emotions expressed in an asynchronous, individual situation, it was also difficult to establish grouping criterion (e.g. group by hour, by day, or by week). The grouped representation of emotions is an open issue that has just started to be investigate [see for instance @bersetVisualisationDonneesRecherche2018].

#### Emotion Awareness Usefulness Rating

According to Boehner and colleagues [-@boehnerHowEmotionMade2007, p. 207], "success of [an affect-aware] system is measured by whether users find the system's responses useful for interpreting, reflecting on, and experiencing their emotions". Based upon this perspective, the study introduces a tentative scale that aims at measuring Emotion Awareness Usefulness (EAU) with respect to 7 dimensions identified in the literature (see also the study overview). The scale comprises 7 items expressing one of the 7 dimensions, for which participants expressed their accord on a scale from 1 (strongly disagree) to 10 (strongly agree) according to the item alone (*i.e.*, with no reference to the dimension):

1.  *Frequency*. I used the tool frequently (e.g. every time I worked for the course).
2.  *Affordance*. The use of the tool prompted me to share my emotions.
3.  *Social Presence*. The use of the tool allowed me to feel less lonely during remote learning periods.
4.  *Self-Understanding*. The use of the tool allowed me to better understand my emotions.
5.  *Understanding Others*. The use of the tool allowed me to better understand the emotions of my colleagues.
6.  *Self-Other Comparison*. The use fo the tool allowed me to compare my emotions with those of my colleagues.
7.  *Self-Regulation*. The use of the tool allowed me to regulate my emotions.

The scale is very straightforward and with only a few items, specifically only one per dimension. This is a potential shortcoming from a reliability and validity standpoint, but, especially in a repeated measure design, brevity is of essence. Once again, the convenience sample of the study provides some leverage on the formulation of items, since MALTT students are, for instance, familiar with terms such as *regulation*, which may be too technical for other populations and therefore would require a more explicit formulation.

#### System Usability Scale

The System Usability Scale (SUS) is a widely adopted scale that measures the usability of a tool [@brookeSUSQuickDirty1996]. It comprises 10 items, usually on a 5-point scale, but that can also be adapted to a 7-point range, which has been the case for this study. The 10 items are as follows:

1.  I think that I would like to use this system frequently
2.  I found the system unnecessarily complex
3.  I thought the system was easy to use
4.  I think that I would need the support of a technical person to be able to use this system
5.  I found the various functions in this system were well integrated
6.  I thought there was too much inconsistency in this system
7.  I would imagine that most people would learn to use this system very quickly
8.  I found the system very cumbersome to use
9.  I felt very confident using the system
10. I needed to learn a lot of things before I could get going with this system

The even items of the scale are reversed, so that a lower evaluation on the item corresponds to a greater perceived usability. The scale uses a system of coefficients that add up to obtain a score between 0 and 100. A more thorough discussion of the scale is available in the next chapter, where the usability score of the SUS will be compared between the usability test (Fritz, 2015) and the score obtained in the present study.

#### Geneva Emotional Competence Test

The Geneva Emotional Competence (GECo) test [@schlegelGenevaEmotionalCompetence2018] is a performance-based test that measures emotional competence as an ability, rather than a trait [@chernissEmotionalIntelligenceClarification2010; @saloveyEmotionalIntelligence1990; @schererComponentialEmotionTheory2007]. The test is primarily concerned with emotions in a workplace, which has relevant congruence with inter-personal dynamics in remote learning. The test is divided in 4 sub-competences tests, namely *emotion recognition*, *emotion understanding*, *emotion regulation*, and *emotion management*.

Emotion recognition determines the ability to infer the corresponding emotional state of a person using video clips of actors. Participants look and hear a professional actor expressing a pre-defined emotion using facial expression and pronouncing pseudo-words with a corresponding vocal prosody. Participants must then choose among different natural language words the one that best describe the displayed emotion.

Emotion understanding determines the ability to infer the corresponding emotional state of a person based on a description of a situation. Participants read the details of an event that occurs to another person and must infer which emotion, among a list of discrete options, has been elicited by that event.

Emotion regulation determines the ability in engaging in adaptive (vs. disruptive) strategies to modify one's own emotional state. Participants read the description of a situation they must imagine has happened to them, which is meant to trigger disruptive emotional episodes and must identify the two appropriate strategies vs. the two inappropriate ones.

Emotion management determines the ability to adopt the better strategy to handle situations eliciting disruptive emotions in others. Participants read a vignette depicting a situation in which they interact with another person. The situation is meant to elicit in that other person a disruptive emotional response such as anger, irritation or misplaced happiness. The participant can then choose between 5 different strategies to manage the emotional response of the other person, among which one is considered to be the more appropriate.

Each sub-test yields a score of accuracy ranging from 0 (low competence) to 1 (high competence). The 4 scores can be combined to obtain an overall emotional competence score.

#### Individual and Class Perceived Frequency of Feelings

Considering the fact that the *word clouds* adopted in the perceiving-monitoring part of the EAT clearly define which feelings have been experienced more frequently than others, the study comprises a survey that asks participants, at the end of the semester, to rate the frequency by which (1) they have felt each of the 20 subjective feelings of the EATMINT circumplex ; and (2) their colleagues have felt each of the same 20 subjective feelings. In the survey, each subjective feeling is presented with a 5-point scale comprising *never*, *seldom*, *sometimes*, *often* and *very often*. Participants were also allowed to skip each particular feeling without rating the frequency both in the individual and the class surveys.

### Procedure

All courses of the Master are organized in three periods per semester, which will defined by P1, P2 and P3. Each period is composed by a week of on-site courses, and 4-5 weeks of remote learning. In order to let students get familiar with distance learning, the use of the tool was integrated only from P2. In this way, students had P1 to get acquainted with the difficulties of distance learning, and could therefore better assess the usefulness of an awareness tool in general, and of an EAT in the specific case of this contribution.

During the on-site course of P2, students were informed that they would be asked to use the EAT as a corollary activity in the course. They were also informed that, beside the pedagogical interest of the activity, the use of the EAT was linked with my thesis and that data could be used for research. The distinction between the participation to the *compulsory* pedagogical activity and the voluntary participation to the research was clearly explained, and students were asked to read a consent form that was linked into the private work-space of the course. They then drew an ID from a urn, which they would use for any interaction with the EAT or with the surveys. With the ID, which was unique for each student, they also received a common code for each class.

#### Expectancy Survey

At this point, each student was asked to fill the *Expectancy* survey in order to collect their perception of the use of an EAT before actually using it. The survey was simply introduced by this description:

> An emotion awareness tool is a tool that allows to share one's own emotions with other people in a computer-mediated context. The tool has two main function: (1) it allows the user to express her own emotions and make them visible to the other users who are using the tool; and (2) it allows the user to perceive the emotions of the other users who have access to the same tool. You will have access to an emotion awareness tool for the two remote periods of the course, so that you can use it while you are working on STIC I: while you are reading the pedagogical material, you are coding the devices for your exercises, or you are contributing to the Wiki.

The questions of the EAU were the same as described in the material above, except that they were transformed in a prospective tense. For instance, *I think I will use this tool frequently* or *I think the tool will prompt me to share my emotions*. A scale from 1 to 10 allowed students to express their agreement which each dimension of the survey.

#### Demo Survey

Once filled the *Expectancy* survey, students were introduced to the EAT through a demo. They discovered the interface of the tool they will use throughout distance periods of the course, and they could directly test the functioning of the tool by expressing emotions, knowing those will not be recorded. The general functioning of the tool (i.e. the use of the appraisal dimensions and the choice of the subjective feeling) was explained. After 5 minutes of practicing with the tool, students took the *Demo* survey, which is exactly the same survey as the *Expectancy*, comprising the dimensions of the EAU in the prospective tense.

#### Set-Up the EAT for Distance Periods

Towards the end of the course, the set-up of the EAT for the actual use was organized. Since it was not possible to combine the EAT on the same interface with the various tools students use as part of the course, a generic web page was therefore the only flexible solution. Students saw how the window could be adapted and put beside another window (e.g. of a software or of another web page) in order to have the EAT close to the task at hand. To ease the access to the EAT, students created a bookmark in their browser that would automatically log them in with their unique ID and the code of the class.

Practically, then, students were supposed to open their browser, click on the bookmark pointing to the EAT, resize the window and place it beside the activity they were performing for the course. Or, alternatively, keep it minimized on their operating system task area and maximize it on recall. In both cases, the use of the EAT required a deliberate action outside the *normal* work-flow of the course.

#### Halfway Survey

During the on-site course of P3, students filled in the *Halfway* survey, after a whole period (i.e. 5 weeks) of use of the tool during remote learning. The survey comprised the EAU dimensions in retrospective tense (*e.g.*, *I have used the tool frequently* or *The tool has prompted me to share my emotions*), as well as an open-ended question in which students could provide additional information about their experience with the tool.

#### Final Survey

Students filled the *final* survey during the first on-site course of a follow-up course (STIC II) in the next semester, that is after 9 weeks from the *halfway* survey. The long period is the result of 5 weeks of *normal* remote learning, interrupted by 2 weeks of Christmas' leave in December (in which students often works, though), and 2 weeks of end-of-semester leave. The EAT was available until the formal end of semester. It has been decided to ask students to fill in the *Final* survey on-site, even with two weeks delay compared to the end of the semester, in order to maximize data collection. The *Final* survey comprised:

-   The EAU survey in retrospective tense (same as *Halfway* survey);
-   The System Usability Scale [@brookeSUSQuickDirty1996], but with a 7-point scale rather than the usual 5-point scale;
-   A survey asking participants to rate the frequency with which they have experienced the 20 subjective feelings belonging to the underlying affective space of the DEW;
-   A survey asking participants to rate the same feelings, but with respect to the frequency with which their colleagues have felt them during the remote periods;
-   An open-ended question in which students could provide additional information about their experience with the tool.

#### Reminders

Reminders to use the EAT during remote learning periods were dispatched twice per periods (P2 and P3) within messages in the private space of the course. The reminders were integrated into wider communications, for instance the feedback of an exercice.

#### Geneva Emotional Competence Questionnaire

In the private workspace of the course, students could find a link to the Geneva Emotional Competence (GECo, @schlegelGenevaEmotionalCompetence2018) test. The presence of the link was reminded at each on-site course, but students were clearly informed that the test was exclusively part of the research, so they were not forced to take it. Students could therefore take the test anytime during the P2 or P3 periods. Considering that there is no evidence yet that the use of an EAT in this context could improve the emotional competence of a person, especially in a performance-based test, this option left more time for students to take a long test during a very active period of learning. Students deciding to take the test had only to provide their anonymous ID in addition to the questions of the test.

### Exclusion Criteria

Having no previous reference for data collection, *a priori* exclusion criteria were difficult to formalize. Since there was the possibility of students dropping out either from the Master or from the research activity (e.g. refusing to fill-in the surveys), participants not having filled both the *Halfway* and the *Final* survey will be considered as if they dropped out and will thus be excluded from data analysis.

## Results

Results are based on $N = `r s2.participants_aggregated %>% nrow()`$ participants, having `r (s2.participants_aggregated_all %>% nrow()) - (s2.participants_aggregated %>% nrow())` students not filled both the *Halfway* and the *Final* survey, and were therefore excluded from the analysis considering they have dropped of the either from the course or the research. Table \@ref(tab:s2-descriptive-table) depicts the number of participants retained for each class across the 4 longitudinal surveys *Expectancy*, *Demo*, *Halfway* and *Final*, for which the two classes have very similar -- if not equal -- sample sizes.

(ref:s2-descriptive-table-caption) Number of participants retained for each class in total, and with respect to the 4 longitudinal surveys.

```{r s2-descriptive-table}
s2.valid_results_description %>% 
  kable(
    col.names = c(NULL, "Class 1", "Class 2"),
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:s2-descriptive-table-caption)\\label{tab:s2-descriptive-table}",
    caption.short = "Study 2: Descriptive statistics",
    longtable = FALSE,
  )
```

### Expressing Emotions

Overall, participants expressed `r s2.participants_aggregated$num_emotions %>% sum()` emotions through the EAT, that is a mean of `r maf.print_m_sd(s2.participants_aggregated$num_emotions, TRUE, TRUE)`. One student in Class 1 and three students in Class 2 did not express any emotions at all. The fact that those students had not expressed emotions, though, does not systematically rule out the fact that they have not used the tool at all: potentially, they could have logged in just to see the emotions of others. Technically, the DEW-RT registers the log for every access, but students were not informed beforehand of this possibility. Thus, data about accesses has not been extracted -- and therefore not used -- and the 4 students are kept in the analysis.

In comparing the two classes, the number of emotions expressed is similar with respect to the central tendency ($M_{class1} = 11.47$ against $M_{class2} = 13.47$), but differs greatly in variation ($SD_{class1} = 21.69$ against $SD_{class2} = 11.79$). The greater SD of Class 1 results in particular by a single participant that expressed 86 emotions. In Class 2, the greatest number of emotions expressed is 34. Taking the median as a more robust reference of comparison, the difference between Class 1 ($Mdn = 3$) and Class 2 ($Mdn = 12$) is much more evident. Figure \@ref(fig:s2-graph-emotions-expressed-boxplot) compares the expression of emotions between the two classes.

(ref:s2-graph-emotions-expressed-boxplot-caption) Boxplot comparing the expression of emotions between the two classes.

```{r s2-graph-emotions-expressed-boxplot, fig.align="center", fig.width=4, fig.height=2.5, fig.cap="(ref:s2-graph-emotions-expressed-boxplot-caption)"}
s2.graph_expressed_emotions_boxplot
```

The use of the tool with respect to the longitudinal duration of the course is depicted in Figure \@ref(fig:s2-graph-expressed-emotions-over-time) The classes have similar profiles in the cumulative number of emotions expressed over time, alternating phases in which they express emotions with periods of pauses.

(ref:s2-graph-expressed-emotions-over-time-caption) Cumulative number of emotions expressed through the EAT

```{r s2-graph-expressed-emotions-over-time, fig.align='center', out.width="80%", fig.cap="(ref:s2-graph-expressed-emotions-over-time-caption)"}
s2.graph_expressed_emotions_over_time
```

Overall, it is safe to assume that Class 2 made a more thorough and homogeneous use of the EAT in expressing emotions compared to Class 1, even though the use in expressing emotions seems to be limited. It is nevertheless worth exploring whether the difference in expressing is also corroborated by differences in the perception of usefulness.

### Emotion Awareness Usefulness

Being the first adoption of the Emotion Awareness Usefulness (EAU) scale, it is worth beginning with some exploratory data analysis aiming at determining to what extent the self-reported rating over the 7 dimensions can contribute to assess (1) whether there is a change over time of the perceived EAU, and (2) whether the two classes differ in the perception of the EAU. In this regard, it is useful to start by evaluating the sensibility of the scale in terms of overall dispersion for each of the 7 underlying dimensions, namely *Frequency*, *Affordance*, *Social Presence*, *Self-Understanding*, *Understanding Others*, *Self-Other Comparison*, and *Self-Regulation*. In Figure \@ref(fig:s2-eda-dimensions-dispersion-graph) every circle represents one of the $N=`r s2.ea_usefulness %>% nrow()`$ ratings, which was made on a scale from 1 to 10, for each dimension across surveys and participants. The dispersion for each of the 7 dimensions spans over the entire range of the rating, suggesting that each dimension has a good sensibility.

(ref:s2-eda-dimensions-dispersion-graph-caption) Overall ratings of each of the 7 EAU dimensions, $N=`r s2.ea_usefulness %>% nrow()`$ ratings. Bars represent 95% confidence intervals around the overall mean.

```{r s2-eda-dimensions-dispersion-graph, fig.cap="(ref:s2-eda-dimensions-dispersion-graph-caption)", out.width="80%", fig.align='center'}
s2.eda.dimensions_dispersion
```

This preliminary assessment therefore corroborates the interest to investigate more specific patterns in the rating of each of the 7 dimensions with respect to the change over time and differences between classes. Table \@ref(tab:s2-description-EAU-stratified) depicts means and standard deviations of the 7 dimensions stratified for both classes across the 4 longitudinal surveys.

(ref:s2-description-EAU-stratified-caption) Means and (standard deviations) of the EAU ratings across longitudinal surveys.

```{r s2-description-EAU-stratified}
s2.ea_usefulness.descriptive_stratified %>%
  select(-dimension) %>% 
kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:s2-description-EAU-stratified-caption)\\label{tab:s2-description-EAU-stratified}",
    caption.short = "Study 2: EAU means and standard deviations",
    longtable = FALSE,
    linesep = c("", "", "\\addlinespace")
  ) %>% 
  kable_styling(
    latex_options = c("repeat_header")
  ) %>% 
  row_spec(c(3, 6, 9, 12, 15, 18, 21), bold = T) %>% 
  pack_rows(
    index = c(
      "Frequency" = 3, 
      "Affordance" = 3, 
      "Social Presence" = 3,
      "Self-Understanding" = 3,
      "Understanding Others" = 3,
      "Self-Other Comparison" = 3,
      "Self-Regulation" = 3
    )
  )

```

In order to assess the presence of effects in the above ratings, a linear mixed-model, also known as multilevel linear models [@finchMultilevelModelingUsing2019; @westLinearMixedModels2015] is fitted with the following components. The dependent variable is the singular value, from 1 to 10, expressed on each dimension of the EAU in the 4 surveys. The fixed covariates are (1) the longitudinal survey with 4 levels (*Expectancy*, *Demo*, *Halfway*, and *Final*); (2) the specific dimension of the EAU with 7 levels (*Frequency*, *Affordance*, *Social Presence*, *Self-Understanding*, *Understanding Others*, *Self-Other Comparison*, and *Self-Regulation*); and (3) the group each student belongs to with 2 conditions (*Class 1* or *Class 2*). All two-way interactions between pairwise covariates, as well as the three-way interaction, are also fitted in the model. The random covariates account for the nested structure of the observations: the repeated measure of the participant is nested inside the class to which the participant belongs.

The use of a linear-mixed model is warranted by a better flexibility compared to repeated-measure ANOVA, especially in case of missing data. Furthermore, linear-mixed models allow to account both for the repeated measures per participant and the nested structure of residuals, with participants potentially influenced by the use of the tool made by the colleagues in their class, which would violate the non-independence of residuals in ordinary least square regression. Finally, it allows to keep each dimension separate rather than averaging over a single score, which will loose an interesting source of variance with respect to the 7 dimensions of the EAU (see @finchMultilevelModelingUsing2019; @mcelreathStatisticalRethinkingBayesian2020; @singmannIntroductionMixedModels2020; @westLinearMixedModels2015 for a more comprehensive overview of linear mixed models compared to ordinary least square regression).

The linear mixed model analyzing the score on the EAU scale was fitted using the mixed function of the Afex [@singmannAfexAnalysisFactorial2020; @singmannIntroductionMixedModels2020] R package version `r packageVersion("afex")`. A Type III analysis of variance of the multilevel linear model detects effects for the longitudinal survey and the dimension of the EAU scale, but not for the group. Two-way interactions were detected between the group and the EAU dimension as well as between the EAU dimension and the longitudinal survey, but not between the group and the longitudinal survey. Finally, a the three-way interaction between group, longitudinal survey and EAU dimension was not detected. Results are depicted in Table \@ref(tab:s2-mlm-eau-table) using Kenward-Roger approximation for computing the *p*-value [@lukeEvaluatingSignificanceLinear2017].

(ref:s2-mlm-eau-caption) Results of a Type III ANOVA on the fitted multilevel linear model using Kenward-Roger approximation for computing the *p*-value

```{r s2-mlm-eau-table}
s2.mlm.ea_usefulness.anova_table %>%
  kable(
    booktabs = TRUE,
    longtable = FALSE,
    digits = 3,
    caption = "(ref:s2-mlm-eau-caption)\\label{tab:s2-mlm-eau-table}",
    caption.short = "Study 2: Emotion Awareness Multilevel Linear Model",
    linesep = c("", "",  "\\addlinespace")
  )
```

Results are graphically depicted in Figure \@ref(fig:s2-graph-dimensions-and-survey), in which the EAU evaluation is stratified by the 7 dimensions of the scale and the 4 longitudinal surveys, as well as grouped by the 2 classes. Data show high *Expectancy* ratings for the *Frequency*. *Affordance*, *Social Presence*, *Understanding Others*, and *Self-Other Comparison* dimensions, but not for *Self-Understanding* and *Self-Regulation*. The ratings are generally maintained even after the *Demo* surveys and tend to decrease with the actual use of the tool in the *Halfway* survey, to remain then more or less stable even in the *Final* survey. Data also show that the two classes basically overlap on all dimensions and across longitudinal surveys. The group per dimension interaction is probably yielded by the *Frequency* and *Affordance* dimensions, for which the two classes differ the most, but it does not seem to play an important role in differentiating EAU ratings. As a consequence, the factor related to the class is dropped in post-hoc contrasts, which will focus only on the difference between the *Final* and the *Expectancy* surveys.

(ref:s2-graph-dimensions-and-survey-caption) EAU rating over longitudinal surveys stratified by dimensions and grouped by class. Bars represents 95% confidence interval and the dashed gray line the median point of the scale.

```{r s2-graph-dimensions-and-survey, fig.align="center", fig.cap="(ref:s2-graph-dimensions-and-survey-caption)"}
s2.ea_usefulness.graph
```

Results of the post-hoc contrasts averaged over group and stratified across EAU dimensions are illustrated in Table \@ref(tab:s2-eau-contrasts-table). Contrasts detect differences between the *Final* and the *Expectancy* surveys in every dimension except the *Self-Understanding* one. All detectable differences highlight a decrease in perceived EAU, with the greatest decrease for *Self-Other Comparison* and *Social Presence* (almost 3 rating points decrease), followed by *Understanding Others* and *Affordance* (more than 2 rating points decrease), and finishing with *Frequency* and *Self-Regulation* (more than 1 rating point decrease).

(ref:s2-eau-contrasts-caption) Contrasts between the Final and the Expectancy surveys for each of the 7 dimensions of the EAU scale averaged over the two classes.

```{r s2-eau-contrasts-table}
s2.mlm.ea_usefulness.comparison %>% 
  as_tibble() %>%
  mutate(
    p.value = round_ps_apa(p.value)
  ) %>% 
  kable(
    digits = 3,
    caption = "(ref:s2-eau-contrasts-caption)\\label{tab:s2-eau-contrasts-table}",
    caption.short = "Study 2: Contrasts between Final and Expectancy surveys per EAU dimension",
    longtable = FALSE,
    booktabs = TRUE
  ) %>% 
  kable_styling(
    latex_options = c("repeat_header")
  )
```

### Perceived Usability of the EAT

$N = `r s2.sus_total %>% nrow()`$ participants filled the System Usability Scale (SUS) survey [@brookeSUSQuickDirty1996] at the end of the semester. The observed score has been of `r maf.print_m_sd(s2.sus_total$sus_score, TRUE, TRUE)`, which is considered *Good* according to the stratification proposed by Bangor, Kortum and Miller [-@bangorDeterminingWhatIndividual2009].

Figure \@ref(fig:s2-sus-items-graph) illustrates the score obtained on each of the 10 items of the SUS for the two classes. The score takes into account the reverse order for even items and is pondered on the 7-point scale used in the present study. As in the case of the EAU, ratings on the SUS are also consistent between classes. More specifically, data clearly show that the first item, inherent to the frequency of use, is far below the other items, which have received overall a high rating. Items 5 (integration of different functions of the system) and 9 (confidence in using the system) also have received lower ratings compared to the other items, even if not as low as the first item about frequency.

(ref:s2-sus-items-graph-caption) Rating of the individual items of the System Usability Scale (SUS) for N = 26 participants.

```{r s2-sus-items-graph, fig.align="center", fig.cap="(ref:s2-sus-items-graph-caption)"}
s2.sus_by_item.graph
```

A more detailed analysis on perceived usability is presented in the next chapter, which will compare the score with the usability test (Fritz, 2015). More accurate benchmarks will also be adopted to assess the individual items of the scale [@lewisItemBenchmarksSystem2018].

### Recollection of Individual and Collective Emotions

In the *Final* survey, that is approximately two weeks after the end of the semester -- and therefore the last day in which the EAT could be used by students during the STIC I course - participants were asked to recall the frequency with which (1) they have experienced the 20 subjective feelings of the underlying affective space as individuals, and (2) their class as a whole has experienced the same 20 subjective feelings. The sparse use in expressing emotions limits the interest in assessing whether there is a recollection of the individual and collective emotions shared with the EAT and is therefore reported here primarily to comply with the disclosure of all the measure collected. Figure \@ref(fig:s2-recollection-feelings-graph) compares three relative frequencies of the 20 subjective feelings of the underlying affective space adopted by the EAT. The first frequency is empirically retrieved using the relative frequency of expression of each subjective feelings compared to all the expressed feelings. The frequencies are mapped so that the most expressed feelings (e.g. *Attentive* and *Interested*) represent the upper bound, and correspond to the *Very often* frequency, whereas the least expressed feelings (e.g. *Surprised* and *Disgusted*) represent the lower bound, and correspond to the *Never* frequency. The second depicted frequency consists in how often participants recalled their colleagues have experienced the specific feelings. It is worth reminding at this purpose that from the interface of the EAT it was not possible to infer which specific colleague expressed a particular emotion, and therefore it was potentially possible to retrieve the frequency from the *word clouds* of the interface. Finally, the third frequency is the self-reported frequency by which participants experienced each subjective feeling themselves. The class and the self frequencies were collected by asking participants to rate the frequency on the same 5-point scale used in the graph (*i.e.*, from never to very often).

(ref:s2-recollection-feelings-graph) Comparison between the observed relative frequency of expressed feelings and the reported frequency recollected for the participants themselves, as well as the frequency attributed to the class as a whole.

```{r s2-recollection-feelings-graph, fig.align='center', fig.cap="(ref:s2-recollection-feelings-graph)"}
s2.perceived_emotions_frequency.graph
```

### Open-Ended Question

In the *Halfway* and *Final* surveys, participants had the possibilities to add general commentaries to the overall questionnaire, with a free text field corresponding to the overarching question *Would you like to add any remark or comment on the study as a whole?*. The open-ended question received 21 answers in the *Halfway* survey, and only 10 in the *Final* survey. Even though the overall approach to the study is nomothetic rather than idiographic, the fact that the tool's adoption has been sparse overall suggests that these questions could provide some useful reasons.

In this regard, most of the answers both in the *Halfway* and *Final* surveys point out that students forgot to use the tool, primarily because they were already overcome by the learning task. A few students, though, precised that the lack of use was also influenced by the presence of alternative communication channels used by the class, as it is clearly stated in this answer:

>At first, I forgot to use it and realized it late in the period. Afterwards, I didn't do it, because I use [the group's messaging platform] Slack to exchange and this tool is sufficient for me to manage my emotions. Using another tool would have generated an additional mobilization of resources and [would] therefore [have been] counterproductive for me. --- Verbatim of the answer, translated from original French.

Other students highlighted the fact that the moment-to-moment use of the tool interfered with their learning activity, diverting their attention. Even if they did manifest their emotion, those were rather prospective or retrospective. Furthermore, the sparse use of the tool even produced the very opposite effect it was intended to obtain, that is, increase social presence. As stated by a student:

>I used the tool and I like the concept but I only found myself online once with my other colleagues which actually made me feel quite alone in my sharing moments. This is the opposite of what I expected because I thought I would feel more connected to my colleagues and my emotions in the moment. --- Verbatim of the answer, translated from original French.

A few students nevertheless provide positive feedbacks about the presence of the tool, even if they regret the sparse use made of it by themselves or their colleagues. The feedback that more closely adheres to the tool overall purpose, though, provide an attentive analysis of its use in terms of the role the appraisal criteria played in its adoption:

>It was a great tool to use. I particularly appreciated to have this tool in order to force myself to analysis [sic] my emotions while working on [the course] topics. [I] [p]articularly appreciated the question about [the] capability to change the situation, which was a good opportunity to take a step back and understand what factors led to the situation and how I could influence them. It was also particularly interesting to be able to see the last emotional states and the most used. [I] [w]ould be happy to continue the survey in the future period. --- Verbatim of the answer, which was provided in English by the student

## Discussion



### (Not So) *Great Expectations*

Learners expectations towards the usefulness of the EAT may be considered the only result of the study, which is not influenced by the sparse use of the tool. As a reminder, in fact, participants were enrolled in a blended course organized over three periods of time. During the first period, students could experience asynchronous and individual learning settings without any particular support. When they filled the *Expectancy* survey, thus, they had a fresh, first-hand experience of the difficulties they faced during the remote period, from which they could project the perceived usefulness of emotional awareness.

The overall expectations of both cohorts suggest a genuine need for a richer experience during remote learning, which is consistent with theoretical and practical efforts in the literature to improve socio-affective support in distance learning [@brackettRULERTheoryDrivenSystemic2019; @jarvenojaRegulationEmotionsSocially2013; @kreijnsSocialAspectsCSCL2013; @lavoueEmotionAwarenessTools2020]. More specifically, learners manifested the highest expectations about the usefulness of an EAT for the *Understanding Others* ($M_{total}=6.93$), and *Self-Other Comparison* ($M_{total}=7.70$), that is, the two more socially-oriented dimensions. Conversely, for the two more self-oriented dimensions, *Self-Understanding* and *Self-Regulation*, expectations were only moderate to low ($M_{total}=4.57$ and $M_{total}=4.23$ respectively). The *Affordance* dimension was rated high ($M_{total}=6.57$), suggesting that the tool is expected to serve its function of prompting learners to share their emotions. Finally, the *Frequency* and *Social Presence* dimensions are more difficult to assess, since they are the two dimensions on which the two classes have a lesser convergence. Frequency of use is notoriously a user-experience dimension that is difficult to foresee: oftentimes it is overestimated, as it is the case here ($M_{total}=5.57$). Finally, the moderate expectations for *Social Presence* ($M_{total}=5.83$), compared with higher expectations for the more socially-oriented dimensions, may suggest that learners consider the EAT as a potential source of social reference rather than a way to perceive the presence of others. This is consistent with theoretical evidence suggesting that emotions in others can be a valuable source of information, especially in challenging situations [@jarvenojaRegulationEmotionsSocially2013; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018].

Inter-dimensional comparison should nevertheless take into account the tentative nature of the EAU scale. Direct comparison between different dimensions, in fact, presupposes that all the seven constructs can be mapped into a similar space [@williamsLevelsMeasurementStatistical2021], which is an assumption that cannot be sustained without previous validation of the scale. Intra-individual change over time on the same dimension, though, is less concerned by the *absolute* measurement space of a construct, for it is determined by the relative shift in perception as the situation evolves [@fitzmauriceAppliedLongitudinalAnalysis2011]. 

In this regard, all the expectations were basically confirmed in the *Demo* survey, once participants had tested the concrete use of the EAT, suggesting that the specific features of the tool neither increased, nor decreased the expectations about its usefulness. This is both good and bad news. On the bright side, learners seem to trust the tool's ability to sustain their expectations. On the other hand, though, the tool failed to change participants' mind on the dimensions rated moderate to low, suggesting that they did not see in the tool additional interest, especially for interpreting, reflecting on, and experiencing their own emotional episodes [@boehnerHowEmotionMade2007]. 

### Limited Impact of Emotion Awareness

Even good expectations, though, did not stand the (longitudinal) test of time. Overall, the EAT has been adopted only by a handful of users throughout the course, with a mean of expressed emotions below 13. This mean is also inflated by a few participants expressing most of the emotions. The limited use of the EAT is corroborated by the Emotion Awareness Usefulness (EAU) ratings, for all dimensions decreased between the *Expectancy* and the *Final* survey, except for the *Self-Understanding* dimension, which was already quite low from the beginning. Participants seem particularly *deceived* by the EAT inability to foster comparison between their own emotions and that of their colleagues, as well as the inability to sustain a social presence, with both dimensions having a decrease of almost 3 rating points on the EAU scale. The incapacity to foster understanding of other emotion also attests a decrease of around two and a half rating points. The three dimensions happen to be the ones more socially-oriented. The accrued *deception*, though, may result simply by an overall floor effect of ratings at the end of the semester. In other words, participants were overall dissatisfied by the holistic experience, and, as a consequence, the dimensions who had the higher expectation ratings suffered more of the overall setback. 

The emotional information did not make any lasting impression on participants either, since self-reported recollection of subjective feelings' frequency of expression both on the individual and collective level was rated in a similar way, that is neutral on the scale from *Never* to *Very often*.

The answers to the open-ended questions proposed halfway and at the end of the semester mainly highlight that students forgot to use the tool, even when they have found it useful. On a superficial analysis, one may infer that the tool is not interesting enough, for it to be remembered. That is, the effort to recall its existence, open, and adopt it outweighs the benefit derived from using it. The course as a whole, though, is very demanding in terms of different environments, tools, and documentation to be consulted [@fritzPenseeComputationnelleAvec2019]. A few students did actually persist in using the tool throughout the remote learning periods, showing at least that the use was possible. 

### Consistency Between *Convenience* Classes

## Corollary Analysis

The chapter proposes two corollary type of analysis that may be of interest for future contributions. The first concerns the assessment of the Emotion Awareness Usefulness (EAU) scale in terms of its structure and reliability. The second explores a provisional link between emotional competence, using the scores obtained through the Geneva Emotional Competence (GECO) test.

### Structure and Reliability of the Emotion Usefulness Scale

Even though the use of multilevel linear model is more adequate for a fine grained assessment of the tentative scale, it is worth exploring whether the Emotion Usefulness scale (EAU) proposes structural features that may be of interest for future use. In this regard, the way the scale has been administered is somehow atypical. On the one hand, participants rated the scale on multiple occasion, which is consistent with the test/re-test paradigm. On the other hand, the conditions in which the scale has been administered where obviously not the same, since the aim of the contribution was to measure the change of perception over time rather then the consistency of the scale. For an exploratory purpose, it could be informative to relax some of the habitual boundaries in reliability measures and *pretend* that each administration of the survey was unique, even when the rater was the same. This choice is warranted by two elements. First, it increases the sample size of measures compared to taking just one of the four survey in which the scale has been administered. Second, if the repeated measure of the administration is taken into consideration, there will be essential overlapping with the multilevel analysis performed above. For these reasons, all $N = `r s2.ea_usefulness_scale.all %>% nrow()`$ administrations of the scale will be considered in the analysis.

Two types of analysis will be conducted to explore the structure and reliability of the EAU. First, it is worth assessing to what extent the scale can be considered as uni-dimensional, that is, measuring a single underlying latent variable, represented in this case by the perceived usefulness of disposing of emotional awareness. Second, it is also worth exploring whether there is an underlying structure of more than one factor.

#### Uni-Dimensionality

For determining the existence of a single, common factor, there is a growing consensus in considering -- despite its extensive use -- that Cronbach's $\alpha$ is not the most adequate choice [see for exemple @hayesUseOmegaRather2020; @revelleReliabilityTutorial2019; @sijtsmaUseMisuseVery2009]. In the R package psych [@revellePsychProceduresPsychological2021], used here in version `r packageVersion("psych")`, Revelle proposes an exploratory index of unidimensionality through the *unidim* function, described in the documentation of the package as follows:

> There are a variety of ways of assessing whether a set of items measures one latent trait. unidim is just one more way. If a one factor model holds in the data, then the factor analytic decomposition F implies that FF' should reproduce the correlations with communalities along the diagonal. In this case, the fit FF' should be identical to the correlation matrix minus the uniquenesses. unidim is just the ratio of these two estimates. The higher it is, the more the evidence for unidimensionality.

Results of fitting the *unidim* approach to the EAU yields a unidimensionality index of `r s2.eau.unidim$uni[1] %>% printnum()`, with a fit of the average correlation to the matrix of `r s2.eau.unidim$uni[2] %>% printnum()`, suggesting the presence of a unidimensional factor. As a comparison, the more traditional Cronbach's coefficient is also promising, with a raw $\alpha$ of `r s2.eau.unidim.alpha$total$raw_alpha %>% printnum()`.

#### Exploratory Factor Analysis

The exploratory factor analysis will be conducted in two steps using the psych R package [@revellePsychProceduresPsychological2021] version `r packageVersion("psych")`. First, a scree test is conducted to determine the number of factors that account for the greatest variance. Then, this number of factors is used in an exploratory factor analysis to retrieve the loading of the seven dimensions of the EAU on the suggested number of factors.

The scree test pictured in Figure \@ref(fig:sc-scree-plot-graph) suggests that three factors account for a fair amount of variance, with small gain achieved by adding a fourth or fifth one.

(ref:sc-scree-plot-caption) Scree plot based on all the EAU surveys administered. $N = `r s2.ea_usefulness_scale.all %>% nrow()`$

```{r sc-scree-plot-graph, fig.align="center", fig.cap="(ref:sc-scree-plot-caption)"}
scree(s2.ea_usefulness_scale.all, pc = FALSE)
```

An exploratory factor analysis with three factors, using the *minimum residuals* method of extraction with the *oblimin* rotation, result in the following loading of dimensions:

1.  The first factor comprises *Social Presence*, *Understanding Others*, and *Self-Other Comparison* dimensions, that is the more *partner(s)-oriented* dimensions.
2.  The second factor includes *Self-Understanding* and *Self-Regulation* dimensions, that is the more *self-centered* dimensions of emotional awareness. The second factor also relates to *Social Presence*, which also loads on the first factor, and *Frequency*, shared with the third factor, but both with a lower coefficient;
3.  The third factor represents *Frequency* and *Affordance* dimensions, that is the more *usability related* dimensions.

The diagram in Figure \@ref(fig:s2-factor-analysis-graph) reports the coefficients of loading. The overall reliability scores are Cronbach's $\alpha =$ `r s2.eau.fa$alpha %>% printnum()`, $\omega_{h}$ = `r s2.eau.fa$omega_h %>% printnum()`, and $\omega_{t}$ = `r s2.eau.fa$omega.tot %>% printnum()`, which suggest good reliability of the scale. Incomplete and tentative, the EAU seems nevertheless a rather promising scale that can be expanded in further research.

(ref:s2-factor-analysis-caption) Graphical representation of the exploratory factor analysis

```{r s2-factor-analysis-graph, fig.align="left", fig.cap="(ref:s2-factor-analysis-caption)"}
omega.diagram(s2.eau.fa, main = "", ,marg=c(5,0,0,0))
```

### Emotion Awareness Usefulness and Emotional Competence

The study also included the possibility to take the Geneva Emotion Competence (GECo) test [@schlegelGenevaEmotionalCompetence2018] anytime before the end of the semester. As a reminder, the GECo is a performance-based test measuring participants' emotional competence on four sub-competences: emotion recognition, emotion understanding, emotion regulation, and emotion management. Given the test was a corollary activity related only to the research -- and that the test is demanding in terms of time and effort due to its accuracy -- only $N = `r s2.geco_score_to_eau.wider %>% nrow()`$ participants performed it. This limits the possibility of exploring relationship between emotional competence and other indicators collected throughout the study beyond a descriptive perspective. In the meantime, participants that did take the test used their precious time during an intense phase of their education, which deserves gratitude and consideration.

Given the overall limited use that has been done of the EAT, the more interesting measure at hand to be related to emotional competence is the prospected usefulness measured in the *Expectancy* survey using the EAU scale. The unidimensionality coefficients exposed above suggest that the average score of the EAU can be, at least in the exploratory context, retained as an indicator of perceived usefulness of emotional awareness and therefore represent the measure of interest.

These are the premises to a multiple regression that has been fit using the average score to the EAU on the *Expectancy* survey as dependent variable, and the score to the four sub-competences of the GECo test as predictors. The aim is to explore to what extent the sub-competences are related to the expected usefulness of an EAT as support to distance learning. The parameters of the multiple regression are depicted in Table \@ref(tab:s2-ec-eau-lm-table), whereas the full model yielded the following results `r apa_print(s2.sub_competences_to_expectancy.lm)$full_result$modelfit`.

(ref:s2-ec-eau-lm-caption) Parameters of the multiple linear regression predicting expected EAU from the GECo emotional sub-comptences. $N = 11$.

```{r s2-ec-eau-lm-table}
apa_print(s2.sub_competences_to_expectancy.lm)$table %>% 
  kable(
    digits = 3,
    caption = "(ref:s2-eau-contrasts-caption)\\label{tab:s2-ec-eau-lm-table}",
    caption.short = "Study 2: Multiple Linear Regression: EC subcomptences predicting EAU",
    longtable = FALSE,
    booktabs = TRUE,
    escape = FALSE
  )
```

As one may expect from the small sample size at hand, there are no detectable effects in the model, with wide confidence intervals around all predictors. Emotion recognition, understanding and regulation have negative estimates, whereas emotion management has a positive estimate, with a lower bound of the confidence interval that is very close to the 0 threshold. These results must obviously be taken with much more than a grain of salt, but if they were to be extended in a similar way, the fact that emotion management is the sub-competence more related to the overall expectation of the usefulness of an EAT would be an interesting result. Emotion management is in fact the subcompetence more oriented towards interpersonal regulation of emotion, that is, modifying the emotional state of others [@reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013]. This phenomenon would therefore be more relevant in a synchronous and collaborative condition, in which there is more leverage in acting upon the emotional episodes of one's partner(s).

## Conclusion

The present observational and longitudinal study aimed at investigating the use and perception of usefulness of an EAT during the distance learning periods, starting from the assumption that the presence of the EAT may provide instrumental information on various dimensions that are considered as pivotal in remote computer-mediated learning environments. Furthermore, by implementing the EAT in two cohorts of the same course, the study also aimed at investigating whether differences between the two classes could be observed, in which case the use and perception of the EAT may depend by interacting dynamics between learners, rather than individual characteristics alone. Finally, as a corollary objective, the study also attempted at determining whether the adoption, use and perception of usefulness of the EAT may be linked to emotional competence as a performance-based construct. 

Overall, the study presents several drawbacks that limited its informative potential. At the same time, it also provided some promising elements, particularly in the material and type of analysis which may help future investigations about the subject at hand. The concluding remarks of this chapters first assess the main limitations of the study, and then resume the elements that may be deployed in future studies.


### Limitations

The study suffered from an unresolved ambiguity between a field study and a pedagogical intervention into an ongoing course. On the one hand, the field study objectives presupposed minimal intervention, letting learners free to adopt and use the tool in the way the saw fit. At the same time, from a pedagogical standpoint, one of the established shortcoming in the implementation of auxiliary technologies in the learning activity is that their presence alone does not guarantee neither the use, nor the effectiveness [@kreijnsIdentifyingPitfallsSocial2003]. The overall sparse use of the EAT, consistent between classes, has therefore diminished the informative potential of the whole study, which proved to be too ambitious in its *group-awareness* perspective. In fact, learners were bestowed with the whole process of reminding to use the tool, open it alongside the different learning activities necessary to comply with the course requirements, and extrapolate meaningful information from displaying their own emotions, as well as monitoring their emotions and that of their colleagues. As many learners have revealed in the free commentary, this was an excessive demand in a course where they already had to face new and complex learning environments or tools. 

A certain amount of *scripting* -- that is, pedagogical scaffolding of the learning activities [@millerScriptingAwarenessTools2015] -- seems therefore necessary. Learners should be more adequately supported in the use of the EAT, for instance with occasional reminder of its presence. Discussions about its use and the available information could also been integrated, for example in the form of focus-groups or experience sharing. These useful elements, though, would have been potential confounders in the comparison between the two classes, since they need external guidance, which is difficult to balance without any previous knowledge about what to expect. Maintaining an equal amount of intervention between the two classes would have been impossible in these conditions. With hindsight, the comparison between cohorts was therefore overly ambitious, for it requires a *laissez-faire* attitude, which is in contrast with learners needs of a more guided scenario and support. This is even more relevant considering the convenience sample of the study, that is, learners that by vocation and training are exposed to various learning technologies. The fact that even learners allegedly open to adopting new technologies made a sparse use of the EAT suggest that there is something flawed in the overall construction of the study.

### Elements to Retain For Future Studies

Despite all the limitations, the study also provides some elements of interest that may be adopted in future studies about emotional awareness in computer-mediated learning environments more generally. 

First, the EAT did not encounter any technical problem. Even though it hasn't been put really under pressure from participants, it has nevertheless been up and running for the two longitudinal studies, apparently collecting all the relevant data provided by students. The intrinsic quality of the tool in itself has not been diminished by the lack of its use, corroborating the idea that the sparse use is mainly due by the design of the study. The perceived usability is in fact consistent across classes and similar to a previous usability test conducted in synchronous and collaborative settings (Fritz, 2015). The EAT's usability will be more thoroughly assessed in the next chapter, but it is worth mentioning here that it appears the tool can be implemented also in an asynchronous and individual use, without modifying the perception of its usability. 

Second, the tentative Emotional Awareness Usefulness (EAU) scale showed promising results with respect to its sensitivity and reliability. The scale nevertheless needs further development, since even the sheer formulation of items considered the vantage point of participant's knowledge in learning technologies. Furthermore, there are at least a few dimensions that are missing and some of the current ones could be decoupled to avoid some ambiguity. For instance, the *Social Presence* item does not take into account the full complexity of the construct as illustrated by the extant literature on the subject. Social isolation is in fact only a facet of social presence. For example, the scale does not include anything about a sense of belonging or authenticity, which are characteristics advocated by several scholars. The *Affordance* dimension suffers from the very same ambiguity that the thesis as a whole tries to dissipate, since it only consider the *self-affordance* in expressing one's own emotions, but does not consider the *social-affordance* in considering others' emotions. The *Understanding Others* and *Self-Other Comparison*, in fact, go a step further and implicitly assume that others' emotions are taken into consideration, whereas this dimension should be indexed in its own term. One can in fact be interested in others' emotions without necessarily acting on their understanding, or do not use the emotions' in others as a comparison, but for other purposes. Furthermore, the scale could also be deplyoed in collaborative settings, in which case *Self-Regulation* should also be supplemented by an item on inter-personal emotion regulation [@reeckSocialRegulationEmotion2016; @zakiInterpersonalEmotionRegulation2013]. Finally, the scale should seek for a better integration with existing scale in the field -- as the scale on social presence proposed by Kreijins and colleagues [-@kreijnsMeasuringPerceivedSocial2011] -- especially in the perspective of having multiple items about the same dimension. Considering the growing interest towards the instrumentality of emotional awareness in computer-mediated learning environments, it would be worth investing in validated measures that take into account the complexity and multi-faceted nature of the construct. 

Speaking of valid instrument, the Geneva Emotional Competence (GECo) test [@schlegelGenevaEmotionalCompetence2018] provides a viable option for researchers interested in in an ability measure of a socio-affective construct. Even if the test is quite long, it provides measurement for  emotion recognition, emotion understanding, emotion regulation, and emotion management. Furthermore, the items used outside emotion recognition are situated in working conditions that have a bearing with learning in higher education. 

### Acknowledgments

I would like to thank professor Daniel K. Schneider for allowing the use of the EAT in two occasions during the STIC I course.

<!--chapter:end:43-study-2.Rmd-->

# Overall and Comparative Assessment of the Emotional Awareness Tool Between Learning Settings

```{r comparison-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

Sys.setenv(LANG = "en")

# Load the relevant data and graphs from the study-comparison folder
source(here("data", "study-comparison", "comparison-export.R"))
```

This chapter provides an overall and comparative assessment about the use and perception of the Emotion Awareness Tool (EAT) using data collected in the two previous empirical chapters of the thesis. In particular, the chapter aims at determining (1) to what extent participants exploited the theory-driven features of the tool, including the emotional structure provided by the underlying affective space (*i.e.*, the EATMINT circumplex adopted in both empirical contributions); and (2) whether there are observable differences in the use and perception of the tool with respect to the prototypical distance learning settings, that is, the Asynchronous and Individual setting (Asynch./Indiv.) of the previous chapter, and the Synchronous and Collaborative setting (Synch./Collab.) of chapter 4. Naturally, the comparison between distance learning settings is not -- and cannot be -- aimed at generalizing the use and perception of the EAT to the specific distance learning condition (*e.g.*, assess whether a particular feeling occurs more often when learners work individually rather than in pairs). This is clearly an overarching assessment that falls way outside the scope of the present contribution. Nonetheless, it is possible to take advantage of the two empirical studies to explore if the multipurpose vocation of the EAT can be sustained in two very different distance learning conditions.

In this regard, the chapter proposes four types of assessment related either to specific features of the EAT or its perception as a whole. First, the chapter explores the use of the two appraisal dimensions Valence and Control/Power that has been done through the two sliders of the Dynamic Emotion Wheel (DEW). Second, it assess the kind and proportion of subjective feelings expressed, either as part of the underlying affective space or not. Third, it evaluates the algorithm dynamically linking appraisal dimensions and subjective feelings in the adopted affective space. And last, it compares the perceived usability of the tool, which comprises the efficacy, efficiency and satisfaction in using the EAT [@brookeSUSQuickDirty1996; @lewisItemBenchmarksSystem2018; @tullisMeasuringUserExperience2013].

## Assessment of the Use of Appraisal Dimensions

The first theory-driven feature of the EAT under scrutiny are the two sliders, which represent the appraisal dimensions through which the eliciting event is evaluated [@schererAppraisalConsideredProcess2001; @schererDynamicArchitectureEmotion2009; @schererWhatAreEmotions2005]. As a reminder, the EATMINT circumplex adopts the Valence and Control/Power appraisal dimensions to prompt the evaluation of the eliciting event. In both empirical contributions, the Valence dimension was prompted with the question *Is the situation pleasant?*, whereas the Control/Power dimension with the question *Is the situation under your control?*. Both dimensions could be rated from a negative pole labeled *Not at all*, corresponding to a score of -100, to the positive pole labeled *Yes, absolutely*, corresponding to a score of 100. Each slider was sensitive to 1-point variation.

### Overall Ratings of the Appraisal Dimensions

One of the interesting indications that can be assessed through the ratings on the appraisal dimensions is to what extent participants could make use of the full range of the slider, that is, whether they discriminate the eliciting events as being more or less pleasant, and more or less under their control. In this regard, Table \@ref(tab:sc-appraisal-descriptive-table) reports the number of participants expressing at least one emotion through the tool, the cumulative number of emotions expressed, as well as the overall mean and standard deviation of the two appraisal dimensions.

(ref:sc-appraisal-descriptive-caption) Descriptive statistics in the rating of the two appraisal dimensions of the affective space

```{r sc-appraisal-descriptive-table}
sc.appraisal.descriptive %>% 
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-appraisal-descriptive-caption)\\label{tab:sc-appraisal-descriptive-table}",
    caption.short = "Comparison: descriptive of appraisal dimensions",
    longtable = FALSE,
  ) %>%
  row_spec(3, bold = T)
```

Results show that for the Valence dimension, the overall mean is almost perfectly neutral for the Asynch./Indiv. setting, whereas it is slightly positive (around 6 points) for the Synch./Collab. setting. In both settings, the rating of the Valence dimension yielded a high standard deviation of around 60 rating points. Data therefore corroborate that participants in both settings took advantage of the full range of the Valence dimension in a very similar way. With respect to the Control/Power dimension, the overall mean for the Asynch./Indiv. setting is slightly positive (around 5 points), whereas it is slightly negative for the Synch./Collab. setting (around -4 points). The standard deviations are also high, but more diverging, with a difference of more than 10 rating points (around 50 for Asynch./Indiv. against around 64 for Synch./Collab.). For this second appraisal dimension, thus, data highlight a slight divergence in central tendency, even though it remains close to the neutral point for both settings, and less variance in the rating of the Control/Power slider in the Asynch./Indiv. setting. 

The descriptive measures are complemented by Figure \@ref(fig:sc-appraisal-density-graph), which shows the density of the two appraisal dimensions for each learning setting. The plots highlight fairly normal and flat distributions (*i.e.*, Leptokurtic-like shapes) around the neutral point for all combinations, except for the Control/Power dimension in the Asnyhc./Indiv. setting, on the top-right plot, which has an higher peek of the distribution (*i.e.*, Platykurtic-like). This higher peek is nevertheless inflated by a single participant who expressed 65 emotions leaving the Control/Power dimension on the neutral point. The distributions also denote some *bumps* on the tails, especially in the positive tail of the Asynch./Indiv. and, to a lesser extent, both tails in the Synch./Collab. setting for the Control/Power dimension. These *bumps* represent ratings in which participants used the extreme poles of the sliders.

All things considering, though, participants in both settings seem to take advantage, individually, of the full range of the appraisal dimensions. Being the two dimensions related, though, the analysis must also consider their joint ratings, which is presented next.

(ref:sc-appraisal-density-caption) Density plots of the two appraisal dimensions' ratings for the each distance learning setting.
```{r sc-appraisal-density-graph, fig.align='center', fig.cap="(ref:sc-appraisal-density-caption)"}
sc.appraisal_density.graph
```

### Joint Ratings of Valence and Control/Power

The second element of interest in the use of the appraisal dimensions is whether their use is independent from one another, in which case the dimensions are truly orthogonal, or if there is a sort of *multicollinearity* due to a high correlation between ratings. In other words, does it happen that participants rate a situation as pleasant, but without feeling control over it, or, conversely, a situation as unpleasant, but feeling control over it? Figure \@ref(fig:sc-appraisal-evaluation-graph) shows two Locally Estimated Scatterplot Smoothing (LOESS) functions [@jacobyLoessNonparametricGraphical2000] -- that is, two non-parametric regressions lines that best fit the data at hand -- applied to the points defined by the Valence appraisal on the x-axis, and the Control/Power appraisal on the y-axis.

(ref:sc-appraisal-evaluation-caption) LOESS functions applied to Valence x Control/Power appraisals in the two distance learning settings.

```{r sc-appraisal-evaluation-graph, fig.align='center', out.width="80%", fig.cap="(ref:sc-appraisal-evaluation-caption)"}
sc.appraisal_evaluation.graph
```

The fitted lines highlight a strong positive correlation between the Valence and the Control/Power ratings, especially in the Synch./Collab. setting, for which the relationship is almost perfectly rectilinear. The ratings of the two appraisal dimensions tend thus to co-vary, so that Valence and Control/Power are both negative or both positive. This phenomenon is corroborated if the expressed emotions are divided in three possible combinations: (1) appraisal dimensions share the same sign; (2) appraisal dimensions are of opposite sign; and (3) either or both appraisal dimensions are on the neutral point 0. Table \@ref(tab:sc-appraisal-combination-table) reports the number of participants that expressed at least one emotion with the appraisal combination, as well as the cumulative number and relative proportion of observations.

(ref:sc-appraisal-combination-caption) Emotions expressed with different combinations of appraisal dimensions.

```{r sc-appraisal-combination-table}
sc.appraisal_sign_comparison %>%
  select(-Setting) %>% 
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-appraisal-combination-caption)\\label{tab:sc-appraisal-combination-table}",
    caption.short = "Comparison: appraisal combinations",
    longtable = FALSE,
  ) %>% 
  pack_rows("Asynch./Indiv. (26 participants)", 1, 3) %>%
  pack_rows("Synch./Collab. (35 participants)", 4, 6) %>%
  kable_styling(
    latex_options = c("repeat_header")
  )
```

For both settings, the same sign combination was expressed at least one time by a greater number of participants, and proportionally more than the other combinations: almost half of the total (0.49) for the Asynch./Indiv. and almost three quarter of the total (0.72) for the Synch./Collab. setting. The opposite sign combination, on the other hand, accounts only for around one fifth of the total both for the Asynch./Indiv. (0.19) and the Synch./Collab. (0.21) settings. Finally, the neutral point was used in greater proportion in the Asynch./Indiv. setting, with almost one third (0.31) of the total, compared to the Synch./Collab. setting with a proportion of around one in twenty (0.06). As in the case of the density plots described above, this score is inflated by a single participant in the Asynch./Indiv. setting who expressed more than 60 emotions leaving the neutral point on the Control/Power dimension.

### Synthesis

All things considering, data suggest that participants take advantage of the full range of each appraisal dimensions individually, but, combined, the two appraisal dimensions are not used as orthogonal. On the contrary, there is strong correlation between the two ratings. This phenomena is consistent in both distance learning settings. To assess whether this is problematic, though, it is necessary to check for the subjective feelings that have been expressed with the appraisal ratings. In fact, it could be the case that participants predominantly expressed subjective feelings that are theoretically characterized by either positive Valence and positive Control/Power, or negative Valence and negative Control/Power, for that would explain the lack of orthogonality. The link between appraisal dimensions and subjective feelings is illustrated below in the chapter.

## Assessment of the Subjective Feelings Expressed

The second assessment concerns the expression of the subjective feeling, that is, the conscious experience of the emotional episode that is usually labeled using natural language words or idioms. [@grandjeanConsciousEmotionalExperience2008]. The assessment primarily aims at determining to what extent the feelings included in the underlying affective space -- that is, the labels that appeared on the buttons or in the dropw-down menu in the expressing-displaying part of the EAT -- met participants' need in terms of representation and differentiation of the conscious experience of the emotion [@barrettKnowingWhatYou2001; @erbasRoleValenceFocus2015]. As a reminder, the EATMINT circumplex proposes 20 subjective feelings, 5 for every combination of the two appraisal dimensions, which have been derived from previous studies [@molinariEmotionFeedbackComputermediated2013; @fritzReinventingWheelEmotional2015]. The 20 subjective feelings are `r combine_words(sort(sc.eatmint_circumplex$label_en))`. If these 20 subjective feelings meet learners' need in best describing their conscious experience, participants should have made a spare use of the possibility to express their feelings with natural language words or idioms falling outside this list. In this regard, it is worth comparing whether the feelings of the underlying affective space are consistent with participants needs in the two distance learning settings. Table \@ref(tab:sc-feelings-cumulative-comparison-table) illustrates the cumulative number of subjective feelings listed or not listed in the underlying affective space.

(ref:sc-feelings-cumulative-comparison-caption) Cumulative number of subjective feelings expressed that were listed or not listed in the underlying affective space

```{r sc-feelings-cumulative-comparison-table}
sc.listed_vs_not_listed %>%
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-feelings-cumulative-comparison-caption)\\label{tab:sc-feelings-cumulative-comparison-table}",
    caption.short = "Comparison: listed vs. not listed feelings",
    longtable = FALSE,
  ) %>%
  row_spec(3, bold = T)
```

Data show that in both settings, participants privileged the subjective feelings included in the EATMINT circumplex with proportions above .80. There is nevertheless a difference between the two conditions of more than ten percentage points, since the proportion of listed feelings for the Asynch./Indiv. setting is around 0.83 against 0.96 in the Synch./Collab. setting. A difference between the two settings is also reinforced by the number of distinct subjective feelings expressed outside the proposed list. In the Asnych./Non-Coll. setting, participants provided 30 distinct subjective feelings, mostly using single words, compared to 12 distinct subjective feelings in the Synch./Collab. setting (the list of the original natural language words or idioms, in french, proposed by each setting are included in the Appendices). These results suggest that, in the Asynch./Indiv. setting, participants may need a richer *emotional vocabulary* to express their conscious emotional experience, even though the 20 subjective feelings proposed by the underlying affective space cover their needs most of the time.

Another aspect worth considering in the assessment of the subjective feelings is whether the relative frequency in expressing the feelings listed in the circumplex varies across distance learning settings. Table \@ref(tab:sc-feelings-frequency-comparison-table) reports the relative frequency of each one of the 20 subjective feelings in the EATMINT circumplex for both distance learning settings, as well as the absolute difference across learning settings. The use of the absolute difference highlights the fact that this comparison does not aim at determining whether participants in one learning setting tend to experience a specific feeling more or less than in the other setting, since the two empirical contributions proposed in the thesis are not fit for this purpose. The aim of the comparison is rather to determine whether the same underlying affective space may adapt to different *needs* in conveying the holistic emotional experience.

(ref:sc-feelings-frequency-comparison-caption) Relative frequencies of the 20 listed subjective feelings and absolute differences between distance learning settings

```{r sc-feelings-frequency-comparison-table}
sc.listed_feelings_frequency %>%
  select(-Quadrant) %>% 
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-feelings-frequency-comparison-caption)\\label{tab:sc-feelings-frequency-comparison-table}",
    caption.short = "Comparison: relative frequency of listed feelings",
    longtable = FALSE,
  ) %>% 
  kable_styling(
    latex_options = c("repeat_header")
  ) %>% pack_rows(
    index = c(
      "Quadrant I. Positive Valence x Positive Control/Power" = 5,
      "Quadrant II. Positive Valence x Negative Control/Power" = 5,
      "Quadrant III. Negative Valence x Negative Control/Power" = 5,
      "Quadrant IV. Negative Valence x Positive Control/Power" = 5
    )
  )
```

Data illustrate roughly three different combinations. First, there are feelings with a high relative frequency in one setting and a low relative frequency in the other (*e.g.*, Amused, Relieved, Surprised or Satisfied). Second, there are feelings with high relative frequencies which are consistent across settings (*e.g.*, Attentive, Interested, Bored or Frustrated). Finally, there are feelings with low relative frequencies in both settings (*e.g.*, Envious, Disgusted, Relaxed, Irritated, Empathic). Results therefore corroborate the assumption that the EATMINT circumplex may adapt to different distance learning settings, even though some of its feelings are seldom chosen by participants. Whether these feelings should continue to be proposed as choices in the affective space is discussed at length in the concluding part of the contribution.

## Assessment of the Link Between Appraisal Dimensions and Subjective Feelings

The two previous sections highlight that, separately, the appraisal dimensions and the subjective feelings composing the affective space seem to adapt to the two distant learning settings. The core of the DEW is nevertheless the theory-driven, computational link between the appraisal dimensions and the subjective feeling. It is therefore pivotal to assess whether the parsimonious model that suggests a subset of subjective feelings given a specific evaluation of the eliciting event according to *Valence* and *Control/Power* eases learners' task in conveying the holistic emotional experience. The link between appraisal dimensions and subjective feeling can be derived only for the emotions that are part of the underlying affective space, and therefore the following analysis will filter out subjective feelings not included in the EATMINT circumplex (see above).

The link between appraisal dimensions and subjective feelings can be assessed mainly in two ways. The first is pragmatic, and pertains to the actual use of the tool with respect to the frequency by which learner's chose one of the subjective feelings proposed in the subset of buttons on the interface, rather than having to recur to the drop-down menu or typing the word themselves. The second is more theoretically-driven and consists in comparing the underlying affective space -- that is, the one *expected* from the theory -- with the *observed* affective space, which can be computed with the means of *Valence* and *Control/Power* every time a given subjective feeling has been chosen by learners.

### Frequency of Choice of the Proposed Subjective Feelings

The pragmatic assessment consists in computing the frequency by which learner's *accepted* to click on one of the three proposed buttons labeled with a subjective feeling, given the evaluation provided through the two sliders representing the appraisal dimensions. In other words, the frequency represent the number of times that learners found one of the suggested subjective feeling as the *right* representation, or *best* approximation, of their holistic emotional experience. The frequency can therefore range from 0 -- that is, the learner never found the corresponding subjective feeling in the buttons and had to provide it through the drop-down menu or text input -- to 1, in which case the learner always *accepted* one of the three suggestions provided by the buttons.

This kind of measure, though, can be influenced, among other things, by (1) the sheer number of emotions expressed, with low numbers inflating either the opposite poles or the central tendency; and (2) individual characteristics such as conformity to accept a suggestion or the dexterity in choosing another feeling from the list. For these reasons, the frequency is computed for each participant that has expressed at least five emotions, that is $N = `r sc.button_vs_other.descriptive[1,2]`$ in the Asynch./Indiv. setting, and $N = `r sc.button_vs_other.descriptive[2,2]`$ in the Synch./Collab. setting. Results are shown in Table \@ref(tab:sc-frequency-button-table).

(ref:sc-frequency-button-caption) Frequency of clicks on one of the suggested subjective feelings

```{r sc-frequency-button-table}
sc.button_vs_other.descriptive %>% 
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-frequency-button-caption)\\label{tab:sc-frequency-button-table}",
    caption.short = "Comparison: frequency of clicks on the buttons",
    longtable = FALSE,
  ) %>%
  row_spec(3, bold = T)
```

Overall, the frequency of clicks on one of the suggested subjective feelings is $M = `r sc.button_vs_other.descriptive[3,3] %>% printnum()`$ ($SD = `r sc.button_vs_other.descriptive[3,4] %>% printnum()`$), which means that around 4 out of 5 emotions are expressed using one of the subjective feelings suggested through the three buttons on the interface. In the Asnyhc./Indiv. setting, the average frequency is $M = `r sc.button_vs_other.descriptive[1,3] %>% printnum()`$ with a standard deviation of $SD = `r sc.button_vs_other.descriptive[1,4] %>% printnum()`$. In the Synch./Collab. setting, the average frequency is $M = `r sc.button_vs_other.descriptive[2,3] %>% printnum()`$, with a standard deviation of $SD = `r sc.button_vs_other.descriptive[2,4] %>% printnum()`$. In both cases, thus, the frequency is well above the 0.5 threshold, which may be considered a random indicator whether the subjective feeling appeared or not in the proposed subset. There is nevertheless a difference of around 0.2 points between the two settings, with participants in the Synch./Collab. setting clicking more often on the buttons. The data, pictured in Figure \@ref(fig:sc-frequency-graph), reveals that most of the participants in the Synch./Collab. setting always *accepted* one of the proposed subjective feelings, whereas in the Asynch./Indiv. setting there is a more heterogenous disposition. This is consistent with evidence in the previous sections of the chapter indicating the need of a more nuanced emotional expression in the Asynch./Indiv. setting.

(ref:sc-frequency-graph-caption) Frequency of click on one of the three buttons labeled with a subjective feeling. Bars represent 95% confidence intervals.

```{r sc-frequency-graph, fig.align='center', out.width="60%", fig.cap="(ref:sc-frequency-graph-caption)"}
sc.button_vs_other.graph
```

Overall, though, the parsimonious computational model fitted into the EAT seems to adequately connect the appraisals dimensions with the subjective feeling: participants took advantage of this feature on average in four out of five emotions expressed. These results seems also to corroborate the limited number of suggestions proposed by the tool. Three buttons, in fact, seem to provide learners with sufficient options to discriminate their feelings. The sheer frequency, though, does not guarantee that this mechanism *works* in the same way at every level of combination between the appraisal dimensions, for which a more detailed analysis is necessary.

### Expected Versus Observed Affective Space

With the data collected every time a subjective feeling is expressed, it is possible to compute an observed position of the feeling on the circumplex. The observed position is computed in two steps. First, the means of the Valence and Control/Power dimensions are calculated for every feeling belonging to the underlying circumplex. For example, every time that the subjective feeling *Attentive* has been expressed, the corresponding ratings that the participant has made of the two appraisal dimensions are pooled to determine the means. Once the mean of Valence and Control/Power are obtained, they are injected into the computational model to retrieve the slope, which will be used to place the feeling on the circumplex. Table \@ref(tab:sc-empirical-feelings-disposition-table) reports the necessary figures to compute the observed slope and compares it with the expected slope, that is the position of the feeling on the EATMINT circumplex. The absolute difference between the two slopes is also provided. The greater the absolute difference, the wider the gap between the *theoretical* position proposed by the underlying affective space and the *empirical* rating made by participants.

(ref:sc-empirical-feelings-disposition-caption) Aggregated means of appraisal ratings for each of the 20 subjective feelings in the EATMINT circumplex, with observed and expected slopes.

```{r sc-empirical-feelings-disposition-table}
sc.empirical_feelings_disposition %>%
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-empirical-feelings-disposition-caption)\\label{tab:sc-empirical-feelings-disposition-table}",
    caption.short = "Comparison: observed appraisal ratings in EATMINT circumplex",
    longtable = FALSE,
    col.names = c("Feeling", "N", "Valence", "Contr./Pow.", "Obs. Slope", "Exp. Slope", "|Slope|")
  ) %>% 
  kable_styling(
    latex_options = c("repeat_header", "HOLD_position")
  ) %>% 
  pack_rows(
    index = c(
      "Quadrant I. Positive Valence x Positive Control/Power" = 5,
      "Quadrant II. Positive Valence x Negative Control/Power" = 5,
      "Quadrant III. Negative Valence x Negative Control/Power" = 5,
      "Quadrant IV. Negative Valence x Positive Control/Power" = 5
    )
  )
```

The results highlight a wide range of differences between the expected and the observed disposition of each feeling, going from almost absolute correspondence for *Bored* (0.22°) to more than a rotation of 90° for *Annoyed* (102.59°). The empirical position of some feelings is computed using only a few observations, such in the case of *Empathetic* (5), *Envious* (1), or *Disgusted* (5); whereas others show a good approximation with many observations, as it is the case for the aforementioned *Bored* (0.22° with 63 observations), *Amused* (7.20° with 94 observations), *Surprised* (5.72° with 35 observations), or *Stressed* (5.11° with 35 observations). The overall disposition of the observed affective space, though, corroborates the lack of orthogonality highlighted earlier in the chapter, with respect to the use of the two appraisal dimensions. In fact, comparing the graphical representations of the theoretical/expected circumplex in Figure \@ref(fig:sc-theoretical-feelings-graph) with the empirical/observed in Figure \@ref(fig:sc-empirical-feelings-graph) confirms that most of the subjective feelings have been chosen with congruent ratings of Valence and Control/Power, that is when both appraisal dimensions are either positive or negative. When the two appraisal dimensions are orthogonal, only *Surprised* and *Empathetic* in the bottom-right quadrant, and *Envious* and *Irritated* in the top-right quadrant -- based on only a few observations, though -- have been chosen with the expected combination of the appraisal dimensions. The other feelings of the orthogonal combination of appraisals moved either in the *upper* or *lower* quadrant depending on the Valence rating. That is, feelings from the Positive Valence x Negative Control/Power quadrant *moved* to the quadrant with both Positive appraisal dimensions; whereas feelings from the Negative Valence x Positive Control/Power *moved* to the quadrant with both Negative appraisal dimensions.

(ref:sc-theoretical-feelings-graph-caption) The theoretical/expected disposition of the EATMINT circumplex.

```{r sc-theoretical-feelings-graph, fig.align='center', out.width="100%", fig.height=9, fig.cap="(ref:sc-theoretical-feelings-graph-caption)", fig.pos="p"}
sc.theoretical_feelings_disposition_circumplex.graph
```

(ref:sc-empirical-feelings-graph-caption) The empirical/observed disposition of the feelings according to the actual rate of participants.

```{r sc-empirical-feelings-graph, fig.align='center', out.width="100%", fig.height=9, fig.cap="(ref:sc-empirical-feelings-graph-caption)", fig.pos="p"}
sc.empirical_feelings_disposition_circumplex.graph
```

The phenomenon is consistent in both learning settings, with nevertheless some variations. Figure \@ref(fig:sc-empirical-feelings-comparison-graph) shows the disposition of feelings with respect to the mean Valence and Control/Power, therefore in a Cartesian plane rather than in a circumplex. The choice of a different format, though, is only dictated by the need to simplify the display of the information, minimizing overlapping; there is thus no change in the underlying algorithm. As Figure \@ref(fig:sc-empirical-feelings-comparison-graph) shows, the overall tendency of co-variation in the two appraisal dimensions permains. Nevertheless, in the Synch./Collab. setting, there is more consistency in the bottom-right quadrant, the one charactherized by Positive Valence x Negative Control. Three out of five feelings appears in the expected quadrant, and a fourth one -- *Relieved* -- is close to the edge with the top-right quadrant.

(ref:sc-empirical-feelings-comparison-graph-caption) Comparing the empirical disposition of the two learning settings.
```{r sc-empirical-feelings-comparison-graph, fig.align='center', out.width="100%", fig.height=9, fig.cap="(ref:sc-empirical-feelings-comparison-graph-caption)", fig.pos="p"}
sc.empirical_space_comparison.graph
```

### Synthesis

The assessment of the link between the appraisal dimensions and the subjective feeling yielded mixed, but overall promising, results. On the one hand, the overall *accuracy* of the dynamic algorithm was around 0.8, which means that four out of five subjective feelings expressed by participants were included in the three buttons suggested on the interface. The accuracy was lower in the Asynch./Indiv. setting, though, which may suggest the need of a more nuanced expression in these conditions.

On the other hand, data clearly confirm a major issue with the Control/Power appraisal dimension, which has a high correlation with the Valence dimension, a phenomenon already observed in the usability test of the DEW (Fritz, 2015). The problem, though, does not seem to be unique to the tool. On the contrary, Scherer and Fontaine [-@schererSemanticStructureEmotion2018] encountered a similar difficulty with the use of the GRID instrument [@fontaineComponentsEmotionalMeaning2013], which, as a reminder, consists in a questionnaire comprising 142 features related to the different components of an emotion (*i.e.*, appraisal, bodily symptoms, expression, action tendencies, and subjective experience). Scherer and Fontaine [-@schererSemanticStructureEmotion2018], in an attept to investigate whether the dimensions of the feeling component could be predicted by the other components, among which the appraisal, failed to make emerge in the results the importance of the Control/Power dimension. In this regard, the authors note (*ibid.*, p. 9) :

> It is exceedingly difficult to construct items that allow one to obtain valid assessments of control/power/coping appraisals, partly because of the strong relationship to valence (it is good to have high power).

As advocated by the authors, this problem requires future studies to find better solution. In this regard, some options pertaining to the DEW will be provided in the general discussion of the thesis.

## Assessment of the Perceived Usability of the Tool

Finally, the EAT will be appraised with respect to its usability, that is the perceived efficiency, effectiveness and satisfaction in using the tool. A usability measure, the System Usability Scale [SUS, @brookeSUSQuickDirty1996], was administered in the Asynch./Indiv. empirical contribution, but not in the Synch./Collab. The SUS was nevertheless administered in the usability test in Fritz (2015), which used the same configuration and task of the Synch./Collab. setting (but without the experimental conditions). The SUS scores of the usability test ($N = 16$) will therefore be used for the Synch./Collab. setting, alongside the scores obtained in the Asynch./Indiv. setting of the present contribution ($N = 26$). 

The scale, using inverse rating for even items, allows to compute an overall score ranging from 0 (very poor perceived usability) to 100 (excellent perceived usability). Results of the SUS have been collected in the last decades in various published and unpublished reports. Thus, there is nowadays the possibility to better assess the overall score of the SUS, as well as of each of its ten items [@bangorDeterminingWhatIndividual2009; @lewisItemBenchmarksSystem2018; @sauroQuantifyingUserExperience2016]. 

Concerning the overall score of the SUS, Sauro and Lewis [-@sauroQuantifyingUserExperience2016] extrapolated a curved grading scale from 241 industrial usability studies and surveys. According to this scale, the average SUS overall score is $M = 68$, but the same authors suggest that "it is becoming a common industrial goal to achieve a SUS of 80" [@lewisItemBenchmarksSystem2018, p. 161] as synonymous of a perceived good experience. 

Table \@ref(tab:sc-sus-overall-score-table) shows the SUS score for the two learning settings, as well as the weighted overall mean. With an overall score of $M = `r sc.sus_comparison$mean[3] %>% printnum()`$ ($SD = `r sc.sus_comparison$sd[3] %>% printnum()`$), the EAT is perceived somehow halfway between the $M = 68$ empirical benchmark, and the target score of 80. As the table shows, the SUS score is consistent across the two learning settings, with a difference of less than 1 point.

(ref:sc-sus-overall-score-caption) Score to the System Usability Scale [SUS, @brookeSUSQuickDirty1996]
```{r sc-sus-overall-score-table}
sc.sus_comparison %>% 
  kable(
    booktabs = TRUE,
    digits = 2,
    caption = "(ref:sc-sus-overall-score-caption)\\label{tab:sc-sus-overall-score-table}",
    caption.short = "Comparison: overall score to the SUS",
    longtable = FALSE,
    col.names = c("Condition", "N", "M", "SD")
  ) %>%
  row_spec(3, bold = T) %>% 
  kable_styling(latex_options = "HOLD_position")
```


Furthermore, Lewis and Sauro [-@lewisItemBenchmarksSystem2018] collected data from 166 unpublished industrial usability studies or surveys, each one comprising a mean of the SUS overall score. The 166 means were computed from a total of 11'855 surveys. From these data, the authors retrieved benchmarks for each of the ten items of the SUS to reach the $M = 68$ empirical mean, or the target score of 80. As a reminder, the SUS items are the following:

1. I think that I would like to use this system frequently
2. I found the system unnecessarily complex
3. I thought the system was easy to use
4. I think that I would need the support of a technical person to be able to use this system
5. I found the various functions in this system were well integrated
6. I thought there was too much inconsistency in this system
7. I would imagine that most people would learn to use this system very quickly
8. I found the system very cumbersome to use
9. I felt very confident using the system
10. I needed to learn a lot of things before I could get going with this system

Figure \@ref(fig:sc-sus-items-graph) depicts the score of each of the SUS items across learning settings. To ease the comparison, even items have already been reversed, so that for each item a higher score equals a higher perceived usability. The horizontal lines represent Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks to reach the target score of 80. The original benchmarks refers to the 1-to-5 rating and have therefore been multiplied by a factor of 1.4 to map to the 1-to-7 scale used in both administrations of the SUS. Also the benchmarks have already been reversed for even items to ease the comparison.

(ref:sc-sus-items-graph-caption) SUS scores on the single items, with the horizontal lines representing Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks to reach the target score of 80, transformed to a 1-to-7 scale. Both items and benchmarks have already been reversed for even items. Bars represent 95% confidence intervals.
```{r sc-sus-items-graph, fig.align='center', out.width="100%", fig.cap="(ref:sc-sus-items-graph-caption)", fig.pos="H"}
sc.sus_items.graph
```

Data show substantial consistency across learning settings for most of the items, with differences in items SUS1 (frequency), SUS3 (ease of use), and SUS8 (intuition). The first item in particular is the one yielding the bigger discrepancy, with participants in the Synch./Collab. setting reporting a perspective use more frequent than the Asynch./Indiv. setting, which is consistent with the observed use, for instance in terms of number of emotions expressed. All in all, though, the EAT seems to possess an *intrinsic* perceived usability that holds -- for good and for bad -- in both distance learning settings.

Comparing the single items to Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks highlights that half of the items are on-or-above the target, and half are below. The fact that all the odd items are below, and all even items are on-or-above the target may seem peculiar, but is consistent with the pattern of the benchmarks which are more demanding for odd items. Lewis and Sauro [-@lewisItemBenchmarksSystem2018] do not mention anything specific about this pattern, but implicitly exclude it could be determined by the negative formulation of odd items, since previous findings collected by the same authors [@sauroWhenDesigningUsability2011] seem to suggest that there is negligible impact of the negative formulation compared to a positive transformation of the odd items.

According to the benchmarks, the EAT performs particularly bad in the frequency of use (SUS1) and in the confidence in the system (SUS9), which are two important dimensions of the scale. The integration of the different parts of the system (SUS5) also does not seem to convince learners, whereas ease of use (SUS3) and quickness of adoption (SUS7) remain below target, but less critically so. On the bright side, the EAT performs very well in learnability (SUS10), and well in consistency (SUS6). Furthermore, simplicity (SUS2), autonomy (SUS4), and intuition (SUS8) are aligned with the target score of 80. 

Feidakis and colleagues [-@feidakisProvidingEmotionAwareness2014] also used the SUS score to assess the usability of an emotion awareness tool, based on a valence x activation circumplex, on which 14 discrete emotional states are positioned. The overall SUS score they obtained on $N = 29$ is $M = 67.81$ ($SD$ not provided), thus roughly in line with the score obtained for the DEW. Contrary to the present results, though, learners in Feidakis and colleagues (*ibid*) reported a higher score on the item about frequency (SUS1).

The implications of the usability evaluation of the EAT will be integrated in the general discussion of the thesis, since they provide useful information that can be linked to broader aspects about emotional awareness and the tool more specifically. A preliminary synthesis nevertheless highlights that the overall perception of the usability of the EAT is fairly good, especially considering the lack of previous experience with this type of device. On the other hand, there are also critical indications, such as the prospected frequency of use or the confidence in the system, which must be carefully considered.

## Conclusion

This chapter proposed an overall assessment of the use and perception of the EAT, as well as a comparison between the use in an asynchronous and individual distance learning environment, versus a synchronous and collaborative one. The primary purpose of the chapter was to determine to what extent the EAT meets learners' need, in particular by evaluating whether learners' take full advantage of the emotional structure implemented into the tool in expressing their emotional states. Results of the overall and comparative assessment provide mixed evidence. 

On the one hand, the tool and the underlying EATMINT affective space seem to adapt fairly well to different distance learning settings. For instance, the 20 subjective feelings proposed in the circumplexes seem to be sufficient to convey most of learners' emotional experiences. Furthermore, the algorithm linking the two appraisal dimensions with the proposed subjective feelings provided a good accuracy, consistent across learning settings. These indicators suggest that the EAT possesses a sort of *intrinsic* value, which may be generalized to different contexts. This does not mean, though, that the EAT will be perceived as useful regardless of other determinants, such as the task at hand or the overall instructional design. For instance, the comparison suggest that learners' may need a more nuanced emotional expression in an asynchronous and individual setting.

On the other hand, results also corroborate and extend some problems already emerged in the first test of the tool (Fritz, 2015), such as the lack of orthogonality of the two appraisal dimensions Valence and Control/Power, or issues with the prospected frequency of use and the confidence in the system. The issues, though, do not seem limited to technical elements of the EAT, but rather extend to more fundamental aspects of emotional awareness or the difficulty in determining an emotional structure that can be leveraged by users. As a consequence, they need a thorough and integrated perspective, which is provided next in the general discussion of the thesis.

<!--chapter:end:44-study-comparison.Rmd-->

# (PART\*) Concluding Remarques {.unnumbered}

<!--chapter:end:50-epilogue.Rmd-->

# General Discussion

* Things should be simple, but not simpler
* Can an EAT really be multi-purpose
* Tenacity vs. obstination (emotional structure vs. use)
* Emotional differentiation vs. frequency
* Research vs. practice

<!--chapter:end:51-discussion.Rmd-->

# Future Directions

<!--chapter:end:52-future-directions.Rmd-->

# Conclusion {.unnumbered}

<!--chapter:end:53-conclusion.Rmd-->

# (PART\*) Appendix {-} 

```{r s1-setup-for-appendices, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(pander)
library(kableExtra)

# Load the relevant data and graphs from the study-1 and study-2 folders
source(here("data", "study-1", "s1-export.R"))
source(here("data", "study-2", "s2-export.R"))
```

# Study 1 - Synch./Coll.

## Transitions Between AOI
(ref:s1-transitions-means-caption) Summary of the number of transitions aggregated per participant, stratified by experimental condition and path.
```{r s1-transitions-means-table}
s1.transitions.descriptive %>%
  as_tibble() %>%
  select(-transition) %>%
  kable(
    digits = 2,
    caption.short = "Study 1: Pairwise means of transitions between AOI",
    longtable = FALSE,
    booktabs = TRUE,
    linesep = c("", "", "\\addlinespace"),
    caption = "(ref:s1-transitions-means-caption)\\label{tab:s1-transitions-means-table}",
  ) %>%
  kable_styling(
    latex_options = c("repeat_header")
  ) %>%
  pack_rows("Expressing to Perceiving", 1, 3) %>%
  pack_rows("Perceiving to Expressing", 4, 6) %>%
  pack_rows("Expressing to Task", 7, 9) %>%
  pack_rows("Task to Expressing", 10, 12) %>%
  pack_rows("Perceiving to Task", 13, 15) %>%
  pack_rows("Task to Perceiving", 16, 18)
```

<!--chapter:end:90-appendices.Rmd-->

<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# (PART\*) References {-}
<!--
This manually sets the header for this unnumbered chapter.
-->
\markboth{References}{References}
<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...

<!--chapter:end:99-references.Rmd-->

