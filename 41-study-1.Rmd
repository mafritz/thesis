---
bibliography: references.bib
---

# (PART) Empirical Contribution {.unnumbered}

# Does a Different Use of and Access to Emotional Information Change the Concrete Use of the Emotion Awareness Tool? {#study-1}

```{r s1-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(see)
library(here)
library(knitr)
library(kableExtra)

# Load the relevant data and graphs from the study-1 folder
source(here("data", "study-1", "s1-export.R"), local = knitr::knit_global(), encoding = "UTF-8")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

This chapter describes the first empirical contribution of the thesis, which investigates to what extent a different use of, and access to emotional information can determine the concrete use of an EAT in synchronous and collaborative computer-mediated settings. The presentation of the experiment follows the traditional structure of empirical contributions [@sollaciIntroductionMethodsResults2004], but without the general theoretical background already presented in Part I.

## Study Rationale

In the few experimental contributions that have investigated emotional awareness in synchronous computer-mediated collaboration so far [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013], the experimental design consisted in comparing a control group, who did not dispose of emotional awareness, with a *treatment* group, who disposed of emotional awareness in *full strength*. That is, either the person had no emotional awareness at all, or she had access to the full *abstract model* depicted in Section \@ref(abstract-model-of-ea): intra-individual and inter-individual awareness, as well as the comparison between the two. The *treatment* setting is therefore consistent with the mutual-modeling perspective [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009], according to which the symmetry of the information available to learners is instrumental to build and update a holistic representation of the partner, upon which the collaborative effort can strive.

As the abstract model has shown, though, emotional awareness can be broken down to a series of passages that, even though tightly related, may have different reasons for, and consequences on the use of an EAT. At the very least, it is possible to identify the intra-personal and the inter-personal *paths*, which are underpinned by two different assumptions about the instrumentality of emotional awareness (see Sections \@ref(ea-intra-personal) and \@ref(ea-inter-personal)). In this regard, for instance, @molinariEmotionFeedbackComputermediated2013 reckon that "one main limitation of [their *control vs. treatment*] study is the difficulty in disentangling the effect of reflecting upon one's own emotions from the effect of sharing one's emotions with the partner" [@molinariEmotionFeedbackComputermediated2013, p.342]. As a consequence, it may be worth investigating the use of different configurations of an EAT, which vary, for instance, according to (1) the use of the emotional information that is produced, and (2) the access to the emotional information that may be perused through the tool. As suggested by @buderGroupAwarenessTools2011, varying the characteristics within an awareness tool may complement the assessment of its specific contribution alongside the all-or-nothing approach of the *control vs. treatment* design.

One way to guide the configuration of different versions of an EAT is to use the abstract model of the functions of emotional awareness and imagine it is a sort of *pipeline* in which the *flow* of emotional awareness circulates. Each passage from one main concept -- the learning task, the intra-indiviudal or inter-individual emotion, and the meaning-making extrapolated from emotional information -- can be seen as a pipe, whose flow can be blocked, decreased, or increased according to the configuration of the EAT. Figure \@ref(fig:conditions-ea-model) shows this concept graphically, with three different configurations of the *pipeline*, identified respectively as Self-Centered, Partner-Oriented, and Mutual-Modeling.

(ref:conditions-ea-model-caption) Comparison of three different *pipelines* in the abstract model of emotional awareness: the Self-Centered, the Partner-Oriented, and the Mutual-Modeling *paths*. The *flow* in a pipe can be blocked (x), decreased (-), or increased (+), depending on the configuration of the the EAT's interface.

```{r conditions-ea-model}
#| fig.align="center", 
#| fig.cap="(ref:conditions-ea-model-caption)",
#| fig.scap = "Study 1: Manipulation of the abstract model to create experimental conditions", 
#| out.width="30%", 
#| fig.show="hold"
knitr::include_graphics(
  c(
    here::here("./figure/s1/self-centered-model.png"),
    here::here("./figure/s1/partner-oriented-model.png"),
    here::here("./figure/s1/mutual-modeling-model.png")
  )
)
```

In the Self-Centered *path*, all the passages related to the inter-personal emotion edge are blocked, whether incoming or outgoing. Conceptually, this means that the learner is provided only with emotional self-awareness, since the emotion expressed by each participant to the collaborative task remain *private*: the emotional information is not disclosed to others, and therefore each learner peruse only the emotional information she has produced.

In the Partner-Oriented *path*, all passages remain open, but the *flow* is reduced in passages 2 (from intra-personal emotion to meaning-making) and 6 (from inter-personal to intra-personal emotion). Conceptually, this means that the emotion expressed by the learner are disclosed to the partner, but they do not persist on the interface of the learner herself as a means to foster intra-individual meaning-making. The only available emotional information to peruse for the learner consists in the emotions of the partner. This entails that direct and persistent comparison between the partner's emotions and that of the learner is considerably reduced: it would depend only on the learner memory of her emotional experience over time.

Finally, in the Mutual-Modeling *path*, all the passages remain open, and the *flow* between the intra-personal and inter-personal emotion edges is *increased* by the direct and persistent comparison between the emotional experience of the learner herself and that of the partner. Conceptually, this entails a total symmetry between the emotional information available to both partners, which is consistent with the mutual-modeling activity through which the holistic representation of the partners is built and updated [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009].

The analogy of the pipeline is also relevant to address two important issues in the study of emotional awareness when the inter-personal perspective is implicated. First, the overall *flow* is influenced by how much information is produced by each learner. That is, the more emotions are shared by the learner, the more the partner will dispose of emotional information to peruse. At the same time, the more emotions learner share respectively, the more the symmetry will be quantitatively and qualitatively achieved. It could in fact be the case that even if learners dispose of the Mutual-Modeling interface, one shares a lot of emotions and the other none at all. Second, as indicated by @janssenCoordinatedComputerSupportedCollaborative2013, the content and the relational space of a computer-mediated collaboration overlap (see Figure \@ref(fig:janssen-bodemer-framework) in Section \@ref(mutual-modeling)). In other words, it is possible to infer emotional information that comes from outside the EAT, and also produce emotional information using other sources than the EAT. For instance, one can infer that a colleague who is not writing in a shared document, but who follows with the cursor the new lines added, is probably uneasy about how to contribute. Thus, if one is interested in the investigation of the specific contribution of an EAT, other sources of emotion-related information shall be accounted for or controlled in some way.

In order to address both issues, the contribution will therefore adopt a simulated collaborative setting as the one used in the usability test of the Dynamic Emotion Wheel illustrated in Section \@ref(dew-ux-test). This will expose all participants to the same *content* of the collaboration, whereas the *relational* information will be different with respect to the emotional information available [@janssenCoordinatedComputerSupportedCollaborative2013].

## Research Question and Hypothesis

The main aim of the experiment is therefore to manipulate the abstract model of the functions of emotional awareness and determine what does it happen -- holding all other sources of potential variation in the emotional information constant -- if the *flow* of emotional information is blocked, reduced, or increased in specific parts of the model. Depending on the use that would be made of the emotions expressed, as well as to what kind of emotional information learners have access to, there are different reasons to *encode* and *decode* emotional information into and from the EAT. This is all the more relevant with a self-reporting and moment-to-moment use of an EAT. As stated by the Component Process Model [@schererWhatAreEmotions2005; @scherer2019; @schererDynamicArchitectureEmotion2009] for the intra-individual level, and by the Emotion As Social Information model [@vankleefEmergingViewEmotion2010; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018] for the inter-individual, producing and consuming emotional information requires a cognitive effort when the process happens at the conceptual and inferential levels. This effort must be evaluated according to the trade-off between the cost of producing or perusing the emotional information on the one hand, and the benefit of disposing or making the others dispose of emotional awareness as instrumental information to the task at hand [@pashlerDualtaskInterferenceSimple1994; @buderGroupAwarenessTools2011; @dillenbourgSymmetryPartnerModelling2016].

In this regard, one way to look at the three different *flows* of emotional awareness is to consider that, starting from the Self-Centered *flow* and ending with the *Mutual-Modeling* flow, there is on every step an increment on how socially oriented the interface is. Figure \@ref(fig:affective-processes-per-condition) shows this mechanism graphically by mobilizing concepts illustrated in Chapter \@ref(eat-general-chapter).

(ref:affective-processes-per-condition-caption) Theoretical concepts mobilized by versions of the EAT differing in the use of, and access to emotional information. Concepts are listed in alphabetic order.

```{r affective-processes-per-condition}
#| fig.align="center", 
#| fig.cap="(ref:affective-processes-per-condition-caption)", 
#| fig.scap = "Study 1: Different emotion-related concepts mobilized by the experimental conditions",
#| out.width="60%"
knitr::include_graphics(
  here::here("figure/s1/s1-theory-diagram.png")
)
```

In the Self-Centered condition, learners will be oriented towards their own emotional experience [@molinariEMORELOutilReporting2016; @lavoueEmotionAwarenessTools2020]. This translates, for instance, in an accurate appraisal of the situation [@scherer2021], which is nevertheless not influenced by the need to reflect on a strategic signal to others or normative pressure [@schererComponentialEmotionTheory2007; @hareliEmotionsSignalsNormative2013]. Another important process in a Self-Centered orientation is represented by emotion self-regulation [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]: expressing and perusing one's own emotions through the EAT can enhance self-reflection on the emotional experience, and therefore strategies to maintain functional emotions and modify dysfunctional ones. In this regard, the use of an EAT linking appraisal dimensions and subjective feelings can enhance a form of regulation through *affect labeling* [@torrePuttingFeelingsWords2018; @liebermanPuttingFeelingsWords2007; @liebermanBooConsciousnessProblem2019], according to which the process of *naming* an emotion with a term entails an implicit regulation of that same emotion.

In the Partner-Oriented *flow*, learners will be concerned by the disclosure of their emotion to the partner and by taking the emotions of the partner into account. All the implications of the Self-Centered condition are maintained, but also implemented by the fact that emotional awareness emerges as a bi-directional communicative act. It therefore entails all the considerations regarding the social-sharing of emotion [@rimeEmotionElicitsSocial2009; @parkinsonEmotionsDirectRemote2008; @fischerSocialFunctionsEmotion2016; @vankleefInterpersonalDynamicsEmotion2018]. In addition, if a form of emotion regulation is implied, that could also be from an inter-personal perspective [@zakiInterpersonalEmotionRegulation2013; @netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016]. Finally, the emotion expressed by the partner can become a trigger for an emotion in the learner, that is, an inter-personal meta-emotion [@miceliMetaemotionsComplexityHuman2019; @normanConceptMetaemotionWhat2016].

Finally, in the Mutual-Modeling *flow*, all the previous concepts coalesce, with the addition of direct and persistent comparison of the evolution of the emotional experience in the learner and in the partner [@avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013]. This visuo-spatial comparison enhance the symmetry of the partner modeling [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009], which is considered a pivotal mechanism for inter-subjective meaning making in collaborative settings [@jarvelaEnhancingSociallyShared2015; @kirschnerAwarenessCognitiveSocial2015; @phielixGroupAwarenessSocial2011; @janssenCoordinatedComputerSupportedCollaborative2013].

The main research question in this experiment consists thus in assessing whether a different use of and access to emotional information changes the concrete use of an emotion awareness tool. The use of the tool is determined by the two fundamental functions of awareness tools: the expressing-displaying function (producing awareness) and the perceiving-monitoring function (perusing awareness). For each function, specific hypotheses and causal mechanisms are depicted hereafter.

### Use in Expressing Emotions

For expressing-displaying emotions, the three interfaces provide the learner with different reasons to express how she feels, as well as different affective triggers that may elicit emotional episodes (see also the hypothesis about the use in perceiving emotions below). More precisely:

-   With the *Self-Centered* interface, the learner knows she is the only recipient of the emotions she expresses. Therefore, if she decides to use the EAT to express an emotion, she probably does it out of self-interest, possibly linked to self-regulation as stated by the *intra-personal* perspective. In the meantime, the EAT does not provide any additional information about the partner's emotions that may serve as a trigger for emotional episodes in the learner herself.
-   With the *Partner-Oriented* interface, the learner knows she will not have access to her own emotions once she has expressed them, but that these are conveyed to the partner. Therefore, if she decides to use the EAT to express an emotion, one can assume that she does it from an *inter-personal* perspective (even if the possibility that she does it exclusively in a *Self-Centered* perspective cannot be excluded). In the meantime, the learner can also access the partner's emotions, which may represent additional triggers for meta-emotional episodes (*e.g.*, *Jane expresses guilt because she thinks Paul has just expressed anger as a result of something she has done*).
-   With the *Mutual-Modeling* interface, the learner knows the emotions she expresses are available both to her and the partner. Therefore, if she decides to use the EAT to express an emotion, she does in a *Self-Centered*, *Partner-Oriented*, or a combined perspective. In the meantime, the learner also disposes of direct and persistent comparison between her own emotions and that of the partner, which may also represent an additional trigger for emotional episodes compared to the *Partner-Oriented* interface (*e.g.*, *Jane expresses relief because she saw from the interface that in the last few minutes both she and Paul were often confused*).

Hypothesis (*H1*) is therefore stated in the following terms: there will be an overall difference in the use of the EAT for expressing-displaying emotions depending on the interface the learner has at disposal. More specifically, in comparing the interfaces, a greater use of the expressing-displaying function of the EAT in the *Partner-Oriented* and *Mutual-Modeling* interfaces compared to the *Self-Centered* interface would corroborate an *inter-personal* interest in expressing emotions. Furthermore, a greater number of emotions expressed in the *Mutual-Modeling* interface compared to the *Partner-Oriented* condition would suggest that the possibility of direct and persistent comparison between one's own and the partner's emotions results in a *surplus* of expression-displaying of emotions.

### Use in Perceiving Emotions

With respect to perceiving emotions, the three interfaces differ in the quality and quantity of the emotional information available on screen. The three interfaces will provide the learner with different reasons to seek and process the emotions expressed during the collaboration:

-   With the *Self-Centered* interface, the learner has access only to the emotions she has expressed over time during the collaboration. This may be interpreted as a *control* condition: what is the interest of having emotional information that the learner is already supposed to know? Seeking and processing the learner's own emotions may be explained by the interest of reflecting on the evolution of her own affective states during the task.
-   With the *Partner-Oriented* interface, the learner has access only to the emotions expressed by the partner, that is, information she does not already know. Seeking and processing the partner's emotions may be explained by the interest in knowing how the other is feeling and/or the evolution of the affective states of the partner during the collaboration.
-   With the *Mutual-Modeling* interface, the learner has access both to her own and the partner's emotions. This condition inserts an additional interest to the previous ones: the possibility of direct and persistent comparison between the learner's own emotions and that of the partner.

Hypothesis (*H2*) is therefore posited as follows: there will be an overall difference in the use of the EAT for seeking and processing the expressed emotions depending on the interface the learner has at disposal. More specifically, the *Partner-Oriented* and *Mutual-Modeling* interfaces will elicit a greater use in perceiving-monitoring emotions compared with the *Self-Centered* interface. Furthermore, greater information seeking and processing in the *Mutual-Modeling* interface compared to the *Partner-Oriented* interface would suggest an accrued interest due to direct and persistent comparison.

## Methods

Following @simmons21WordSolution2012 suggestion to increase transparency in experimental contributions, I report how I determined the sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

### Participants and Design

$N =$ 48 participants (29 women, 19 men), aged 18 to 55 ($M_{age}=37.3$, $SD_{age}=10.01$), voluntarily participated to the study. The sample size was determined by time constraints, since data had to be collected in 15 days in co-located conditions. 23 participants were university students from different faculties, both at undergraduate and graduate levels. 25 participants were professionals working for a company adopting distance learning practices. No remuneration was provided for taking part in the study. Participants were randomly assigned to one of the three conditions/interfaces (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*) in order to produce a balanced design with 16 participants per condition.

### Material

#### Overall Interface of the Task

The experimental material comprises different components; therefore an overview is provided before specifying the various details. Figure \@ref(fig:s1-task-interface-figure) shows the disposition of the screen during the experimental task. It comprises the EAT on the left-side of the screen, outlined in blue, and the simulated, joint-problem solving task on the right. The image indicates what part of the interface was simulated for the *Mutual-Modeling* condition. The interfaces of the other conditions are illustrated below. Some parts of the interface have been translated in English for the current contribution. In the experiment, though, the french language was used consistently for every condition.

(ref:s1-task-interface-caption) Overview of the interface that presents the various part of the material adopted for the CSCL task.

```{r s1-task-interface-figure}
#| fig.align="center",
#| fig.cap="(ref:s1-task-interface-caption)",  
#| fig.scap = "Study 1: Overall interface of the task",
#| out.width="100%" 
include_graphics(
  here::here("figure/s1/s1-task-interface.png")
)
```

#### Problem-Solving Task

The joint problem-solving task comprised four enigmas taken from a game. The first three enigmas had a clear response that could be inferred, whereas the last one was more of a non-nonsensical type. The same enigmas have been used in the usability test of the Dynamic Emotion Wheel [@fritzReinventingWheelEmotional2015], where they elicited different emotions both in number and kind in a population similar to that of the current sample (see Section \@ref(ux-reference-measures)). In spite of being apparently simple problems, in fact, most participants in the usability test managed to solve at best two out of four enigmas, manifesting disparate emotional reactions when the expected solutions were revealed. Each enigma was subdivided in three phases:

-   40 seconds during which the text of the enigma was showed on the interface. At this stage, participants could only express their emotions, but could not write their reasoning or the reply;
-   3 minutes and 20 seconds during which participants could write their reasoning and reply to the enigma, as well as see the *playback* reasoning (but not yet the answer) of the simulated partner;
-   1 minute in which the given answers from the participant and the partner were displayed on screen with the expected solution to the enigma. At this stage, once again, the reasoning and reply fields were not available on screen.

#### Configuration of the Emotion Awareness Tool

The toolbox was adopted to manipulate the interface in order to obtain the three different conditions. The expressing-displaying function was common to all conditions, which in turn differed in the perceiving-monitoring part of the interface.

For expressing an emotion, the underlying affective space was represented by EATMINT circumplex depicted at length in Section \@ref(eatmint-circumplex), which is determined by the *Valence* and *Control/Power* appraisals [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005]. Valence was prompted with the question *Is the situation pleasant?* *Control/Power* was evaluated with the question *Is the situation under your control?* Both evaluations were determined with the extreme negative pole *Not at all*, and extreme positive pole *Yes, absolutely*.

For the perceiving-monitoring function of the EAT, each condition differed in the following ways (depicted in Figure \@ref(fig:s1-eat-interfaces-figure)):

-   *Self-Centered*: the interface comprises an emotion time-line, then a line chart that depicts the evolution of the appraisal dimensions over time, and finally a tag cloud where the size of each subjective feelings is proportional to the frequency with which it has been expressed. The information provided is based only on the emotions expressed by the participant herself.
-   *Partner-Oriented*: the interface mirrors that of the *Self-Centered* condition, for the information provided is based only on the emotions expressed by the simulated partner (see below).
-   *Mutual-Modeling*: the interface comprises an emotion time-line, but with both the participant and the simulated partner's subjective feelings organized in two different rows. Two line charts complete the interface, one with the appraisal dimensions of the participant, and the other of the partner.

The *Self-Centered* and *Partner-Oriented* conditions present a tag cloud at the end of the interface in order to balance the surface of the EAT that contains information. In this way, the EAT occupies more or less the same amount of the screen.

(ref:s1-eat-interfaces-caption) The three different interfaces used in the experiment. From left to right: the *Self-Centered*, the *Partner-Oriented*, and the *Mutual-Modeling* versions.

```{r s1-eat-interfaces-figure}
#| fig.align="center", 
#| fig.cap="(ref:s1-eat-interfaces-caption)",
#| fig.scap = "Study 1: The three different interfaces of the experimental conditions", 
#| out.width="100%" 
knitr::include_graphics(
  here::here("figure/s1/s1-eat-interfaces.png")
)
```

#### Simulated partner

```{r s1-simulated-partner}
s1.simulated_partner <- s1.dew_configuration$task$simulation |>
  select(-user) |>
  filter(time <= 1200) |>
  left_join(s1.feelings_translation, by = c("feeling" = "label"))
```

The partner with whom participants were supposed to cooperate was actually simulated. To induce the effect of a *real* participant on the other side of the screen, the manipulations on the whole interface of the task (*i.e.*, the EAT and the problem-solving inputs) were recorded in the same pilot test as for the usability assessment of the DEW already mentioned above. 4 confederates (2 men and 2 women) performed the same joint problem-solving task, but in a synchronous situation. The *playback* is thus comprised by: (1) all the emotional episodes expressed, represented by the evaluation on the two appraisal dimensions and the related subjective feeling; and (2) what the confederate has typed, at the very same moment, into the reasoning field, as well as the answer to each of the 4 enigmas. In this regard, confederates were explicitly asked not to communicate directly through the text fields, but limit their typing to the reasoning for solving the problem. One of the *playback* was then randomly chosen for the task and *injected* into the experimental task interface. The simulated partner expresses `r s1.simulated_partner |> nrow()` emotions and finds the solution to 2 out of 4 enigmas. The full list of emotions -- comprising the time of expression, the associated *Valence* and *Control/Power* appraisals, and the subjective feeling -- are depicted in Table \@ref(tab:s1-simulated-emotions-table).

(ref:s1-simulated-emotions-caption) List of the emotions of the simulated partner. Time is expressed in seconds.

The result is that the simulated partner acts exactly in the same way with every participant for the problem-solving part of the interface. For the emotional information, though, the expressed emotions are also exactly the same, but the information is disclosed (or not) depending on which experimental condition the participant is attributed to. 

```{r s1-simulated-partner-table}
kable(
  s1.simulated_partner,
  col.names = c("Time", "Valence", "Control", "Feeling (FR)", "Feeling (EN)"),
  caption = "(ref:s1-simulated-emotions-caption)\\label{tab:s1-simulated-emotions-table}",
  caption.short = "Study 1: Simulated Partner's Emotions",
  longtable = FALSE,
  booktabs = TRUE
)
```

#### Eye-tracking

A Tobii T120 eye-tracker with Tobii Studio Pro v3.4.8 software [@tobiiabTobiiStudio2015] was used for eye-tracking measures. Areas Of Interest (AOI) were disposed on the EAT as a whole (left side of the screen) and the task (right side of the screen). The AOI of the EAT was further divided in the expressing-displaying upper zone, and the perceiving-monitoring lower zone.

### Procedure

Participants were given a specific time to come to the test, which was performed at Geneva University, and were reminded of the importance to be on time since another participant was performing the test in the meantime. The experimenter welcomed the participant in the room with the eye-tracking equipment. Once installed, the experimenter proceeded to explain the outline of the study:

-   Introduction and explication (10 minutes)
-   Warm-up session with the DEW and instructions for the task (5 minutes)
-   Collaborative task (20 minutes)
-   Debriefing (10 minutes)

#### Introduction and explication

The general aim of the study was explained. The experimenter reassured participants about the fact that the data would be anonymous, and that they could stop the experiment at any time without any reason. A first consent form was then signed if the participant agreed to take part in the study.

At this point, the experimenter explained how the collaborative task would take place. She first illustrated a demo about the functioning of the DEW. Since in a previous study [@fritzReinventingWheelEmotional2015], whose aim was to observe the spontaneous use of the tool, participants were confused about the dimension of *Control/Power*, in this experiment the experimenter proposed a more thorough explanation of what the two sliders of the DEW stand for. The explication also aimed at reducing the risk that participants will move the cursors for the *Valence* and *Control/Power* dimensions until they found the *right* subjective feeling. Next, the experimenter showed the perceiving-monitoring part of the EAT, which was explained according to the experimental condition the participant was attributed to. In this way, participants were informed about both what the emotional information they provided would be used for (*Self-Centered* vs *Partner-oriented/Mutual-modeling*), and to what kind of emotional information they would have access to (*Self-Centered* vs *Partner-Oriented* vs *Mutual-Modeling*).

Finally, the experimenter explained the right-hand side of the screen, which implemented the joint problem-solving task. Participants were informed about the three parts (reading, solving and solution) that composed each of the 4 enigmas to solve. They were also prompted to write their reasoning to solve the problem in the appropriate field, but to avoid direct communication with the partner. Since in the usability test of the DEW participants spontaneously expressed emotions and participated to the task, a system of point to foster collaboration originally adopted was dropped.

#### Warm-up session with the DEW

Participants were placed in front of the screen used for the test and could practice with a simplified version of the interface for the task. Participants could familiarize with the expression of emotions through the DEW, and random emotions were also injected in the interface at short intervals to emulate the emotion of the partner. The side of the screen devoted to the task was filled in with generic texts explaining what participants will see in the actual task (*e.g.*, *here will appear the text of the enigma*, *here you must write your reasoning*, ...)

#### Experimental task

Once the participant was ready for the test, the experimenter simulated to check-in with another confederate to simulate that the other participant was ready to start the experiment as well. Then the experimenter proceeded to calibrate the eye-tracker equipment. After being reminded about the general functioning of the eye-tracker and the importance of not moving during the task, the participant would then proceed with the task. At first, she had to fill the *log-in form* of the front-end interface of the toolbox, providing a random ID and an identifier for the pair. The system simulated a synchronization latency time, for the other participant to reach the same stage. Once the task started, the participant had access to the overall interface depicted above, including the *playback* of all the manipulations made by a confederate.

#### Post-test debriefing

After the task, participants were asked to fill in a survey whose data will not be used in the present contribution, but have been analyzed in another contribution [@perrierCollaborationEnvironnementMediatise2017]. At the end of the study, participants were informed about the manipulation of the simulated partner and the experimental reasons behind it. A second consent form was therefore submitted to participants, for them to confirm they understood the reason of the manipulation, and that they accepted the use of the data for scientific purposes.

### A Priori Exclusion Criteria

Exclusion criteria determined beforehand concerned only technical issues that could jeopardize the task, especially with respect to the simulated partner. Any interruption of the task or technical failure would make the trial not recoverable. Exclusion caused for low quality of eye-tracking measures, due for instance to the participant moving too much, were also foreseen, but not yet quantified due the lack of a precise benchmark.

### Data analysis

For hypothesis *H1*, concerning differences in expression of emotions, a omnibus one-way ANOVA with pairwise comparison between all conditions was planned beforehand. The number of emotions expressed through the EAT represents the dependent variable, and the interface of the EAT the independent variable with the three conditions (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*).

For hypothesis *H2*, concerning differences in perception of emotions, two indicators retrieved by eye-tracking measures [@blascheckVisualizationEyeTracking2017; @pooleEyeTrackingHumanComputer2005] are used as dependent variables. First, the number of times participants sought information by orienting their gaze inside the perceiving-monitoring zone of the interface, which is usually interpreted as an indicator that the person intentionally seeks for information she may find useful or that she got lost and needs reorientation. Second, the total time (in seconds) that participants spent looking at the perceiving-monitoring zone of the interface. Such indicator is usually interpreted as a proxy for information processing and could account for interest (*i.e.*, people look at it longer because it is interesting) or complexity (*i.e.*, people look at it longer because they need more time to understand what it means). Given the relative simplicity of the information provided -- even though people do not like graphs [@carpenterModelPerceptualConceptual1998; @pinkerTheoryGraphComprehension1990] -- and the use of a fixed interface of the EAT, both measures are used as indicators of interest, and therefore the greater the indicator, the greater use of the EAT is inferred. For both dependent variable, a omnibus one-way ANOVA with pairwise comparison between the three conditions of the independent variable (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*) were planned beforehand. A family-wise correction to account for inflation in Type I error has been planned for each pairwise comparison.

All analysis are conducted using the statistical software R, version 4.2.0. Analysis of variance use the Afex package version `r packageVersion("afex")` [@singmannAfexAnalysisFactorial2020].

## Results For Planned Analyses

In this section, only the results of the planned analysis are provided. A subsequent section describes post-hoc, corollary analysis that were not planned beforehand.

### Post-Hoc Exclusion

Results will be based on $N = 35$ participants. 10 participants were excluded due to technical issues during the task or low quality of eye-tracking measures. One participant was excluded for statistical reasons: the participant expressed 62 emotions during the task, against a mean of `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)`, that is, more than 8 standard deviations above the mean. Such a number, not even close to any participant to the same task in Fritz (2015), suggests a non representative use of the tool. The distribution of participants after post-hoc exclusions with respect to the experimental conditions is depicted in Table \@ref(tab:s1-observed-design-table).

(ref:s1-observed-design-capture) Number of participants retained for each experimental condition ($N = 35$).

```{r s1-observed-design}
s1.participants_repartition <- s1.et_data |>
  group_by(group) |>
  summarise(
    n = n()
  )

kable(
  s1.participants_repartition,
  col.names = c("Condition", "N"),
  caption = "(ref:s1-observed-design-capture)\\label{tab:s1-observed-design-table}",
  caption.short = "Study 1: Observed Experimental Design",
  longtable = FALSE,
  booktabs = TRUE,
) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header"))
```

The resulting unbalanced design and overall small $N$, particularly low in the *Partner-Oriented* condition, decrease the power of an already ambitious planned test and make it more exposed to violation of assumptions [@hoekstraAreAssumptionsWellKnown2012]. The interpretation of results from an hypothesis-testing perspective is nevertheless maintained, but provided from an exploratory perspective, such as a reference for future studies, rather than a confirmatory standpoint [@scheel2020; @fiedlerCycleTheoryFormation2004; @lakensTooTrueBe2017].

### Differences in Expressing Emotions

Participants expressed a total of `r s1.dew_emotions |> nrow()` emotions, which corresponds to a mean close to 14 emotions per participants (`r maf.print_m_sd(s1.aggregated_emotions$n, TRUE, FALSE)`). As a reference, in the usability test (see Section \@ref(ux-reference-measures)) the average was of $M = 17.14$, with a similar standard deviation of $SD =$ 5.05. Participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions |> filter(group == "Self") |> pull(n), FALSE, TRUE)` emotions; `r maf.print_m_sd(s1.aggregated_emotions |> filter(group == "Partner") |> pull(n), FALSE, TRUE)` in the *Partner-Oriented* condition; and `r maf.print_m_sd(s1.aggregated_emotions |> filter(group == "Mutual") |> pull(n), FALSE, TRUE)` in the *Mutual-Modeling* condition (see Figure \@ref(fig:s1-expressed-emotions-graph)).

(ref:s1-expressed-emotions-graph-caption) Number of emotions expressed by experimental condition. Bars represent 95% confidence intervals.

```{r s1-expressed-emotions-graph}
#| fig.align="center", 
#| fig.cap="(ref:s1-expressed-emotions-graph-caption)", 
#| fig.scap = "Study 1: Number of emotions expressed",
#| out.width="60%"

maf.plot_means_comparison(s1.aggregated_emotions, aes(x = group, y = n, color = group)) +
  labs(x = NULL, y = "Number of emotions") +
  theme(legend.position = "none")
```

An overall effect of the interface on the number of emotion expressed could not be detected neither in the omnibus ANOVA (`r apa_print(s1.anova.expressed_emotions)$full_result`), nor in the pairwise comparisons, which are illustrated in Table \@ref(tab:s1-expressed-emotions-contrats). Hypothesis *H1* is therefore not supported by the data at hand. In any case, the check of the assumptions of the ANOVA model revealed a problem with the normal distribution of residuals [@hoekstraAreAssumptionsWellKnown2012]. Results would have been thus withdrawn even if a difference could be detected.

(ref:s1-expressed-emotions-caption) Pairwise comparisons of the three interfaces with respect to the number of emotions expressed during the task (p-values are adjusted with the Tukey method).

```{r s1-expressed-emotions-contrats}
s1.anova.expressed_emotions.comp.summary |>
  mutate(p.value = round_ps_apa(p.value)) |>
  kable(
    digits = 2,
    caption = "(ref:s1-expressed-emotions-caption)\\label{tab:s1-expressed-emotions-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Expressed Emotions.",
    longtable = FALSE,
    booktabs = TRUE,
    col.names = c("Comp.", "Estimation", "SE", "df", "t", "p", "$\\Delta$M"),
    align = "l",
    escape = FALSE
  ) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header"))
```

### Differences in Perceiving Emotions

Before analyzing the results for the specific hypotheses implicating eye-tracking measures, it is worth looking at overall measures about participants' gaze during the task. For instance, the information inside the AOI with the problem solving task was processed with an average of $M =$ `r s1.et_data$Total_Visit_Duration_TASK_Sum |> mean() |> printnum()` ($SD =$ `r s1.et_data$Total_Visit_Duration_TASK_Sum |> sd()`) seconds, which is roughly aligned with the $M =$ 650.61 ($SD =$ 116.53) observed in Fritz (2015), see again Section \@ref(ux-reference-measures). All retained participants spent at least 7 minutes with their gaze intercepted inside the task area, which suggests that the task was performed as expected. The visits duration on the whole EAT have been of $M =$ `r s1.et_data$Total_Visit_Duration_EAT_Sum |> mean() |> printnum()` ($SD =$ `r s1.et_data$Total_Visit_Duration_EAT_Sum |> sd() |> printnum()`) seconds, thus for a shorter time compared to the $M =$ 231.55 ($SD =$ 70.86) of the usability test. All things considering, though, the measures do not seem to deviate too much from the usability test, which may be considered as reassuring. The analysis about the perception of the emotional information is therefore presented hereafter, divided in emotional information seeking and emotional information processing.

#### Seeking Emotional Information

Information seeking was measured by the total number of visits that each participants effectuated on the perceiving-monitoring zone of the interface. That is, how many times the gaze moved from any other part of the screen to the area where emotional information could be perused. Participants' gaze entered the perceiving-monitoring zone of the interface on average `r maf.print_m_sd(s1.visit_count$value, TRUE, TRUE)` times. In the *Self-Centered* condition, the number of visits has been `r maf.print_m_sd(s1.visit_count |> filter(group == "Self") |> pull(value), FALSE, TRUE)`, whereas the count roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.visit_count |> filter(group == "Partner") |> pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.visit_count |> filter(group == "Mutual") |> pull(value), FALSE, FALSE)`) conditions, for which the count was similar (see Figure \@ref(fig:s1-visit-count-graph)).

(ref:s1-visit-count-graph-caption) Total visits count in the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-visit-count-graph}
#| fig.align="center",
#| fig.cap="(ref:s1-visit-count-graph-caption)", 
#| fig.scap = "Study 1: Total visits count",
#| out.width="60%"

maf.plot_means_comparison(s1.visit_count, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Number of visits") +
  theme(legend.position = "none")
```

An overall effect of the interface adopted on emotional information seeking could be detected (`r apa_print(s1.anova.visit_count)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-visit-count-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions. Furthermore, the confidence intervals around all parameters are wide, with the lower bound of each interval approaching zero. As stated as a preamble to the results section, thus, even if the difference reaches the threshold of statistical significance, the population effect remain uncertain.

(ref:s1-visit-count-caption) Pairwise comparisons of the three interfaces with respect to the number of visits at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-visit-count-contrats}
s1.anova.visit_count.comp.summary |>
  mutate(p.value = round_ps_apa(p.value)) |>
  kable(
    digits = 2,
    caption = "(ref:s1-visit-count-caption)\\label{tab:s1-visit-count-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Visits Count.",
    longtable = FALSE,
    booktabs = TRUE,
    col.names = c("Comp.", "Estimation", "SE", "df", "t", "p", "$\\Delta$M"),
    align = "l",
    escape = FALSE
  ) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header"))
```

#### Processing Emotional Information

Information processing was measured by the accumulated time that each participant spent with her gaze inside the perceiving-monitoring area of the interface. That is, for how many seconds, across all visits where information was sought after, participants' gaze remained within the perimeter of the area on the screen where emotional information was available. Participants spent on average `r maf.print_m_sd(s1.total_visit_duration$value, TRUE, TRUE)` seconds looking at any part of the perceiving-monitoring zone of the interface, which amounts to 4.28% of the total task time. Participants in the *Self-Centered* condition spent `r maf.print_m_sd(s1.total_visit_duration |> filter(group == "Self") |> pull(value), FALSE, TRUE)` seconds, whereas this time roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.total_visit_duration |> filter(group == "Partner") |> pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.total_visit_duration |> filter(group == "Mutual") |> pull(value), FALSE, FALSE)`) conditions, for which time differed slightly (see Figure \@ref(fig:s1-total-visit-duration-graph)).

(ref:s1-total-visit-duration-graph-caption) Total time (in seconds) spent looking at the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-total-visit-duration-graph}
#| fig.align="center",
#| fig.cap="(ref:s1-total-visit-duration-graph-caption)", 
#| fig.scap = "Study 1: Total visit duration",
#| out.width="60%"
maf.plot_means_comparison(s1.total_visit_duration, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Seconds") +
  theme(legend.position = "none")
```

An overall effect of the experimental condition on the time spent processing emotional information could be observed (`r apa_print(s1.anova.total_visit_duration)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-total-visit-duration-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions. As for information seeking, once again the confidence interval are wide, with the lower bounds approaching zero. Results should be therefore calibrated according to the uncertainty around the population parameters.

(ref:s1-total-visit-duration-caption) Pairwise comparisons of the three interfaces with respect to total time spent looking at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-total-visit-duration-contrats}
s1.anova.total_visit_duration.comp.summary |>
  mutate(p.value = round_ps_apa(p.value)) |>
  kable(
    digits = 2,
    caption = "(ref:s1-total-visit-duration-caption)\\label{tab:s1-total-visit-duration-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Total Visit Duration.",
    longtable = FALSE,
    booktabs = TRUE,
    col.names = c("Comp.", "Estimation", "SE", "df", "t", "p", "$\\Delta$M"),
    align = "l",
    escape = FALSE
  ) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header"))
```

## Post-Hoc Corollary Analyses

In this section, I provide the results of additional analyses that have not been planned before the study. First, I extend the analysis of eye-tracking measures using transitions between Areas Of Interest as a viable measure of the use of an EAT. Second, I provide indications of the moment-to-moment use of the EAT with respect to the appraisals and subjective feeling measures collected throughout the task. Finally, I take advantage of the use of the same task as in @fritzReinventingWheelEmotional2015 to conduct a small internal meta-analysis [@gohMiniMetaAnalysisYour2016] that can be of interest for the use of the same -- or similar -- task in future studies.

### Transitions Between Areas of Interest

The eye-tracking measures used in the planned analyses of variance treated each zone of the interface as a separated element. Given the importance of dynamic, moment-to-moment phenomena in the overall thesis, it is worth investigating also transitions between the three main Areas of Interest (AOI) of the overall interface, that is (1) the expressing zone, which is common to all conditions; (2) the perceiving zone, which varies according to the experimental condition; and (3) the area dedicated to the *main* task, which is also common to all conditions. Given that transitions can go in either direction between AOI, there are 6 possible combinations of transitions: (1) Expressing to Perceiving and (2) Perceiving to Expressing; (3) Expressing to Task and (4) Task to Expressing; and finally (5) Perceiving to Task and (6) Task to Perceiving. An exploratory analysis of the transitions may reveal whether specific transitions are more frequent than others depending on the interface at disposal.

The number of transitions between AOI was computed by searching for subsequent rows in the eye-tracking logs for each of the $N = 35$ participants in which the first row had a certain AOI activated, and the following row had another AOI activated. Is it worth noting, though, that this method is sub-optimal because the experimental task included the use of mouse and keyboard. Therefore some transitions may have been lost since the gaze-path may have been interrupted by a *detour* to the peripherals. Nevertheless, it is safe to assume that participants held their mouse throughout the task, and directed their gaze into the AOI they were interested in acting upon -- for instance, in order to focus the pointer into the text area -- before turning the gaze away from the screen if they needed to look at the keyboard for typing. All things considering, thus, this method can be of interest at least as an exploratory method, even though it currently lacks external validity with an appropriate test. It should therefore be revisited before being deployed in a substantial analysis.

After seeing the data, one participant was excluded for having a number of transitions from Expressing to Perceiving and from Perceiving to Expressing much higher than all other participants: more than 100 against a mean of `r maf.print_m_sd(s1.transitions |> filter(transition == "Expressing to Perceiving" || transition == "Perceiving to Expressing") |> pull(num_transitions), FALSE, TRUE)` for the other participants regardless of the condition. Results are therefore based on $N = 34$ participants.

Participants made on average `r maf.print_m_sd(s1.transitions |> group_by(ParticipantName) |> summarise(total_transitions = sum(num_transitions)) |> pull(total_transitions), TRUE, TRUE)` transitions between any two AOI. Figure \@ref(fig:s1-transitions-graph) reports the number of transitions stratified by experimental condition between the 6 AOI organized in three rows so that each row displays the transitions between the same two AOI in each direction.

(ref:s1-transitions-graph-caption) Number of transitions between Areas of Interest (AOI) on the interface. Transitions aggregated for $N = 34$ participants. Bars represent 95% confidence intervals.

```{r s1-transitions-graph}
#| fig.align = "center",
#| fig.cap="(ref:s1-transitions-graph-caption)", 
#| fig.scap = "Study 1: Number of transitions between AOI",
#| fig.height=6, 
#| out.width="100%"
s1.transitions.graph
```

Data suggest that there are differences that may be accounted for by the type of interface the participants have access to. In particular, participants in the *Mutual-Modeling* condition seem to be more prone to make transitions between the two AOI that are more directly related to the social sharing of emotions. Transitions between the expressing-displaying and the perceiving-monitoring zone (first row in the graphic) may indicate that the possibility of direct and persistent comparison of one's own emotions with that of the partner could serve as a *social reference* before expressing one's own emotions, or *social comparison* after having expressed them. Furthermore, transitions between the perceiving-monitoring zone and the task zone (last row in the graphic) may indicate that the emotional information is taken into account as instrumental information to the task at hand. In fact, this path is the most interesting one if one consider that emotional information may be instrumental to the task at hand on a moment-to-moment basis.

Participants in the *Self-Centered* condition seem to privilege the paths between the expressing-displaying zone and the task zone, which is consistent with the fact that the perceiving-monitoring zone has only information about their own emotions. It is interesting to notice, though, that in the *Self-Centered* condition, transitions from the expressing-displaying zone to the perceiving-monitoring zone (first row, graph on the left) do not seem to be more frequent compared to the *Partner-Oriented* condition. This may be relevant because it could rule out the possibility that a difference between the *Partner-Oriented* and *Mutual-Modeling* condition may be due simply to the fact that, in the *Mutual-Modeling* condition, participants only seek confirmation of what they have expressed, since this phenomenon is not available in the *Partner-Oriented* condition. It is therefore more likely that participants in the *Mutual-Modeling* condition seek the perceiving-monitoring zone for the emotional information about the partner.

Finally, the *Partner-Oriented* condition seems *stuck in the middle*. Results for this group are difficult to assess due to the small sample and the great inter-individual variance that is also present in the planned analysis. As a rule of thumb interpretation, the *Partner-Oriented* condition seems to go hand-in-hand with the *Self-Centered* condition in transitions between the *expressing-displaying* zone and the *perceiving-monitoring* zone (first row); and with the *Mutual-Modeling* condition in the other transitions (second and third rows).

In an attempt to figure out whether this kind of analysis may be used in a more structured manner, a multilevel linear model, also known as mixed linear model [@batesFittingLinearMixedEffects2014; @kuznetsovaLmerTestPackageTests2017; @westLinearMixedModels2015], was fitted to the data at hand using the mixed function of the Afex [@singmannAfexAnalysisFactorial2020; @singmannIntroductionMixedModels2020] R package version `r packageVersion("afex")`. The model was fitted in the following terms: the number of transitions per participant for each possible path represented the outcome variable; the type of transition and the interface of the EAT (*i.e.* the experimental condition) were considered as fixed factors, with an interaction between the two; the participant was used as a random intercept to account for the non-independence of observations. A more complex model could have been more interesting, but hardly feasible due to the small number of participants [@batesParsimoniousMixedModels2018].

A Type III Analysis of Variance of the multilevel linear model confirms effects of both individual factors and the interaction. Results are depicted in Table \@ref(tab:s1-transitions-anova-table) using Kenward-Roger approximation for computing the *p*-value [@lukeEvaluatingSignificanceLinear2017].

(ref:s1-transitions-anova-caption) Results of a Type III ANOVA on the fitted multilevel linear model

```{r s1-transitions-anova-table}
s1.transitions.lmm.anova_table |>
  kable(
    caption = "(ref:s1-transitions-anova-caption)\\label{tab:s1-transitions-anova-table}",
    caption.short = "Study 1: Transitions between AOI",
    booktabs = TRUE,
    longtable = TRUE
  ) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header"))
```

Table \@ref(tab:s1-transitions-comparisons-table) reports the pairwise comparisons between the three experimental conditions stratified by the bi-directional path of the transition. Differences in the pairwise comparisons can be detected between *Self-Centered* vs *Mutual-Modeling* and *Partner-Oriented* vs *Mutual-Modeling* in the transitions between expressing-displaying and perceiving-monitoring. In the four comparisons, the more socially-oriented interface obtained more transitions, in both directions, than the less socially-oriented one, corroborating the assumption that participants make use of the emotional information about the partner as a reference.

In the transitions between expressing-monitoring and the task, only a difference for the path going from the task to the expression-displaying zone was detected, with more transitions in the *Partner-Oriented* than in the *Self-Centered* interface. The effect is nevertheless not corroborated by any other comparison in the same transition path.

Finally, in the transitions between perceiving and the task, differences were detected between the *Self-Centered* and the *Mutual-Modeling* interfaces, with the *Mutual-Modeling* interface yielding more transitions in both directions. These results may support the role of emotional awareness as instrumental information directly related to the task at hand, but are not corroborated by a difference between the *Self-Centered* and the *Partner-Oriented* interfaces.

(ref:s1-transitions-comparisons-caption) Comparisons between the groups stratified by the path of the transitions. The Kenward-Roger approximation for the degrees of freedom is adopted and *p*-values are adjusted using the Tukey method for comparing a family of 3 estimates.

```{r s1-transitions-comparisons-table}
s1.transitions.contrasts$contrasts |>
  as_tibble() |>
  select(-transition) |>
  mutate(p.value = round_ps_apa(p.value)) |>
  kable(
    caption.short = "Study 1: Pairwise comparison between transitions",
    longtable = TRUE,
    booktabs = TRUE,
    linesep = c("", "", "\\addlinespace"),
    col.names = c("Comparison", "Est.", "SE", "df", "t.ratio", "p.value"),
    align = "l",
    caption = "(ref:s1-transitions-comparisons-caption)\\label{tab:s1-transitions-comparisons-table}",
  ) |>
  pack_rows("Expressing to Perceiving", 1, 3) |>
  pack_rows("Perceiving to Expressing", 4, 6) |>
  pack_rows("Expressing to Task", 7, 9) |>
  pack_rows("Task to Expressing", 10, 12) |>
  pack_rows("Perceiving to Task", 13, 15) |>
  pack_rows("Task to Perceiving", 16, 18) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

All things considering, transitions may represent a more interesting measure of the perceiving-monitoring function of emotional awareness compared to information seeking and information processing adopted in the planned analyses. Even considering the shortcomings (e.g. transitions interrupted by the use of the keyboard), transitions provide a more *dynamic* outlook on how emotional information is integrated into the task. The concept of transition may even be pushed further by measuring at which moment the transition has occurred, which would provide useful information on the interest -- but also the potential distraction -- of moment-to-moment awareness. For instance, it would be possible to assess whether learners look at the emotions expressed by the partner as soon as they appear on the interface, or if they wait idle period in the task. In this regard, the next analysis focus precisely on the moment-to-moment perspective, but with respect to emotion expression-displaying.

### Emotions and Time: Evaluating the Purpose of Moment-to-Moment Awareness in Expressing Emotions

One of the main tenets of the present contribution is the advantage of moment-to-moment emotional awareness. Some exploratory analyses on the sample were performed in order to check to what extent the moment-to-moment feature has been exploited with respect to the expression of emotions.

#### Cognitive Evaluation Over Time

Congruently with appraisal theories of emotions -- which state that it is the evaluation one does of the situation and not the situation *per se* that elicits the emotion -- it is worth checking for the emergence of a pattern in the appraisals of *Valence* and *Control/Power* over time. Since all participants were exposed to the same stimuli (except participants in the *Self-Centered* condition, who did not see the emotions of the simulated partner), a clear pattern in the evaluation of the two criteria would not be congruent with appraisal theories. Figure \@ref(fig:s1-appraisal-evolution-graph) shows all the $N = `r nrow(s1.dew_emotions)`$ emotions that have been expressed by all the participants over the 20 minutes of the task, organised by appraisal dimensions and stratified by experimental conditions. Each gray dot represents an emotion expressed by the participant, which are united by a gray line that forms an appraisal profile over time for each participant. A Locally Estimated Scatterplot Smoothing (LOESS) -- that is, non-parametric curve that best fit the empirical data [@jacobyLoessNonparametricGraphical2000] -- is superposed to the appraisal profiles for both appraisal dimensions, stratified by experimental condition.

(ref:s1-appraisal-evolution-caption) Evolution of the appraisal dimensions over time with a LOESS smoother ($N = 35$).

```{r s1-appraisal-evolution-graph}
#| fig.align="center", 
#| fig.cap="(ref:s1-appraisal-evolution-caption)",
#| fig.scap = "Study 1: Evolution of the appraisal dimensions",
#| out.width="100%"
s1.appraisal_over_time.graph 
```

The graphic indicates that there is indeed great variation in the appraisal profiles of each participant, with gray lines and dots spanning the whole range of the appraisal rating. When put together with the smoother, though, the patterns tend to overlap between appraisal dimensions, which corroborates the lack of orthogonality between Valence and Control/Power already observed with the Geneva Emotion Wheel and during the usability test of the Dynamic Emotion Wheel (see Sections \@ref(gew-limitations) and \@ref(dew-ux-test)).

With respect to the experimental conditions more specifically, the *less neutral* smoother corresponds to the Mutual-Modeling condition, for which both appraisal dimensions diminish over time. The data certainly does not allow any solid interpretation, but it is nevertheless worth noting that this kind of evolution could be investigated more thoroughly, for instance through the use fo spline regression [@mcelreathStatisticalRethinkingBayesian2020], which can account for the non-linearity of the process. In this way, it would be possible to test whether appraisal profiles tend to vary according to specific factors, for instance the emotions expressed by the partner.

#### Subjective Feelings Over Time

The same analysis can be conducted with respect to the expression of subjective feelings over time. Figure \@ref(fig:s1-feelings-evolution-graph) plots the evolution of the expression of the 20 subjective feelings -- which are part of the underlying EATMINT affective space used in the study -- aggregated for all $N=35$ participants. The observations are stratified per condition. In order to reduce the space of the graph, the legend for each condition has been omitted, but the colors are congruent with previous graphs.

(ref:s1-feelings-evolution-caption) Expression of the subjective feelings. $N = `r nrow(filter(s1.dew_emotions, listed == TRUE))`$ emotions (out of `r nrow(s1.dew_emotions)`) whose subjective feeling belongs to the underlying affective space used by the DEW, aggregated for the $N = 35$ participants. (Legend omitted for reducing space, see previous graphs.)

```{r s1-feelings-evolution-graph}
#| fig.align="center", 
#| fig.cap="(ref:s1-feelings-evolution-caption)",
#| fig.scap = "Study 1: Evolution of the expression of the subjective feelings",
#| out.width="100%"
s1.feelings_over_time_graph
```

Not considering the feelings that have been expressed only a few times (*e.g.*, *envious* or *disgusted*), most of the subjective feelings have been expressed rather uniformly over the 20 minutes of the task. Interesting exceptions are the feelings *bored* and *frustrated* that only starts around 5 minutes into the task -- that is, around the end of the first enigma -- which may be due to the repetitive nature of the task for boredom, and the increasing difficulty of the enigmas for frustration.

Finally, the overall small sample size combined with the unbalanced number of participants for experimental condition imply caution even on superficial interpretations about the effect of the interface. It is nevertheless worth noting how participants felt often *relieved* or *satisfied* in the *Mutual-Modeling* condition, but not in the *Self-Centered* or *Partner-Oriented* condition; or that the *empathetic* subjective feeling was expressed in the *Mutual-Modeling* and *Partner-Oriented* condition, but not in the *Self-Centered*. With a greater number of participants, it would be interesting to perform this kind of stratification in a more systematic way.

#### Expression of Emotions During the Different Parts of the Problem-Solving Task

Finally, a major conjecture that can be proposed with respect to the moment-to-moment feature of an EAT is that the emotional information is directly integrated within the task at hand. Since every enigma was divided in three sub-activities (reading, solving, and assessing the solution), it is worth investigating the number of emotions that have been expressed on each of the three type of activities. In fact, it may be assumed that the solving part of the task is the one that requires more interaction with the partner, since participants were supposed to base their solution on each others reasoning. In this sense, the number of emotions shared during the solving part of the enigma would suggest that the emotional information is considered instrumental to the task at hand.

To assess this claim, the number of emotions expressed by each participant in each of the three parts of the task was averaged across the four enigmas. Since the three sub-activities did not last the same amount of time (40 seconds for reading, 3:20 seconds for solving, and 60 seconds for assessing the solution), each average was also projected as if the whole task of 20 minutes was potentially constituted of only the interested part. Figure \@ref(fig:emotions-per-activity) depicts the observed as well as the projected number of expressed emotions stratified by task sub-parts and experimental conditions. The horizontal gray line corresponds to the observed mean of expressed emotions, that is $M =$ 13.8.

(ref:emotions-per-activity-caption) Observed and projected number of expressed emotions depending on the type of activity during the task (reading, solving or assessing the solution). The gray line corresponds to the observed overall mean of $M =$ 13.8.

```{r emotions-per-activity-graph}
#| fig.align="center", 
#| fig.cap="(ref:emotions-per-activity-caption)",
#| fig.scap = "Study 1: Observed and projected number of emotion expressed per task activity", 
#| fig.height=4,
#| out.width= "80%"
s1.emotion_per_task_activity.graph
```

The data suggest that, proportionally, participants expressed more emotions while reading the enigma and, in particular, assessing the solution, whereas fewer emotion were expressed during the problem-solving task. This is most probably due to the fact that solving the problem needs more cognitive effort and also direct manipulation on the keyboard to write the reasoning compared to the passive situation of reading the enigma and finding out the solution. At the same time, it is interesting to note that the number of emotion expressed is more or less consistent across experimental conditions, which corroborates the interest for an intra-individual perspective already mentioned above, in particular when facing the problem and discovering the solution -- that is, two situations which are novel and relevant for the person [@schererNatureDynamicsRelevance2013].

### Internal Meta-Analysis on Task Indicators

Taking advantage of the fact that the same task was adopted, under similar conditions, in the usability test of the DEW [@fritzReinventingWheelEmotional2015], small internal meta-analyses [@gohMiniMetaAnalysisYour2016] were performed on the point-estimate means for the three dependent variables adopted in the current contribution. The interest of the meta-analyses is to provide a better assessment of the performance-based indicators of the use of the EAT, for instance as reference for future studies[^This use was actually planned for a empirical contribution initially considered as part of the thesis, but which was not carried out due to the pandemic].

Each meta-analysis has been conducted using the R meta package version `r packageVersion("meta")`, adopting the inverse of the variance weighting mechanism to account for differences in the sample size of the two internal studies [@borensteinIntroductionMetaanalysis2009]. Results both for the fixed and the random models (using the DerSimonian-Laird estimator for $tau^2$) are provided.

#### Expressing Emotions Internal Meta-Analysis

The meta-analysis on the expression of emotions has been conducted on all the retained participants for both studies, since, even for participants in the *Self-Centered* condition, the overall situation in which participants have expressed their emotions are sufficiently close for an internal meta-analytic purpose. Consequently, the sample size are of $N = 14$ in Fritz (2015) and of $N = 35$ in this contribution. Results, depicted in Figure \@ref(fig:s1-forest-expressing), assess an estimated mean of `r s1.meta_analysis_expressing.summary$fixed$TE |> printnum()` [`r s1.meta_analysis_expressing.summary$fixed$lower |> printnum()`; `r s1.meta_analysis_expressing.summary$fixed$upper |> printnum()`] expressed emotions for the fixed effect model, and of `r s1.meta_analysis_expressing.summary$random$TE |> printnum()` [`r s1.meta_analysis_expressing.summary$random$lower |> printnum()`; `r s1.meta_analysis_expressing.summary$random$upper |> printnum()`] for the random effect model. The meta-analysis highlights the presence of considerable heterogeneity in the expression of emotions ($\tau^2$ = 10.29; $\tau$ = 3.21; I\^2 = 82.0% [23.9%; 95.7%]; H = 2.36 [1.15; 4.84]; $\chi^2$ = 5.55 *p* = .018). This may suggest that the different conditions of the two studies may have played a role in inflating the number of emotions expressed in Fritz (2015), where participants were explicitly asked, if possible, to express at least one emotion in each phase of the 4 enigmas (*i.e.* which would amount to 12). On the contrary, in this contribution they did not receive any guidance as of the number of emotion to express. This may be interpreted as a warning about the importance of being careful in framing how the expression of emotional information is prompted, even if the inflation of the number of emotions may not be necessarily accounted by *forced* emotions, that is, emotional episodes that are not *really* felt, but nevertheless reported. It may also be the case that prompting for emotional expression may ease participants into expressing their emotions, something they could be less prone to do otherwise.

```{r s1-forest-expressing}
#| fig.align="left",  
#| fig.cap="Internal meta-analysis of the number of emotions expressed in the experimental task.",
#| fig.scap = "Study 1: Small meta-analysis of the number of emotions expressed", 
# fig.height=2,
#| out.width="100%"
forest.meta(s1.meta_analysis_expressing, hetstat = TRUE, xlim = c(5, 25), layout = "JAMA")
```

#### Information Processing Internal Meta-Analysis

The internal meta-analysis on information processing has been conducted using only the participants retained for the eye-tracking analysis ($N = 12$) in Fritz (2015), and only participants in the *Partner-Oriented* and *Mutual-Modeling* conditions ($N = 23$) in this contribution, for the interface in these situations is identical or at least very similar with respect to the *social* information shared. Results, depicted in Figure \@ref(fig:s1-forest-processing), assess an estimated mean of `r s1.meta_analysis_processing.summary$fixed$TE |> printnum()` [`r s1.meta_analysis_processing.summary$fixed$lower |> printnum()`; `r s1.meta_analysis_processing.summary$fixed$upper |> printnum()`] total visit duration, in seconds, for the fixed effect model, and of `r s1.meta_analysis_processing.summary$random$TE |> printnum()` [`r s1.meta_analysis_processing.summary$random$lower |> printnum()`; `r s1.meta_analysis_processing.summary$random$upper |> printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 40.81; $\tau$ = 6.39; I\^2 = 45.2%; H = 1.35; $\chi^2$ = 1.82 *p* = .177), which may indicate that the time spent at looking at emotional information could be determined by a balance between the primary problem-solving activity and the sustaining emotional awareness. The point estimate of total visit duration being around 1 minute over the 20 minutes of the task, it corresponds to a proportion of 5% of the total time.

```{r s1-forest-processing}
#| fig.align="left", 
#| fig.cap="Internal meta-analysis of the time spent at processing emotional information available on screen.",
#| fig.scap = "Study 1: Small meta-analysis of emotion information processing", 
#| fig.height=2,
#| out.width="100%"
forest.meta(s1.meta_analysis_processing, hetstat = TRUE, xlim = c(30, 80), layout = "JAMA")
```

#### Information Seeking Internal Meta-Analysis

With respect to emotional information seeking, the internal meta-analyses comprise the same samples as for information processing, that is $N = 12$ in Fritz (2015) and $N = 23$ in this contribution. Results, depicted in Figure \@ref(fig:s1-forest-seeking), assess an estimated mean of `r s1.meta_analysis_seeking.summary$fixed$TE |> printnum()` [`r s1.meta_analysis_seeking.summary$fixed$lower |> printnum()`; `r s1.meta_analysis_seeking.summary$fixed$upper |> printnum()`] number of visits for the fixed effect model, and of `r s1.meta_analysis_seeking.summary$random$TE |> printnum()` [`r s1.meta_analysis_seeking.summary$random$lower |> printnum()`; `r s1.meta_analysis_seeking.summary$random$upper |> printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 0; $\tau$ = 0; I\^2 = 0%; H = 1; $\chi^2$ = .56 *p* = .453), which is nevertheless rather due to the huge variability within studies rather than homogeneity between studies. In future studies, the number of transitions between AOI could represent a more informative measure for emotional information seeking.

```{r s1-forest-seeking}
#| fig.align="left", 
#| fig.cap="Internal meta-analysis of the number of times emotional information has been visited on screen.", 
#| fig.scap = "Study 1: Small internal meta-analysis of emotion information processing",
#| fig.height=2,
#| out.width="100%"
forest.meta(s1.meta_analysis_seeking, hetstat = TRUE, xlim = c(50, 100), layout = "JAMA")
```

## Discussion

The planned analyses in this experiment aimed at investigating whether a different use of, and access to emotional information determine differences in the concrete use of an EAT during a computer-mediated collaborative task, which has been simulated in order to control from other sources of emotional information outside the manipulated interface of the EAT. The technical issues, due to articulated settings between the material and the simulation implicated in the experiment, have nevertheless reduced an already limited sample size. Furthermore, the inter-individual differences in all dependent variables entail wide confidence intervals, whose source can be traced back to the many processes implicated in the task. Some of them may not be directly inherent to a genuine interest (or disinterest) in emotional awareness, and may have potentially influenced participants' capacity in conveying and taking into account emotional awareness beyond their intentions. For instance, participants had to coordinate multiple functions, both cognitively and practically (e.g. writing on a keyboard, manipulating the EAT, etc.), under specific time constraints. Participants with less dexterity in writing at the keyboard or manipulating the interface may have found less time to dedicate to the EAT even if they were willing to. The small sample size cannot guarantee that these individual differences are sufficiently balanced by the randomized trial. Even in the presence of detectable effects, thus, the assessment of their relevance in terms of *practical* consequences is limited: their size is inherently high due to the small sample and they should not even be taken as reliable benchmarks for future studies [@albersWhenPowerAnalyses2018].

On the other hand, the controlled environment in which the *performance-based* measures have been obtained make them worth of interest in assessing to what extent the presence of an EAT serves as an affordance in conveying and taking notice of emotional awareness during a computer-mediated collaborative task. Eye-tracking measures, in particular, may be considered spontaneous reactions occurring to some extent even beyond participants' top-down control [@jacobEyeTrackingHuman2003; @pooleEyeTrackingHumanComputer2005]. The discussion of the obtained results may thus contribute to sketch a more defined outlook of the use of an EAT and provide cues for further hypotheses worth investigating or shortcomings to be taken into account in future studies. At the same time, corollary analyses can also be implemented in a more general assessment of the EAT with respect to its fundamental features: self-report, emotion structure injected into the tool, and moment-to-moment availability of emotional awareness.

### Emotion Expression Seems Viable Also From a Self-Centered Perspective

In the first hypothesis, it has been posited that learners expression of emotions through the EAT varies depending on what use would be made of them, and what emotional information they have access to through the interface. More precisely, it has been stated that participants in the *Self-Centered* condition would express fewer emotions, compared to the other, more socially-oriented, conditions. It has also been posited that participants in the *Mutual-Modeling* condition would express more emotions than in the *Partner-Oriented* condition by virtue of an additional prompt in social sharing due to the direct and persistent comparison between one's own emotions and that of the partner.

This hypothesis could not be supported, with results that also revealed a violation of assumption about normality of residuals in the ANOVA model [@hoekstraAreAssumptionsWellKnown2012]. The number of emotion expressed may in fact be considered a tricky measure, since, depending on the specific number of emotion expected, it may be considered a countable measure, thus closer to a Poisson sampling distribution rather than a Gaussian one. Notwithstanding, the usability test conducted in similar conditions yielded an average of emotions around 17, and this experiment yielded an average close to 14, with neither of the measures that was zero-inflated as it is often the case with *pure* countable measures. As a consequence, the use of ANOVA could be warranted. The non-normality of residuals in this specific case may rather be due, thus, to an intrinsic inter-individual difference towards expressing emotions, which can result from a mixture of dispositional stances [@schererEmotionEmotionalCompetence2010; @scherer2021] and technical skills in manipulating the interface. If this is the case, then, small effects accountable to the interface should be expected. As a consequence, researchers planning for effects of contextual/external factors on the expression of emotion shall probably aim at a considerable sample size to discern the signal from the inter-individual noise.

It may thus be interesting to reverse the perspective and, rather than seeking for differences, highlight how participants in all conditions expressed on average `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)` emotions, that is more than 1 emotion every 2 minutes. In particular, participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions |> filter(group == "Self") |> pull(n), FALSE, TRUE)` emotions despite knowing they were the only recipient of the information. This result could be considered, in principle at least, as supportive of the intra-personal function of emotional awareness*.* The presence of an EAT could have increased emotion alertness as a first step towards emotional expression, which may then be used as a form of (implicit) emotion regulation [@liebermanPuttingFeelingsWords2007; @torrePuttingFeelingsWords2018]. On the other hand, though, this result may also be explained by side effects of the experimental task. For instance, this number may be inflated by task compliance, since the overall experimental setting was overtly aimed at expressing emotions. Furthermore, the characteristics of the experimental task, whose timing is fixed and not determined by the participants' actions, may also have pushed participants to express emotions to fill idle time between enigmas or part of the task within each enigma, rather than for an urge to express and regulate their emotions. This seems corroborated by the corollary analysis comparing the number of emotions expressed during the three different parts of each enigma: reading, solving, and assessing the solution. Participants expressed proportionally more emotions while reading the enigma and assessing the solution compared to the problem-solving task, in which emotional awareness could be directly integrated into collaborative processes such as transactivity [@molinariEmotionFeedbackComputermediated2013; @molinariKnowledgeInterdependencePartner2009:1; @sanginFacilitatingPeerKnowledge2011] or the resolution of socio-cognitive tensions [@andriessenSociocognitiveTensionCollaborative2011]. In other words, it would seem that participants felt the need to express how they felt from an holistic perspective. Even in the conditions with emotional information that could potentially be used for partner- and mutual-modeling, participants did not seem to feel the urge to convey more emotions to the partner. On the other hand, when participants were more free to assess the situation, they indeed expressed how they felt. The time constraint imposed and the simulated nature of the task require nevertheless caution on this kind of inference. Especially the time constraint introduces a clear bias in the rhythm of tension and relaxation that could be beneficial to producing and consuming emotional awareness even while performing an active task.

The moment-to-moment possibility of expressing emotion needs, thus, a more ecological situation for a thorough assessment, compared to the controlled environment adopted in this experiment. On the other hand, the strict environment allowed to expose all participants to exactly the same condition, except for the manipulated interface. In this regard, the assessment of the appraisal dimensions have shown that, congruently with appraisal tenets, participants evaluated the very same situation in different ways, especially through the use of the sliders. This corroborates the interest for implementing a dimensional approach into the tool, especially from an appraisal-driven perspective. On the other hand, the evaluation on the two sliders seem to overlap, a general problem that will be assessed more thoroughly in Chapter \@ref(study-3).

All things considering, thus, the present contribution conveys limited and mixed evidence with respect to the interest for expressing emotions during a computer-mediated collaborative task as a function of the use of and access to emotional information. The expressing-displaying function is also the one more influenced by the task at hand. Further investigation on the subject should thus not only increase the sample size, but also implement a real collaboration with more relaxed time constraints. The dynamics within each group or dyad of participants could be accounted for by multi-level models, and therefore a strict controlled environment can be deferred in favor of a more ecological setting. Further investigation may also plan for a sample size determined on an equivalence test, and perform both tests [@fidlerEpistemicImportanceEstablishing; @lakensEquivalenceTestsPractical2017]: one in search of a difference, and one that attempts ar ruling out an effect of the use of, and access to emotional information. With respect to the latter, potential limits for the same kind of task -- but with real collaboration -- could be set in raw units at +/- 3 emotions, which roughly corresponds to half a standard deviation from the internal meta-analysis.

### Emotional Information Seeking and Processing Seem Corroborated by a Socially Oriented Perspective

In the second hypothesis, it has been posited that learners would seek and process the emotional information available on screen depending on whose emotions were available on screen and whether a direct and persistent comparison was enhanced or not. More precisely, it has been stated that participants in the *Self-Centered* condition would seek less often and process for shorter time the information available through the perceiving-monitoring part of the interface compared to the *Partner-Oriented* and *Mutual-Modeling* conditions. It has also been posited that participants in the *Mutual-Modeling* condition would seek information more often, and process it longer compared to the *Partner-Oriented* condition due to the increased interest enhanced by direct and persistent comparison of emotional information.

Results corroborate the presence of an overall effect of the interface both on emotional information seeking and processing. For information seeking, the experimental condition yielded a generalized effect size [@olejnikGeneralizedEtaOmega2003] of `r apa_print(s1.anova.visit_count)$estimate$group`, whereas for information processing the effect consisted in `r apa_print(s1.anova.total_visit_duration)$estimate$group`. In both cases, thus, the experimental condition seems to account for a considerable amount of variation in the perceiving-monitoring function of emotional awareness, even though the confidence intervals are once again wide enough to include a small overall effect.

On a more fine-grained level, the differences between conditions only partially corroborated the directional hypothesis. Differences were detected, both for information seeking and processing, only in pairwise comparisons between *Self-Centered* vs. *Partner-Oriented* and *Self-Centered* vs. *Mutual-Modeling*, but not between *Partner-Oriented* vs. *Mutual-Modeling*. The difference for seeking emotional information are wider between the *Self-Centered* and the *Mutual-Model*, with en estimation of around 44 more visits in total, and confidence intervals spanning between 15 and 72 total visits. The difference between the *Self-Centered* and the *Partner-Oriented* conditions is lower and more uncertain, with an estimation of around 35 more visits in total, but confidence intervals between around 2 to 67 visits. On the contrary, for emotional information processing, the difference is wider between the *Self-Centered* and the *Partner-Oriented* conditions, with an estimation of around 40 additional seconds spent processing the graphical representation of emotions, and confidence intervals between around 10 and 71 seconds. The difference between the *Self-Centered* and *Mutual-Modeling* conditions was instead of around 32 additional seconds, with confidence interval between around 4 to 59 seconds. The effect seems therefore consistent for both seeking and processing emotional information, but the estimation too uncertain to determine whether the difference could be of any practical value.

Furthermore, the lack of a detectable difference between the *Self-Centered* and the more socially-oriented interfaces would have undermined the usefulness of an EAT, whereas its presence may be explained as a *simple* novelty effect. The *Partner-Oriented* and *Mutual-Modeling* condition convey information that the learner does not know, whereas in the *Self-Centered* condition the emotions are just a reminder of what the learner should already know. In this regard, compared to emotion expression for which the estimated mean of 12.5 emotions was considerable, the estimation for seeking and processing emotion information leave more doubts about potential intra-personal meaning-making extrapolated from disposing of one's own emotions on screen, even if the representation facilitates the perception of the emotional experience evolution. The more socially-oriented interfaces also presented a word cloud, which may have been perceived as a more interesting representation compared to an additional graph [@pinkerTheoryGraphComprehension1990].

The comparison between the *Partner-Oriented* and *Mutual-Modeling* interfaces has, on the other hand, deeper implications with respect to the *raison d'être* of an EAT, and it also represents a more severe test given that the two conditions are more similar [@mayoStatisticalInferenceSevere2018]. The lack of a discernible effect between the two social-oriented conditions, if confirmed with an equivalence test with increased power, may suggest that there is no additional value conveyed by direct and persistent comparison available on screen. But the phenomenon could also be explained by the fact that the additional value is gained without the need for further information seeking and processing. That is, participants in the *Mutual-Modeling* condition were able to compare their own and the partner emotion thorough the interface without having to look at the perceiving-monitoring zone of the EAT more often or longer, because they could get more information with the same effort. The eye-tracking measures alone cannot unravel whether the lack of a discernible effect is a positive or negative outcome with respect to the social-oriented hypothesis. In future studies, the hypothesis should first be corroborated by an equivalence test [@lakensEquivalenceTestingPsychological2018], and then also be assessed with the aid of self-reported measures about the perceived usefulness of direct and persistent comparison of learners' emotional experience.

On the other hand, the use of transitions as a more dynamic indicator to measure the perusing of emotional information corroborates an overall increased activity within the *Mutual-Modeling* condition where the perceiving-monitoring zone of the tool is implicated. Even though the difference in the exploratory model are not always statistically significant, it is possible to outline some patterns. For instance, the gaze-path between the expressing-displaying and the perceiving-monitoring part of the interface counted around 17 transitions forth, and 13 more transitions back in the *Mutual-Modeling* compared to the *Partner-Oriented* condition. This may suggest that the direct and persistent comparison was taken into account before and/or after expressing an emotion, which can be interpreted as a mutual-modeling activity in itself.

The more interesting path between the perceiving-monitoring part and that of the task, on the other hand, indicates more than 15 transitions (back and forth) in the *Mutual-Modeling* condition compared to the *Self-Centered*. This may also be considered an indicator of some mutual-modeling activity, even though this phenomenon is not corroborated by differences implicating the *Partner-Oriented* condition, which do not reach the significance threshold.

All things considering, thus, the experiment, even without stating any stable estimation on the effects, corroborates a favorable outlook towards the two more socially-oriented interfaces with respect to emotional information seeking and processing. Having the emotion of someone else on the interface produce at least the curiosity to seek that information and also process it, whether valuable meaning-making is extrapolated or not. Emotions of the partner also increase the number of transitions in the *Mutual-Modeling* condition where the perceiving-monitoring zone is implicated. This is consistent with the idea that emotionally-charged stimuli have a privileged access to attention [@broschPerceptionCategorisationEmotional2010; @poolAttentionalBiasPositive2015]. It also suggests that despite the limitation of the graphical representation of emotion available through the toolbox, those are considered interesting enough to be sought and processed. The development of better visualizations of the emotional information should therefore considered more confidently [@leonyProvisionAwarenessLearners2013; @derickEvaluatingEmotionVisualizations2017; @bersetVisualisationDonneesRecherche2018; @fritzRealTimeEmotionalAwareness2016]. At the same time, these measures can be assessed as a more hectic attitude towards the overall interface, with participants switching their gaze more erratically back and forth between zones. Under this assumption, the EAT would represent rather a distraction, considering once again the salience of emotional stimuli in calling for attention [@poolAttentionalBiasPositive2015; @broschAdditiveEffectsEmotional2011; @broschPerceptionCategorisationEmotional2010]. In this regard, it may be useful to perform a more detailed analysis on when the participants' gaze has transitioned into the perceiving-monitoring zone, for instance by using the expression of emotions from the partner as cues after which transitions should be looked for in a short period of time. This analysis has not been performed since the synchronization between the timing of the eye-tracker software and the toolbox was difficult to achieve. It will nevertheless be possible to synchronize the two timer in future studies.

## Conclusion

This chapter presented a detailed illustration of an empirical study investigating whether a different use of, and access to emotional information expressed and available through an EAT had an effect on the actual use of the EAT itself. $N=48$ participants, then reduced to $N=35$ mostly for technical reasons, were randomly assigned to three different interfaces of the EAT -- namely a *Self-Centered*, a *Partner-Oriented*, and a *Mutual-Modeling* interface -- which varied on how socially-oriented each interface was. The main assumption underlying the empirical investigation stated that the more socially oriented interface would yield a greater use of the EAT in terms of emotion expressed and emotional information seeking and processing. This assumption was not corroborated for the number of emotion expressed, and was only partially corroborated for emotional information seeking and processing, for which participants in the *Partner-Oriented* and *Mutual-Modeling* conditions sought and processed emotional information to a gretaer extent compare to the *Self-Centered* condition, but without a detectable difference between the two more socially-oriented conditions.

Despite the hypotheses of the study being rejected or only partially corroborated, most of the performance-based indicators about the use of the EAT were congruent with the main assumption. In most measures, the *Mutual-Modeling* interface yielded the greater use of the EAT, followed by the *Partner-Oriented*. Congruently with a inter-personal perspective on emotional expression [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], these results seem to corroborate the usefulness of an EAT as an affordance to share emotion with a partner and take the partner's emotions into account during a computer-mediated collaborative task. Participants seemed to show a genuine interest in seeking and processing emotional information available about the partner, and knowing that their emotions would be conveyed to the partner did not stop participants to express them. The presence of the partner's emotions on the EAT interface also resulted in a more *dynamic* gaze-path, with more transitions between the part of the interface dedicated to the task, and that dedicated to the monitoring-perceiving function of awareness. This fact seems to corroborate the usefulness of providing moment-to-moment emotional awareness, since the emotional information may be truly integrated as instrumental information to the task at hand [@buderGroupAwarenessTools2011; @dourishAwarenessCoordinationShared1992]. A word of caution is nevertheless in order, since the method by which transitions have been computed is not externally validated yet. Whether and when it will, transitions could represent a more adequate measure of integrated and dynamic information seeking and processing compared to the *static* number of visits and seconds spent inside an Area of Interest used in the directional hypotheses of the study. All things considering, thus, a moderate optimism is warranted about the usefulness of an EAT during a computer-mediated collaborative task. Despite the fact that it does not directly provide information about the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013], sharing emotions could nevertheless sustain the mutual-modeling activity by which learners build and update a holistic representation of their partner in the collaboration [@dillenbourgSymmetryPartnerModelling2016; @sanginFacilitatingPeerKnowledge2011; @molinariKnowledgeInterdependencePartner2009].

At the same time, participants in the *Self-Centered* condition also seemed to harness the presence of the EAT, which is congruent with an intra-personal usefulness in expressing emotions [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018]. Even though participants in this condition knew beforehand that they were the exclusive sender and receiver of the emotional information, they still expressed emotions as well as sought and processed their own emotional information available on the EAT. This fact suggests that the presence of an EAT may prompt learners to inquiry about their own emotional state, appraising the situation and seeking for the congruent subjective feeling elicited by the circumstances [@boehnerHowEmotionMade2007; @grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009].

### Limitations and Future Development

The present contribution adopted a controlled environment in order to expose every participant to the same stimuli, except for the randomly assigned interface. The use of a simulated partner limited the inter-personal communication that would be normally available in a real collaborative setting. For the purposes of the study, a distinction between cooperation (roughly, doing the same thing with limited interdependence) and collaboration (integrating efforts into a common outcome) was not of primary concern. Nevertheless, this certainly represents a limitation to the generalization of the obtained results to a more articulated communication flow, which could overlap with the emotional information expressed through the EAT. For instance, learners could have used the text area dedicated to their reasoning to inject circumstantial information such as *I don't understand* or *I don't agree with you*, which would have conveyed social, cognitive and emotional information [@derksRoleEmotionComputermediated2008]. A focal question avoided by the present experimental setting is therefore whether participants would still have used the EAT the way they did, were they allowed to convey emotional information through the content space of the joint problem-solving activity. The task at hand, though, can be easily extended to a real collaboration between two participants, and therefore it would be possible to investigate the matter in a way that can be directly related to the results of this contribution. Rather than for its result, this experiment may be more relevant for its design: testing different versions of an EAT, where differences have also theoretical or pedagogical implications. The same design can be progressively enriched, first by the same task, but with real collaboration, and then by introducing tasks that are closer to authentic Computer-Supported Collaborative Learning activities.

With respect to an hypothetical next step consisting in a direct replication with real collaboration, the comparison with present results would have to take the difference in time and setting into account. In this regard, a possible solution is to retain a ratio between the duration of the joint-problem solving task and the performance-based measures of the use of the EAT. For example, participants in a dyad that solved the four enigmas in 12 minutes and expressed 18 emotions for one participant and 24 for the second would have a ratio of 1.5 and 2 respectively. Taking into account the temporal dimension would also add another element of interest: investigate whether the ration holds constant across different durations of the joint problem-solving task, or it yields a moderation effect.

Furthermore, the focal question of whether learners would use the EAT having a more thorough channel of communication could also be assessed by allowing learners to display or not the EAT on their screen. This choice may also be instrumental in investigating one of the main assumptions of the thesis, namely the interest for moment-to-moment emotional awareness: whether and when participants would decide to display the EAT may convey pivotal evidence about the usefulness of moment-to-moment emotional awareness compared, for instance, to a *scripting* strategy in which partners share their emotions at specific intervals outside the task.

### Experiment's Overall Contribution

To sum up, in spite of the technical difficulties and the limited external validity, the experiment provided contributions that may be retained for future studies. First, breaking down the abstract model of emotional awareness can be a useful guideline to set up experiments. Second, the use of transitions between Areas of Interest can be maintained as a sensible, reliable and valid measure for a genuine interest in perusing emotional information through the EAT. Third, the specific measures provided by the toolbox can be analysed in different ways, empowering the investigation of emotional awareness.

### Acknowledgments

I would like to thank Stéphanie Perrier who contributed to the collection of shared data with her Master thesis [@perrierCollaborationEnvironnementMediatise2017], which Mireille Bétrancourt and I co-directed.