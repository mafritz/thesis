---
bibliography: references.bib
---

# Does a Different Use of and Access to Emotional Information Change the Concrete Use of the Emotion Awareness Tool? {#study-1}

```{r s1-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(here)
library(knitr)
library(kableExtra)

# Load the relevant data and graphs from the study-1 folder
source(here("data", "study-1", "s1-export.R"), local = knitr::knit_global(), encoding = "UTF-8")
```

In this first empirical contribution, the toolbox is adopted in order to investigate whether a different use of and access to emotional information has an impact on the concrete use of the Emotional Awareness Tool in its basic functions: expressing emotions, and perusing the emotional information available. The main objective of the experiment is to break down the overall abstract model of emotional awareness described in Section \@ref(abstract-model-of-ea) into three different *paths*, corresponding to three different interfaces configured through the toolbox: a Self-Centered, a Partner-Oriented and a Mutual-Modeling interface. Using a simulated synchronous and collaborative task -- as in the usability test of the Dynamic Emotion Wheel

## Study Rationale

In the few experimental contributions that have investigated emotional awareness in synchronous computer-mediated collaboration so far [*e.g.*, @avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013], the experimental design consisted in comparing a control group, who did not dispose of emotional awareness, with a *treatment* group, who disposed of emotional awareness in *full strength*. That is, either the person had no emotional awareness at all, or she had access to the full *abstract model* depicted in Section \@ref(abstract-model-of-ea): intra-individual and inter-individual awareness, and also the comparison between the two. The *treatment* setting is therefore consistent with the mutual-modeling perspective [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009], according to which the symmetry of the information available to learners is instrumental to build and update a holistic representation of the partner, upon which the collaborative effort can strive.

As the abstract model has shown, though, emotional awareness can be broken down to a series of passages that, even though tightly related, may have different reasons for, and consequences on the use of an EAT. At the very least, it is possible to identify the intra-personal and the inter-personal *paths*, which are underpinned by two different assumptions about the instrumentality of emotional awareness (see Sections \@ref(ea-intra-personal) and \@ref(ea-inter-personal)). In this regard, for instance, @molinariEmotionFeedbackComputermediated2013 reckon that "one main limitation of [their *control vs. treatment*] study is the difficulty in disentangling the effect of reflecting upon one's own emotions from the effect of sharing one's emotions with the partner" [@molinariEmotionFeedbackComputermediated2013, p.342]. As a consequence, it may be worth investigating the use of different configurations of an EAT, which may vary, for instance, according to (1) the use of the emotional information that is produced, and (2) the access to the emotional information that may be perused through the tool. As suggested by @buderGroupAwarenessTools2011, varying the characteristics within an awareness tool may complement the assessment of its specific contribution alongside the all-or-nothing approach of the *control vs. treatment* design.

One way to guide the configuration of different versions of an EAT is to use the abstract model of the functions of emotional awareness and imagine it is a sort of *pipeline* in which the *flow* of emotional awareness circulates. Each passage from one main concept (*i.e.* the learning task, the intra-indiviudal or inter-individual emotion, and the meaning-making extrapolated from emotional information) can be seen as a pipe, whose flow can be blocked, decreased, or increased according to the configuration of the EAT. Figure \@ref(fig:conditions-ea-model) shows this concept graphically, with three different configurations of the *pipeline*, identified respectively as Self-Centered, Partner-Oriented, and Mutual-Modeling.

(ref:conditions-ea-model-caption) Comparison of three different *pipelines* in the abstract model of emotional awareness illustrated in Section @ref(abstract-model-of-ea): the Self-Centered, the Partner-Oriented, and the Mutual-Modeling *paths*. The *flow* in a pipe can be blocked (x), decreased (--), or increased (+), depending on the configuration of the the EAT's interface.

```{r conditions-ea-model, fig.align="center", fig.cap="(ref:conditions-ea-model-caption)", out.width="30%", fig.show="hold"}
knitr::include_graphics(
  c(
    here::here("./figure/s1/self-centered-model.png"),
    here::here("./figure/s1/partner-oriented-model.png"),
    here::here("./figure/s1/mutual-modeling-model.png")
  )
)
```

In the Self-Centered *path*, all the passages related to the inter-personal emotion edge are blocked, whether incoming or outgoing. Conceptually, this means that the learner is provided only with emotional self-awareness, since the emotion expressed by each participant to the collaborative task remain *private*: the emotional information is not disclosed to others, and therefore each learner peruse only the emotional information she has produced.

In the Partner-Oriented *path*, all passages remain open, but the *flow* is reduced in passages 2 (from intra-personal emotion to meaning-making) and 6 (from inter-personal to intra-personal emotion). Conceptually, this means that the emotion expressed by the learner are disclosed to the partner, but they do not persist on the interface of the learner herself as a means to foster intra-individual meaning-making. The only available emotional information to peruse for the learner consists in the emotions of the partner. This entails that direct and persistent comparison between the partner's emotions and that of the learner is considerably reduced (*i.e.*, it would depend only on the learner memory of her emotional experience over time).

Finally, in the Mutual-Modeling *path*, all the passages remain open, and the *flow* between the intra-personal and inter-personal emotion edges is *increased* by the direct and persistent comparison between the emotional experience of the learner herself and that of the partner. Conceptually, this entails a total symmetry between the emotional information available to both partners, which is consistent with the mutual-modeling activity through which the holistic representation of the partners is built and updated [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009].

The analogy of the pipeline is also relevant to address two important issues in the study of emotional awareness when the inter-personal perspective is implicated. First, the overall *flow* is influenced by how much information is produced by each learner. That is, the more emotions are shared by the learner, the more the partner will dispose of emotional information to peruse. At the same time, the more emotions learner share respectively, the more the symmetry will be quantitatively and qualitatively achieved. It could in fact be the case that even if learners dispose of the Mutual-Modeling interface, one shares a lot of emotions and the other none. Second, as indicated by @janssenCoordinatedComputerSupportedCollaborative2013, the content and the relational space of a computer-mediated collaboration overlap (see Figure \@ref(fig:janssen-bodemer-framework) in Section \@ref(mutual-modeling)). In other words, it is possible to infer emotional information that comes from outside the EAT, and also produce emotional information using other sources than the EAT. For instance, one can infer that a colleague who is not writing in a shared document, but who follows with the cursor the new lines added, is probably uneasy about how to contribute. Thus, if one is interested in the investigation of the specific contribution of an EAT, other sources of emotion-related information shall be accounted for or controlled in some way. In order to address both issues, the contribution will therefore adopted a simulated collaborative setting as the one adopted in the usability test of the Dynamic Emotion Wheel illustrated in Section \@ref(dew-ux-test). This will expose all participants to the same *content* of the collaboration, whereas the *relational* information will be different with respect to the emotional information available.

## Research Question and Hypothesis

The main aim of the experiment is therefore to manipulate the abstract model of the functions of emotional awareness and determine what does it happen -- holding all other sources of potential variation in the emotional information constant -- if the *flow* of emotional information is blocked, reduced, or increased in specific parts of the model. Depending on the use that would be made of the emotions expressed, as well as to what kind of emotional information learners have access to, there are different reasons to *encode* and *decode* emotional information into and from the EAT. This is all the more relevant with a self-reporting and moment-to-moment use of an EAT. As stated by the Component Process Model [@schererWhatAreEmotions2005; @scherer2019; @schererDynamicArchitectureEmotion2009] for the intra-individual level, and by the Emotion As Social Information model [@vankleefEmergingViewEmotion2010; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018] for the inter-individual, producing and consuming emotional information implicate a cognitive effort when the process happens at the conceptual and inferential levels. This effort must be evaluated according to the trade-off between the cost of producing or perusing the emotional information on the one hand, and the benefit of disposing or making the others dispose of emotional awareness as instrumental information to the task at hand [@pashlerDualtaskInterferenceSimple1994; @buderGroupAwarenessTools2011; @dillenbourgSymmetryPartnerModelling2016].

In this regard, one way to look at the three different *flows* of emotional awareness is to consider that, starting from the exclusively intra-individual *flow* in the Self-Centered, the Partner-Oriented and the Mutual-Modeling *flows* pile up additional layers that are more oriented towards a social, inter-individual function of emotional awareness. Figure \@ref(fig:affective-processes-per-condition-caption) shows this mechanism graphically by mobilizing concepts illustrated in Chapter \@ref(eat-general-chapter).

(ref:affective-processes-per-condition-caption) Theoretical concepts mobilized by versions of the EAT differing in the use of, and access to emotional information. Concepts are listed in alphabetic order.

```{r affective-processes-per-condition, fig.align="center", fig.cap="(ref:affective-processes-per-condition-caption)", out.width="60%"}
knitr::include_graphics(here::here("figure/s1/s1-theory-diagram.png"))
```

In the Self-Centered condition, learners will be oriented towards their own emotional experience [@molinariEMORELOutilReporting2016; @lavoueEmotionAwarenessTools2020]. This translates, for instance, in an accurate appraisal of the situation, which is nevertheless not influenced by communication concerns [@schererComponentialEmotionTheory2007; @scherer2021]. Another important part in a Self-Centered orientation is represented by emotion self-regulation [@grossEmotionRegulationCurrent2015; @grossHandbookEmotionRegulation2014]: expressing and perusing one's own emotions through the EAT can enhance self-reflection on the emotional experience, and therefore strategies to maintain functional emotions and modify dysfunctional ones. In this regard, the use of an EAT linking appraisal dimensions and subjective feelings can enhance a form of regulation through *affect labeling* [@torrePuttingFeelingsWords2018; @liebermanPuttingFeelingsWords2007; @liebermanBooConsciousnessProblem2019], according to which the process of *naming* an emotion with a term entails an implicit regulation of that same emotion.

In the Partner-Oriented *flow*, learners will be concerned by the disclosure of their emotion to the partner and by taking the emotions of the partner into account. All the implications of the Self-Centered condition are maintained, but also implemented by the fact that the emotional awareness emerge as a bi-directional communicative act. It therefore entails all the considerations regarding the social-sharing of emotion [@rimeEmotionElicitsSocial2009; @parkinsonEmotionsDirectRemote2008; @fischerSocialFunctionsEmotion2016; @vankleefInterpersonalDynamicsEmotion2018]. If a form of emotion regulation is implied, that could also be from an inter-personal perspective [@zakiInterpersonalEmotionRegulation2013; @netzerInterpersonalInstrumentalEmotion2015; @reeckSocialRegulationEmotion2016]. Finally, the emotion expressed by the partner can become a trigger for an emotion in the learner, that is, an inter-personal meta-emotion [@miceliMetaemotionsComplexityHuman2019; @normanConceptMetaemotionWhat2016].

Finally, in the Mutual-Modeling *flow*, all the previous concepts coalesce, with the addition of direct and persistent comparison of the evolution of the emotional experience in the learner and in the partner [@avrySharingEmotionsContributes2020; @eligioEmotionUnderstandingPerformance2012; @molinariEmotionFeedbackComputermediated2013]. This visuo-spatial comparison enhance the symmetry of the partner modeling [@dillenbourgSymmetryPartnerModelling2016; @molinariKnowledgeInterdependencePartner2009; @sanginPeerKnowledgeModeling2009], which is considered a pivotal mechanism for inter-subjective meaning making in collaborative settings [@jarvelaEnhancingSociallyShared2015; @kirschnerAwarenessCognitiveSocial2015; @phielixGroupAwarenessSocial2011; @janssenCoordinatedComputerSupportedCollaborative2013].

The main research question in this experiment consists thus in assessing whether a different use of and access to emotional information changes the concrete use of an emotion awareness tool. The use of the tool is determined by the two fundamental functions of awareness tools: the expressing-displaying function (producing awareness) and the perceiving-monitoring function (perusing awareness). For each function, specific hypothesis and causal mechanisms are depicted hereafter.

### Use in Expressing Emotions

For expressing-displaying emotions, the three interfaces provide the learner with different reasons to express how she feels, as well as different affective triggers that may elicit emotional episodes (see also the hypothesis about the use in perceiving emotions below). More precisely:

-   With the *Self-Centered* interface, the learner knows she is the only recipient of the emotions she expresses. Therefore, if she decides to use the EAT to express an emotion, she probably does it out of self-interest, possibly linked to self-regulation as stated by the *intra-personal* perspective. In the meantime, the EAT does not provide any additional information about the partner's emotions that may serve as trigger for emotional episodes in the learner herself.
-   With the *Partner-Oriented* interface, the learner knows she will not have access to her own emotions once she has expressed them, but that these are conveyed to the partner. Therefore, if she decides to use the EAT to express an emotion, one can assume that she does it from an *inter-personal* perspective (even if the possibility that she does it exclusively in a *Self-Centered* perspective cannot be excluded). In the meantime, the learner can also access the partner's emotions, which may represent additional triggers for meta-emotional episodes (*e.g.*, *Jane expresses guilt because she thinks Paul has just expressed anger as a result of something she has done*).
-   With the *Mutual-Modeling* interface, the learner knows the emotions she expresses are available both to her and the partner. Therefore, if she decides to use the EAT to express an emotion, she does in a *Self-Centered*, *Partner-Oriented*, or a combined perspective. In the meantime, the learner also disposes of direct and persistent comparison between her own emotions and that of the partner, which may also represent an additional trigger for emotional episodes compared to the *Partner-Oriented* interface (*e.g.*, *Jane expresses relief because she saw from the interface that in the last few minutes both she and Paul were often confused*).

Hypothesis (*H1*) is therefore stated in the following terms: there will be an overall difference in the use of the EAT for expressing-displaying emotions depending on the interface the learner has at disposal. More specifically, in comparing the interfaces, a greater use of the expressing-displaying function of the EAT in the *Partner-Oriented* and *Mutual-Modeling* interfaces compared to the *Self-Centered* interface would corroborate an *inter-personal* interest in expressing emotions. Furthermore, a greater number of emotions expressed in the *Mutual-Modeling* interface compared to the *Partner-Oriented* condition would suggest that the possibility of direct and persistent comparison between one's own and the partner's emotions results in a *surplus* of expression-displaying of emotions.

### Use in Perceiving Emotions

With respect to perceiving emotions, the three interfaces differ in the quality and quantity of the emotional information available on screen. The three interfaces will provide the learner with different reasons to seek and process the emotions expressed during the collaboration:

-   With the *Self-Centered* interface, the learner has access only to the emotions she has expressed over time during the collaboration. This may be interpreted as a *control* condition: what is the interest of having emotional information that the learner is already supposed to know? Seeking and processing the learner's own emotions may be explained by the interest of reflecting on the evolution of her own affective states during the task.
-   With the *Partner-Oriented* interface, the learner has access only to the emotions expressed by the partner, that is, information she does not already know. Seeking and processing the partner's emotions may be explained by the interest in knowing how the other is feeling and/or the evolution of the affective states of the partner during the collaboration.
-   With the *Mutual-Modeling* interface, the learner has access both to her own and the partner's emotions. This condition inserts an additional interest to the previous ones: the possibility of direct and persistent comparison between the learner's own emotions and that of the partner.

Hypothesis (*H2*) is therefore posited as follows: there will be an overall difference in the use of the EAT for seeking and processing the expressed emotions depending on the interface the learner has at disposal. More specifically, the *Partner-Oriented* and *Mutual-Modeling* interfaces will elicit a greater use in perceiving-monitoring emotions compared with the *Self-Centered* interface. Furthermore, greater information seeking and processing in the *Mutual-Modeling* interface compared to the *Partner-Oriented* interface would suggest an accrued interest due to direct and persistent comparison.

## Methods

Following @simmons21WordSolution2012 suggestion to increase transparency in experimental contributions, I report how I determined the sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

### Participants and Design

48 participants (29 women, 19 men), aged 18 to 55 ($M_{age}=37.3$, $SD_{age}=10.01$), voluntarily participated to the study. The sample size was determined by time constraints, since data had to be collected in 15 days in co-located conditions. 23 participants were university students from different faculties, both at undergraduate and graduate levels. 25 participants were professionals working for a company adopting distance learning practices. No remuneration was provided for taking part in the study. Participants were randomly assigned to one of the three conditions/interfaces (*Self-Centered*, *Partner-Oriented*, or *Mutual-Modeling*) in order to produce a balanced design with 16 participants per condition.

### Material

#### Overall Interface of the Task

The experimental material comprises different components; I therefore provide an overview before specifying the various details. Figure \@ref(fig:s1-task-interface-figure) shows the disposition of the screen during the experimental task. It comprises the EAT on the left-side of the screen, outlined in blue, and the simulated, joint-problem solving task on the right. The image indicates what part of the interface was simulated for the *Mutual-Modeling* condition. The interfaces of the other conditions are illustrated below. Some parts of the interface have been translated in English for the current contribution. In the experiment, though, the french language was used consistently for every condition.

(ref:s1-task-interface-caption) Overview of the interface that presents the various part of the material adopted for the CSCL task.

```{r s1-task-interface-figure, fig.cap="(ref:s1-task-interface-caption)", fig.align="center", out.width="100%" }
include_graphics(here("figure/s1/s1-task-interface.png"))
```

#### Problem-Solving Task

The joint problem-solving task comprised four enigmas taken from a game. The first three enigmas had a clear response that could be inferred, whereas the last one was more of a non-nonsensical type. The same enigmas have been used in the usability test of the Dynamic Emotion Wheel [@fritzReinventingWheelEmotional2015], where they elicited different emotions both in number and kind in a population similar to that of the current sample (see Section \@ref(ux-reference-measures)). Each enigma was subdivided in three phases:

-   40 seconds during which the text of enigma was showed on the interface. At this stage, participants could only express their emotions, but could not write their reasoning or the reply;
-   3 minutes and 20 seconds during which participants could write their reasoning and reply to the enigma, as well as see the *playback* reasoning (but not answer) of the simulated partner;
-   1 minute in which the given answers from the participant and the partner were displayed on screen with the expected solution to the enigma. At this stage, once again, the reasoning and reply fields were not available on screen.

#### Configuration of the Emotion Awareness Tool

The DEW toolbox was adopted to configure the tool with the desired features. The expressing-displaying function was common to all conditions, which differed only in the perceiving-monitoring part of the interface.

For expressing an emotion, the Valence and Control/Power dimensions [@schererGRIDMeetsWheel2013; @schererWhatAreEmotions2005] determined the appraisals for the expression-displaying of emotional episodes. Valence was prompted with the question "*Is the situation pleasant?*", whereas Control/Power with the question "*Is the situation under your control?*". Both evaluations were determined with the extreme negative pole *Not at all*, and extreme positive pole *Yes, absolutely*. The underlying affective space was represented by EATMINT circumplex depicted at lenght in Section \@ref(eatmint-circumplex).

For the perceiving-monitoring function of the EAT, each condition differed in the following ways (depicted in Figure \@ref(fig:s1-eat-interfaces-figure)):

-   *Self-Centered*: the interface comprises an emotion time-line, then a graphic line chart that depicts the evolution of the appraisal dimensions over time, and finally a tag cloud where the size of each subjective feelings is proportional to the frequency with which it has been expressed. The information provided is based only on the emotions expressed by the participant herself.
-   *Partner-Oriented*: the interface is the same as in the *Self-Centered* condition, but the information provided is based only on the emotions expressed by the simulated partner (see below).
-   *Mutual-Modeling*: the interface comprises an emotion time-line, but with both the participant and the simulated partner's subjective feelings organized in two different rows. Two line charts complete the interface, one with the appraisal dimensions of the participant, and the other of the partner.

The *Self-Centered* and *Partner-Oriented* conditions present a tag cloud at the end of the interface in order to balance the surface of the EAT that contains information. In this way, the EAT occupies more or less the same amount of the screen.

(ref:s1-eat-interfaces-caption) The three different interfaces used in the experiment. From left to right: the *Self-Centered*, the *Partner-Oriented*, and the *Mutual-Modeling* versions.

```{r s1-eat-interfaces-figure, fig.cap="(ref:s1-eat-interfaces-caption)", fig.align="center", out.width="100%" }
knitr::include_graphics(here::here("figure/s1/s1-eat-interfaces.png"))
```

#### Simulated partner

```{r s1-simulated-partner}
s1.simulated_partner <- s1.dew_configuration$task$simulation %>%
  select(-user) %>%
  filter(time <= 1200) %>%
  left_join(s1.feelings_translation, by = c("feeling" = "label"))
```

The *playback* manipulations displayed on the interface were recorded in the same pilot test as for the usability assessment of the DEW already mentioned above. 4 confederates (2 men and 2 women) performed the same joint problem-solving task, but in a synchronous situation. The *playback* is thus comprised by: (1) all the emotional episodes expressed, represented by the evaluation on the two appraisal dimensions and the related subjective feeling; and (2) what the confederate have typed, at the very same moment, into the reasoning field, as well as the answer to each of the 4 enigmas. In this regard, confederates were explicitly asked not to communicate directly through the text fields, but limit their typing to the reasoning for solving the problem. One of the *playback* was then randomly chosen for the task and *injected* into the experimental task interface. The simulated partner expresses `r s1.simulated_partner %>% nrow()` emotions and finds the solution to 2 out of 4 enigmas. The full list of emotions -- comprising the time of expression, the associated Valence and Control/Power appraisals, and the subjective feeling -- are depicted in Table \@ref(tab:s1-simulated-emotions-table).

(ref:s1-simulated-emotions-caption) List of the emotions of the simulated partner. Time is expressed in seconds.

```{r s1-simulated-partner-table}
kable(s1.simulated_partner,
  col.names = c("Time", "Valence", "Control", "Feeling (FR)", "Feeling (EN)"),
  caption = "(ref:s1-simulated-emotions-caption)\\label{tab:s1-simulated-emotions-table}",
  caption.short = "Study 1: Simulated Partner's Emotions",
  longtable = FALSE,
  booktabs = TRUE
)
```

#### Eye-tracking

A Tobii T120 eye-tracker with Tobii Studio Pro v3.4.8 software [@tobiiabTobiiStudio2015] was used for eye-tracking measures. Areas Of Interest (AOI) were disposed on the EAT as a whole (left side of the screen) and the task (right side of the screen). The AOI of the EAT was further divided in the expressing-displaying upper zone, and the perceiving-monitoring lower zone.

### Procedure

Participants were given a specific time to come to the test, which was performed at Geneva University, and were reminded of the importance to be on time since another participant was performing the test in the meantime. The experimenter welcomed the participant in the room with the eye-tracking equipment. Once installed, the experimenter proceeded to explain the outline of the study:

-   Introduction and explication (10 minutes)
-   Warm-up session with the DEW and instructions for the task (5 minutes)
-   Collaborative task (20 minutes)
-   Debriefing (10 minutes)

#### Introduction and explication

The general aim of the study was explained. The experimenter reassured participants about the fact that the data would be anonymous, and that they could stop the experiment at any time without any reason. A first consent form was then signed if the participant agreed to take part in the study.

At this point, the experimenter explained how the collaborative task would take place. She first showed a demo about the functioning of the DEW. Since in a previous study [@fritzReinventingWheelEmotional2015], whose aim was to observe the spontaneous use of the tool, participants were confused about the dimension of Control/Power, in this study the experimenter proposed a more thorough explanation of what the two sliders of the DEW stand for. The explication also aimed at reducing the risk that participants will move the cursors for the Valence and Control/Power dimensions until they found the *right* subjective feeling. Next, the experimenter showed the perceiving-monitoring part of the EAT, which was explained according to the experimental condition the participant was attributed to. In this way, participants were informed about both what the emotional information they provided would be used for (*Self-Centered* vs *Partner-oriented/Mutual-modeling*), and to what kind of emotional information they would have access to (*Self-Centered* vs *Partner-Oriented* vs *Mutual-Modeling*).

Finally, the experimenter explained the right-hand side of the screen, which implemented the joint problem-solving task. Participants were informed about the three parts (reading, solving and solution) that composed each of the 4 enigmas to solve. They were also prompted to write their reasoning to solve the problem in the appropriate field, but to avoid direct communication with the partner. Since in the usability test of the DEW participants spontaneously expressed emotions and participated to the task, the system of points adopted to enhance collaboration was dropped.

#### Warm-up session with the DEW

Participants were placed in front of the screen used for the test and could practice with a simplified version of the interface for the task. Participants could familiarize with the expression of emotions through the DEW, and random emotions were also injected in the interface at short intervals to emulate the emotion of the partner. The side of the screen devoted to the task was filled in with generic texts explaining what participants will see in the actual task (*e.g.*, *here will appear the text of the enigma*, *here you must write your reasoning*, ...)

#### Experimental task

Once the participant was ready for the test, the experimenter simulated to check-in with another confederate to simulate that the other participant was ready to start the experiment as well. Then the experimenter proceeded to calibrate the eye-tracker equipment. After being reminded about the general functioning of the eye-tracker and the importance of not moving during the task, the participant would then proceed with the task. At first, she had to fill the *log-in form* of the front-end interface of the toolbox, providing a random ID and an identifier for the pair. Once the task started, the participant had access to the overall interface depicted above, including the *playback* of all the manipulations made by a confederate.

#### Post-test debriefing

After the task, participants were asked to fill in a survey whose data that will not be used in the present contribution, but have been analyzed by Perrier [-@perrierCollaborationEnvironnementMediatise2017]. At the end of the study, participants were informed about the manipulation of the simulated partner and the experimental reasons behind it. A second consent form was therefore submitted to participants, for them to confirm they understood the reason of the manipulation, and that they accepted the use of the data.

### A Priori Exclusion Criteria

Exclusion criteria determined beforehand concerned only technical issues that could jeopardize the task, especially with respect to the simulated partner. Any interruption of the task or technical failure would make the trial not recoverable. Exclusion caused for low quality of eye-tracking measures, due for instance to the participant moving too much, were also foreseen, but not yet quantified due the lack of a precise benchmark.

### Data analysis

For hypothesis *H1*, concerning differences in expression of emotions, a omnibus one-way ANOVA with pairwise comparison between all conditions was planned beforehand. The number of emotions expressed through the EAT represents the dependent variable, and the interface of the EAT the independent variable.

For hypothesis *H2*, concerning differences in perception of emotions, two indicators retrieved by eye-tracking measures [@blascheckVisualizationEyeTracking2017; @pooleEyeTrackingHumanComputer2005] are used as dependent variables. First, the total time (in seconds) that participants spent looking at the perceiving-monitoring zone of the interface. Such indicator is usually interpreted as a proxy for information processing and could account for interest (*i.e.*, people look at it longer because it is interesting) or complexity (*i.e.*, people look at it longer because they need more time to understand what it means). Second, the number of times participants sought information by orienting their gaze inside the perceiving-monitoring zone of the interface, which is usually interpreted as an indicator that the person intentionally seeks for information she may find useful or that she got lost and needs reorientation. Given the relative simplicity of the information provided -- even though people do not like graphs [@carpenterModelPerceptualConceptual1998; @pinkerTheoryGraphComprehension1990] -- and the use of a fixed interface of the EAT, both measures are used as indicators of interest. For both measures, a omnibus one-way ANOVA with pairwise comparison between all conditions were planned beforehand. A family-wise correction to account for inflation in Type I error has been planned for each pairwise comparison.

All analysis are conducted using the statistical software R, version 4.2.0. Analysis of variance use the Afex package version `r packageVersion("afex")` [@singmannAfexAnalysisFactorial2020].

## Results

### Post-Hoc Exclusion

Results will be based on $N = 35$ participants. 10 participants were excluded due to technical issues during the task or low quality of eye-tracking measures. One participant was excluded for statistical reasons: the participant expressed 62 emotions during the task, against a mean of `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)`, that is, more than 8 standard deviations above the mean. Such a number, not even close to any participant to the same task in Fritz (2015), suggests a non representative use of the tool. The distribution of participants after post-hoc exclusions with respect to the experimental conditions is depicted in Table \@ref(tab:s1-observed-design-table).

(ref:s1-observed-design-capture) Number of participants retained for each experimental condition ($N = 35$).

```{r s1-observed-design}
s1.participants_repartition <- s1.et_data %>%
  group_by(group) %>%
  summarise(
    n = n()
  )

kable(s1.participants_repartition,
  col.names = c("Condition", "N"),
  caption = "(ref:s1-observed-design-capture)\\label{tab:s1-observed-design-table}",
  caption.short = "Study 1: Observed Experimental Design",
  longtable = FALSE,
  booktabs = TRUE,
) %>% 
  kable_styling(latex_options = "HOLD_position")
```

The resulting unbalanced design and overall small $N$, particularly low in the *Partner-Oriented* condition, decrease the power of an already ambitious planned test and make it more exposed to violation of assumptions [@hoekstraAreAssumptionsWellKnown2012]. The interpretation of results from an hypothesis-testing, inferential perspective is therefore maintained only if basic assumptions of the model are met. They are also provided more from an exploratory perspective, such as a reference for future studies, rather than a confirmatory standpoint [@scheel2020; @fiedlerCycleTheoryFormation2004; @lakensTooTrueBe2017].

### Differences in Expressing Emotions

Participants expressed a total of `r s1.dew_emotions %>% nrow()` emotions, which corresponds to a mean close to 14 emotions per participants (`r maf.print_m_sd(s1.aggregated_emotions$n, TRUE, FALSE)`). Participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions; `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Partner") %>% pull(n), FALSE, TRUE)` in the *Partner-Oriented* condition; and `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Mutual") %>% pull(n), FALSE, TRUE)` in the *Mutual-Modeling* condition (see Figure \@ref(fig:s1-expressed-emotions-graph)).

(ref:s1-expressed-emotions-graph-caption) Number of emotions expressed by experimental condition. Bars represent 95% confidence intervals.

```{r s1-expressed-emotions-graph, fig.cap="(ref:s1-expressed-emotions-graph-caption)", fig.align="center", out.width="60%"}

maf.plot_means_comparison(s1.aggregated_emotions, aes(x = group, y = n, color = group)) +
  labs(x = NULL, y = "Number of emotions") +
  theme(legend.position = "none")
```

Assumptions check for the ANOVA model highlighted major discrepancies in the distribution of residuals, suggesting violation of the assumption of normality, as illustrated in Figure \@ref(anova-expressed-emotions-assumptions-caption) The results of the omnibus ANOVA and of the individual comparisons are therefore not provided.

(ref:anova-expressed-emotions-assumptions-caption) Assumptions check on the ANOVA model for expressing emotions revealed violation of the normality of residuals assumption. The test of hypothesis H1 is therefore withdrawn.

```{r anova-expressed-emotions-assumptions, fig.align="center", fig.cap="(ref:anova-expressed-emotions-assumptions-caption)", out.width="90%"}
s1.anova.expressed_emotions.assumptions
```

### Differences in Perceiving Emotions

The perception of the emotional information is divided in emotional information processing and emotional information seeking.

#### Processing Emotional Information

Participants spent on average `r maf.print_m_sd(s1.total_visit_duration$value, TRUE, TRUE)` seconds looking at any part of the perceiving-monitoring zone of the interface, which amounts to 4.28% of the total task time. Participants in the *Self-Centered* condition spent `r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)` seconds, whereas this time roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.total_visit_duration %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which time differed slightly (see Figure \@ref(fig:s1-total-visit-duration-graph)).

(ref:s1-total-visit-duration-graph-caption) Total time (in seconds) spent looking at the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-total-visit-duration-graph, fig.cap="(ref:s1-total-visit-duration-graph-caption)", out.width="60%", fig.align="center"}
maf.plot_means_comparison(s1.total_visit_duration, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Seconds") +
  theme(legend.position = "none")
```

An overall effect of the experimental condition on the time spent processing emotional information could be observed (`r apa_print(s1.anova.total_visit_duration)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-total-visit-duration-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-total-visit-duration-caption) Pairwise comparisons of the thee interfaces with respect to total time spent looking at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-total-visit-duration-contrats}
s1.anova.total_visit_duration.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-total-visit-duration-caption)\\label{tab:s1-total-visit-duration-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Total Visit Duration.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

#### Seeking Emotional Information

Participants' gaze entered the perceiving-monitoring zone of the interface on average `r maf.print_m_sd(s1.visit_count$value, TRUE, TRUE)` times. In the *Self-Centered* condition, the number of visits has been `r maf.print_m_sd(s1.visit_count %>% filter(group == "Self") %>% pull(value), FALSE, TRUE)`, whereas the count roughly doubles in the *Partner-Oriented* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Partner") %>% pull(value), FALSE, FALSE)`) and the *Mutual-Modeling* (`r maf.print_m_sd(s1.visit_count %>% filter(group == "Mutual") %>% pull(value), FALSE, FALSE)`) conditions, for which the count was similar (see Figure \@ref(fig:s1-visit-count-graph)).

(ref:s1-visit-count-graph-caption) Total visits count in the perceiving-monitoring zone of the interface. Bars represent 95% confidence intervals.

```{r s1-visit-count-graph, fig.cap="(ref:s1-visit-count-graph-caption)", out.width="60%", fig.align="center"}

maf.plot_means_comparison(s1.visit_count, aes(x = group, y = value, color = group)) +
  labs(x = NULL, y = "Number of visits") +
  theme(legend.position = "none")
```

An overall effect of the interface adopted on emotional information seeking could be detected (`r apa_print(s1.anova.visit_count)$full_result` in a one-way ANOVA). Pairwise comparisons, depicted in Table \@ref(tab:s1-visit-count-comparison-table), confirm detectable differences between the *Self-Centered* vs *Partner-Oriented*, and *Self-Centered* vs *Mutual-Modeling* conditions, but not between the *Partner-Oriented* and *Mutual-Modeling* conditions. Hypothesis (*H2*) is therefore partially corroborated: the overall effect is detected, but with only two out of three comparisons between conditions.

(ref:s1-visit-count-caption) Pairwise comparisons of the three interfaces with respect to the number of visits at the perceiving-monitoring zone of the EAT (*p*-values are adjusted with the Tukey method).

```{r s1-visit-count-contrats}
s1.anova.visit_count.comp.summary %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    digits = 2,
    caption = "(ref:s1-visit-count-caption)\\label{tab:s1-visit-count-comparison-table}",
    caption.short = "Study 1: Pairwise Comparisons for Visits Count.",
    longtable = FALSE,
    booktabs = TRUE
  )
```

## Discussion

The planned analyses aimed at investigating whether a different use of, and access to, emotional information determine differences in use of an EAT during a computer-mediated collaborative task, which -- beside evident shortcomings in the *collaborative* nature -- may be considered representative of a CSCL application in distance learning. The reduced sample size upon which the analyses are based requires caution in interpreting the obtained results. The inter-individual differences in all measured dependent variables entail wide confidence intervals, whose source can be traced back to the many processes implicated in the task. Some of them may not be directly inherent to a genuine interest in emotional awareness, and may have potentially influenced participants' capacity in conveying and taking into account emotional awareness beyond their intentions. For instance, participants had to coordinate multiple functions, both cognitively and practically (e.g. writing on a keyboard, manipulating the EAT, etc.), under specific time constraints. Participants with less dexterity in writing at the keyboard or manipulating the interface may have found less time to dedicate to the EAT even if they were willing to. The small sample size cannot guarantee that these individual differences are sufficiently balanced by the randomized trial. Even in the presence of detectable effects, thus, the assessment of their relevance in terms of *practical* consequences is limited: their size is inherently high due to the small sample and they should not even be taken as reliable benchmarks for future studies [@albersWhenPowerAnalyses2018].

On the other hand, the controlled environment in which the *performance-based* measures have been obtained make them worth of interest in assessing to what extent the presence of an EAT serves as an affordance in conveying and taking notice of emotional awareness during a computer-mediated collaborative task. Eye-tracking measures, in particular, may be considered spontaneous reactions occurring to some extent even beyond participants' top-down control. The discussion of the obtained results may thus contribute to sketch a more defined outlook of the use of an EAT and provide cues for further hypotheses worth investigating or shortcomings to be taken into account in future studies.

### Expressing Emotions May not Depend Exclusively on Social Sharing

In the first hypothesis, it has been posited that learners expression of emotions through the EAT varies depending on what use would be made of them, and what emotional information they have access to through the interface. More precisely, it has been stated that participants in the *Self-Centered* condition would express fewer emotions, compared to the *Partner-Oriented* and *Mutual-Modeling* condition, because of the absence of social sharing. It has also been posited that participants in the *Mutual-Modeling* condition would express more emotions than in the *Partner-Oriented* condition by virtue of an additional prompt in social sharing due to the direct and persistent comparison between one's own emotions and that of the partner.

Results failed to corroborate any of these assumptions given that neither an overall effect, nor differences in the comparison between each condition could be detected. The effect size to yield significant results was already ambitious with the planned $N=48$ sample size, and was therefore even further undercut by post-hoc exclusions, for which even observed effect sizes above $d = 0.5$ (*Self-Centered* vs *Partner-Oriented* and *Self-Centered* vs *Mutual Modeling*) cannot yield a discernible difference. Hypothesis *H1* must therefore be provisionally rejected, even though the observed directions and size of the effects are, in part at lest, consistent with an increased expression of the emotions with a more social-oriented interface.

Notwithstanding the limits of the sample, it is also worth reversing the perspective and, rather, highlight how participants in all conditions expressed on average `r maf.print_m_sd(s1.aggregated_emotions$n, FALSE, TRUE)` emotions, that is more than 1 emotion every 2 minutes of task. In particular, participants in the *Self-Centered* condition expressed on average `r maf.print_m_sd(s1.aggregated_emotions %>% filter(group == "Self") %>% pull(n), FALSE, TRUE)` emotions despite knowing they were the only recipient of the information. This result could be considered, in principle at least, as support for *not* ruling out the interest of *intra-personal* interest in expressing emotions: the presence of an EAT could indeed serve as an *individual affordance* for emotional expression as a support for (implicit) emotion regulation. On the other hand, though, this result may also be explained by side effects of the experimental task. For instance, this number may be inflated by task compliance, since the overall experimental setting was overtly aimed at expressing emotions. Furthermore, the characteristics of the experimental task, whose timing is fixed and not determined by the participants' actions, may also have pushed participants to express emotions to *fill-in* idle time between enigmas or part of the task within each enigma, rather than for an urge to express and regulate their emotions. A more fine-grained analysis of the time of expression should therefore be taken into account.

All things considering, thus, the present contribution conveys limited and mixed evidence with respect to the interest for expressing emotions during a computer-mediated collaborative task. Nevertheless, the experimental settings elicited considerable variation in the number of emotions expressed: with an increased sample size, the experimental plan could potentially contribute to assess the matter more thoroughly, for instance using a planned equivalence test with the aim to rule out differences, rather than detecting ones [@fidlerEpistemicImportanceEstablishing; @lakensEquivalenceTestsPractical2017].

### Emotional Seeking and Processing Seem Related to Social Sharing

In the second hypothesis, it has been posited that learners would seek and process the emotional information available on screen depending on the source and the comparison it facilitates. More precisely, it has been stated that participants in the *Self-Centered* condition would seek less often and process for shorter time the information available through the perceiving-monitoring part of the interface, compared to the *Partner-Oriented* and *Mutual-Modeling* conditions. It has also been posited that participants in the *Mutual-Modeling* condition would seek information more often, and process it longer compared to the *Partner-Oriented* condition due to the increased interest enhanced by direct and persistent comparison of emotional information.

Results corroborate the presence of an overall effect of the interface both on emotional information seeking and processing. For information seeking, the experimental condition yielded a generalized effect size [@olejnikGeneralizedEtaOmega2003] of `r apa_print(s1.anova.visit_count)$estimate$group`, and of `r apa_print(s1.anova.total_visit_duration)$estimate$group` for information processing. In both cases, thus, the experimental condition seems to account for a considerable amount of variation in the perceiving-monitoring function of emotional awareness.

On a more fine-grained level, though, the differences between conditions only partially corroborated the directional hypothesis; differences were detected, both for information seeking and processing, only in pairwise comparisons between *Self-Centered* vs. *Partner-Oriented* (Cohen's $d$ \> 1 for both measures) and *Self-Centered* vs. *Mutual-Modeling* (again Cohen's $d$ \> 1 for both measures), but not between *Partner-Oriented* vs. *Mutual-Modeling*, which was actually a more severe test [@mayoStatisticalInferenceSevere2018].

The lack of a detectable difference between the *Self-Centered* and the more social-oriented interfaces would have undermined the usefulness of an EAT, whereas its presence may be explained as a *simple* novelty effect: the *Partner-Oriented* and *Mutual-Modeling* condition convey information that the learner does not know, whereas in the *Self-Centered* condition the emotions are just a reminder of what the learner should already know. Nonetheless, taking the raw measures as benchmark, it is reassuring to observe that in a task of 20 minutes, the time spent looking at emotional information is around 1 minute when information about the partner is available, compared to 30 seconds when it is not. As it is the case for the expression of emotions, though, information processing and seeking in the *Self-Centered* measures must be considered as support for an intra-personal interest for the use of an EAT, even in the absence of communication with the partner.

The comparison between the *Partner-Oriented* and *Mutual-Modeling* interfaces has deeper implications with respect to the *raison d'tre* of an EAT. The lack of a discernible effect between the two social-oriented conditions may suggest that there is no additional value conveyed by direct and persistent comparison available on screen. But the phenomenon could also be explained by the fact that the additional value is gained without the need for further information seeking and processing. That is, participants in the *Mutual-Modeling* condition were able to compare their own and the partner emotion thorough the interface without having to look at the perceiving-monitoring zone of the EAT more often or longer, because they could get more information for the same effort. The eye-tracking measures alone cannot unravel whether the lack of a discernible effect is a positive or negative outcome with respect to the social-oriented hypothesis. In future studies, the hypothesis should therefore also be assessed with the aid of self-reported measures about the perceived usefulness of direct and persistent comparison of learners' emotional states.

## Post-Hoc Corollary Analyses

In this section, I provide the results of additional analysis that have not been planned before the study. First, I extended the analysis of eye-tracking measures using transitions between Areas Of Interest as an interesting measure of the use of an EAT. Second, I provide indications of the use of the EAT in real-time with respect to the appraisals and subjective feeling measures collected through the task. And finally, I take advantage of the use of the same task as in @fritzReinventingWheelEmotional2015 to conduct a small, internal meta-analysis that can be of interest for the use of the same task in future studies.

### Transitions Between Areas of Interest

The eye-tracking measures used in the planned analyses of variance treated each zone of the interface as a separated element. Given the importance of dynamic, real-time phenomena in the overall thesis, it is worth investigating also transitions between the three main Areas of Interest (AOI) of the experimental task, that is (1) the expressing zone, which is common to all conditions; (2) the perceiving zone, which varies according to the experimental condition; and (3) the area dedicated to the *main* task, which is also common to all conditions. Given that transitions can go in either direction between AOI, there are 6 possible combinations of transitions: (1) Expressing to Perceiving and (2) Perceiving to Expressing; (3) Expressing to Task and (4) Task to Expressing; and finally (5) Perceiving to Task and (6) Task to Perceiving. An exploratory analysis of the transitions may reveal whether specific transitions are more frequent than others depending on the interface at disposal, and thus contribute to better assess the perceiving-monitoring function of an EAT.

The number of transitions between AOI was computed by searching for subsequent rows in the eye-tracking logs for each of the $N = 35$ participants in which the first row had a certain AOI activated, and the following row had another AOI activated. Is it worth noting, though, that this method is sub-optimal because the experimental task included the use of a keyboard. Therefore some transitions may have been lost due to the fact that each gaze-path may have been interrupted by a *detour* to the keyboard. Nevertheless, it is safe to assume that participants directed their gaze into the AOI they were interested in acting upon -- for instance, in order to focus the pointer into the text area -- before turning the gaze away from the screen if they needed to look at the keyboard for typing. All things considering, thus, this method can be of interest at least as an exploratory method, even though it lacks external validity and should be revisited before being deployed in a substantial analysis.

After seeing the data, one participant was excluded for having a number of transitions from Expressing to Perceiving and from Perceiving to Expressing much higher than all other participants regardless of the group: more than 100 against a mean of `r maf.print_m_sd(s1.transitions %>% filter(transition == "Expressing to Perceiving" || transition == "Perceiving to Expressing") %>% pull(num_transitions), FALSE, TRUE)` for the other participants regardless of the condition. Results are therefore based on $N = 34$ participants.

Participants made on average `r maf.print_m_sd(s1.transitions %>% group_by(ParticipantName) %>% summarise(total_transitions = sum(num_transitions)) %>% pull(total_transitions), TRUE, TRUE)` transitions between any two AOI. Figure \@ref(fig:s1-transitions-graph) reports the number of transitions stratified by experimental condition between the 6 AOI organized in three rows such as each row displays the transitions between the same two AOI in both directions.

(ref:s1-transitions-graph-caption) Number of transitions between Areas of Interest (AOI) on the interface. Transitions aggregated for $N = 34$ participants. Bars represent 95% confidence intervals.

```{r s1-transitions-graph, fig.cap="(ref:s1-transitions-graph-caption)", fig.height=6, out.width="100%"}
s1.transitions.graph
```

Data suggest that there are differences that may be accounted for by the type of interface the participants have access to. In particular, participants in the *Mutual-Modeling* condition seem to be more prone to make transitions between the two AOI that are more directly related to the social sharing of emotions. Transitions between the *expressing-displaying* and the *perceiving-monitoring* zone (first row in the graphic) may indicate that the possibility of direct and persistent comparison of one's own emotions with that of the partner could serve as *social reference* before expressing one's own emotions, or *social comparison* after having expressed them. Furthermore, transitions between the *perceiving-monitoring* zone and the *task* zone (last row in the graphic) may indicate that the emotional information is taken into account as instrumental information to the task at hand.

Participants in the *Self-Centered* condition seem to privilege the paths between the *expressing-displaying* zone and the *task* zone, which is consistent with the fact that the *perceiving-monitoring* zone has only information about their own emotions. It is interesting to notice, though, that in the *Self-Centered* condition, transitions from the *expressing-displaying* zone to the *perceiving-monitoring* zone (first row, graph on the left) do not seem to be more frequent compared to the *Partner-Oriented* condition. This may be relevant because it could rule out the possibility that a difference between the *Partner-Oriented* and *Mutual-Modeling* condition may be due simply to the fact that, in the *Mutual-Modeling* condition, participants only seek confirmation of what they have expressed, since this confirmation is not available in the *Partner-Oriented* condition.

Finally, the *Partner-Oriented* condition seems once again *stuck in the middle*, and results for this group are difficult to assess due to the greater inter-individual variance that is also present in the other planned analysis. As a rule of thumb interpretation, the *Partner-Oriented* condition seems to go hand-in-hand with the *Self-Centered* condition in transitions between the *expressing-displaying* zone and the *perceiving-monitoring* zone (first row); and with the *Mutual-Modeling* condition in the other transitions (second and third rows).

In an attempt to figure out whether this kind of analysis may be used in a more structured manner, a multilevel linear model, also known as mixed linear model [@batesFittingLinearMixedEffects2014; @kuznetsovaLmerTestPackageTests2017; @westLinearMixedModels2015], was fitted to the data at hand using the mixed function of the Afex [@singmannAfexAnalysisFactorial2020; @singmannIntroductionMixedModels2020] R package version `r packageVersion("afex")`. The model was fitted in the following terms: the aggregated number of transitions per participant for each possible path represented the outcome variable; the type of transition and the interface of the EAT (*i.e.* the experimental condition) were considered as fixed factors, with an interaction between the two; the participant was used as a random intercept to account for the non-independence of observations. A more complex model could have been more interesting, but hardly feasible due to the small number of participants [@batesParsimoniousMixedModels2018].

A Type III Analysis of Variance of the multilevel linear model confirms effects of both individual factors and the interaction. Results are depicted in Table \@ref(tab:s1-transitions-anova-table) using Kenward-Roger approximation for computing the *p*-value [@lukeEvaluatingSignificanceLinear2017].

(ref:s1-transitions-anova-caption) Results of a Type III ANOVA on the fitted multilevel linear model

```{r s1-transitions-anova-table}
s1.transitions.lmm.anova_table %>%
  kable(
    caption = "(ref:s1-transitions-anova-caption)\\label{tab:s1-transitions-anova-table}",
    caption.short = "Study 1: Transitions between AOI",
    booktabs = TRUE,
    longtable = FALSE
  )
```

Table \@ref(tab:s1-transitions-comparisons-table) reports the pairwise comparisons between the three experimental conditions stratified by the bi-directional path of the transition. Detectable differences in the pairwise comparisons can be observed between *Self-Centered* vs *Mutual-Modeling* and *Partner-Oriented* vs *Mutual-Modeling* in the transitions between expressing-displaying and perceiving-monitoring. In the four comparisons, the more *social-oriented* interface obtained more transitions, in both directions, than the less *social-oriented* one, corroborating the assumption that participants make use of the emotional information about the partner as a reference.

In the transitions between expressing-monitoring and the task, only the path going from the task to the expression-displaying zone yielded a detectable difference with more transitions in the *Partner-Oriented* than in the *Self-Centered* interface. The effect is nevertheless not corroborated by any other comparison in the same transition path.

Finally, in the transitions between perceiving and the task, detectable differences were observed between the *Self-Centered* and the *Mutual-Modeling* interfaces, with the *Mutual-Modeling* interface yielding more transitions in both directions. These results may support the role of emotional awareness as instrumental information directly related to the task at hand, but are not corroborated by a difference between the *Self-Centered* and the *Partner-Oriented* interfaces.

(ref:s1-transitions-comparisons-caption) Comparisons between the groups stratified by the path of the transitions. The Kenward-Roger approximation for the degrees of freedom is adopted and *p*-values are adjusted using the Tukey method for comparing a family of 3 estimates.

```{r s1-transitions-comparisons-table}
s1.transitions.contrasts$contrasts %>%
  as_tibble() %>%
  select(-transition) %>%
  mutate(p.value = round_ps_apa(p.value)) %>%
  kable(
    caption.short = "Study 1: Pairwise comparison between transitions",
    longtable = FALSE,
    booktabs = TRUE,
    linesep = c("", "", "\\addlinespace"),
    col.names = c("Comparison", "Est.", "SE", "df", "t.ratio", "p.value"),
    caption = "(ref:s1-transitions-comparisons-caption)\\label{tab:s1-transitions-comparisons-table}",
  ) %>%
  kable_styling(
    latex_options = c("repeat_header")
  ) %>%
  pack_rows("Expressing to Perceiving", 1, 3) %>%
  pack_rows("Perceiving to Expressing", 4, 6) %>%
  pack_rows("Expressing to Task", 7, 9) %>%
  pack_rows("Task to Expressing", 10, 12) %>%
  pack_rows("Perceiving to Task", 13, 15) %>%
  pack_rows("Task to Perceiving", 16, 18)
```

All things considering, transitions may represent a more interesting measure of the perceiving-monitoring function of emotional awareness compared to the information seeking and processing measures adopted in the planned analyses. Even considering the shortcomings (e.g. transitions interrupted by the use of the keyboard), transitions provide a more *dynamic* outlook on how emotional information is integrated into the task. The concept of transition may even be pushed further by measuring at which moment the transition has occurred, which would provide useful information on the dual-task nature of emotional awareness. For instance, it would be possible to assess whether learners look at the emotions expressed by the partner as soon as they appear on the interface, or if they wait idle period in the task.

### Emotions and Time: Evaluating the Purpose of Real-Time Awareness

One of the main tenets of the present contribution is the advantage of *real-time* emotional awareness -- at least to the extent that participants do not have to wait predefined stops to share their emotions. Some exploratory analyses on the sample were performed in order to check to what extent the *real-time* feature has been exploited with respect to the expression of emotions.

#### Cognitive Evaluation Over Time

Congruently with appraisal theories of emotions -- which state that it is the evaluation one does of the situation and not the situation *per se* that elicits the emotion -- it is worth checking for the emergence of a pattern in the appraisals of Valence and Control/Power over time. Since all participants were exposed to the same stimuli (except participants in the *Self-Centered* condition, who did not see the emotions of the simulated partner), a clear pattern in the evaluation of the two criteria would not be congruent with appraisal theories. Figure \@ref(fig:s1-appraisal-evolution-graph) shows all the $N = `r nrow(s1.dew_emotions)`$ emotions that have been expressed by all the participants over the 20 minutes of the task, stratified by condition. For each observation, the value of Valence and Control/Power are displayed with respect to the elapsed time in the task. A Locally Estimated Scatterplot Smoothing (LOESS) -- that is, non-parametric curve that best fit the empirical data [@jacobyLoessNonparametricGraphical2000] -- is superposed to the raw data.

(ref:s1-appraisal-evolution-caption) Evolution of the appraisal dimensions over time with a LOESS smoother ($N = 35$, all emotions of participants are aggregated per condition).

```{r s1-appraisal-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-appraisal-evolution-caption)"}

grid.arrange(
  s1.appraisal_over_time_valence,
  s1.appraisal_over_time_control,
  nrow = 1
)
```

The graphic indicates that the smoother remain, overall, close to the neutral point, since data-points are evenly spread over the elapsed time in the task. It is worth noting, though, that in the *Mutual-Modeling* condition, both appraisals decreased as the task went on.

#### Subjective Feelings Over Time

The same analysis can be conducted with respect to the expression of subjective feelings over time. Figure \@ref(fig:s1-feelings-evolution-graph) below plots the evolution of the expression of the 20 subjective feelings -- which are part of the underlying affective space used in the study -- aggregated for all $N=35$ participants. The observations are stratified per condition. (In order to reduce the space of the graph, the legend for each condition has been omitted, but the colors are congruent with previous graphs.)

(ref:s1-feelings-evolution-caption) Expression of the subjective feelings. $N = `r nrow(filter(s1.dew_emotions, listed == TRUE))`$ emotions (out of `r nrow(s1.dew_emotions)`) whose subjective feeling belongs to the underlying affective space used by the DEW, aggregated for the $N = 35$ participants. (Legend omitted for reducing space, see previous graphs.)

```{r s1-feelings-evolution-graph, fig.align="center", out.width="100%", fig.cap="(ref:s1-feelings-evolution-caption)"}
s1.feelings_over_time_graph
```

Not considering the feelings that have been expressed only a few times (*e.g.*, *Envious* or *Disgusted*), most of the subjective feelings have been expressed rather uniformly over the 20 minutes of the task. Interesting exceptions are the feelings *Bored* and *Frustrated* that only starts around 5 minutes into the task -- that is, around the end of the first enigma -- which may be due to the repetitive nature of the task for boredom, and the increasing difficulty of the enigmas for frustration.

Finally, the overall small sample size combined with the unbalanced number of participants for experimental condition imply caution even on superficial interpretations about the effect of the interface. It is nevertheless worth noting how participants felt often *Relieved* or *Satisfied* in the *Mutual-Modeling* condition, but not in the *Self-Centered* or *Partner-Oriented* condition; or that the *Emphatic* feeling was expressed in the *Mutual-Modeling* and *Partner-Oriented* condition, but not in the *Self-Centered*. With a greater number of participants, it would be interesting to perform this kind of stratification in a more systematic way.

### Internal Meta-Analysis on Task Indicators

Taking advantage of the fact that the same task was adopted, under similar conditions, of a previous study [@fritzReinventingWheelEmotional2015], an internal meta-analysis was performed on the point-estimate means for the three dependent variables adopted in the current contribution. The interest of the meta-analyses is two-fold. On the one hand, they provide a better assessment of the point estimates about the performance-based indicators of the use of the EAT. On the other hand, those same indicators will be used as references in a subsequent study in this contribution.

Each meta-analysis has been conducted using the R meta package version `r packageVersion("meta")`, adopting the inverse of the variance weighting mechanism to account for differences in the sample size of the two internal studies. Results both for the fixed and the random models (using the DerSimonian-Laird estimator for $tau^2$) are provided.

#### Expressing Emotions Internal Meta-Analysis

The meta-analysis on the expression of emotions has been conducted on the whole sample size of both studies, since, even for participants in the *Self-Centered* condition, the overall situation in which participants have expressed their emotions are sufficiently close for an internal meta-analytic purpose. Consequently, the sample size are of $N = 16$ in Fritz (2015) and of $N = 35$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-expressing), assess an estimated mean of `r s1.meta_analysis_expressing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$fixed$upper %>% printnum()`] expressed emotions for the fixed effect model, and of `r s1.meta_analysis_expressing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_expressing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_expressing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis highlights the presence of considerable heterogeneity in the expression of emotions ($\tau^2$ = 10.29; $\tau$ = 3.21; I\^2 = 82.0% [23.9%; 95.7%]; H = 2.36 [1.15; 4.84]; $\chi^2$ = 5.55 *p* = .018). This may suggest that the different conditions of the two studies may have played a role in inflating the number of emotions expressed in Fritz (2015), where participants were explicitly asked, if possible, to express at least one emotion in each phase of the 4 enigmas (*i.e.* which would amount to 12), whereas in Perrier (2017) they did not receive any guidance. This may be interpreted as a warning about the importance of being careful in framing how the expression of emotional information is prompted, even if the inflation of the number of emotions may not be necessarily accounted by *forced* emotions, that is, emotional episodes that are not *really* felt, but nevertheless reported. It may also be the case that prompting for emotional expression may ease participants into expressing their emotions, something they could be less prone to do otherwise.

```{r s1-forest-expressing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of emotions expressed in the experimental task.", fig.height=2 }
forest.meta(s1.meta_analysis_expressing, hetstat = TRUE, xlim = c(5, 25), layout = "JAMA")
```

#### Information Processing Internal Meta-Analysis

The internal meta-analysis on information processing has been conducted using only the participants retained for the eye-tracking analysis ($N = 14$) in Fritz (2015), and only participants in the *Partner-Oriented* and *Mutual-Modeling* conditions ($N = 23$) in Perrier (2017), for the interface in these situations is identical or at least very similar with respect to the *social* information shared. Results, depicted in Figure \@ref(fig:s1-forest-processing), assess an estimated mean of `r s1.meta_analysis_processing.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$fixed$upper %>% printnum()`] total visit duration, in seconds, for the fixed effect model, and of `r s1.meta_analysis_processing.summary$random$TE %>% printnum()` [`r s1.meta_analysis_processing.summary$random$lower %>% printnum()`; `r s1.meta_analysis_processing.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 40.81; $\tau$ = 6.39; I\^2 = 45.2%; H = 1.35; $\chi^2$ = 1.82 *p* = .177), which may indicate that the time spent at looking at emotional information could be determined by a balance between the primary problem-solving activity and the sustaining emotional awareness. The point estimate of total visit duration being around 1 minute over the 20 minutes of the task, it corresponds to a proportion of 5% of the total time.

```{r s1-forest-processing, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the time spent at processing emotional information available on screen.", fig.height=2}
forest.meta(s1.meta_analysis_processing, hetstat = TRUE, xlim = c(30, 80), layout = "JAMA")
```

#### Information Seeking Internal Meta-Analysis

With respect to emotional information seeking, the internal meta-analyses comprise the same samples as for information processing, that is $N = 14$ in Fritz (2015) and $N = 23$ in Perrier (2017). Results, depicted in Figure \@ref(fig:s1-forest-seeking), assess an estimated mean of `r s1.meta_analysis_seeking.summary$fixed$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$fixed$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$fixed$upper %>% printnum()`] number of visits for the fixed effect model, and of `r s1.meta_analysis_seeking.summary$random$TE %>% printnum()` [`r s1.meta_analysis_seeking.summary$random$lower %>% printnum()`; `r s1.meta_analysis_seeking.summary$random$upper %>% printnum()`] for the random effect model. The meta-analysis does not detect heterogeneity between studies ($\tau^2$ = 0; $\tau$ = 0; I\^2 = 0%; H = 1; $\chi^2$ = .56 *p* = .453), which is nevertheless rather due to the huge variability within studies rather than homogeneity between studies. In future studies, the number of transitions between AOI could represent a more informative measure for emotional information seeking.

```{r s1-forest-seeking, fig.align="left", out.width="100%", fig.cap="Internal meta-analysis of the number of times emotional information has been visited on screen.", fig.height=2 }
forest.meta(s1.meta_analysis_seeking, hetstat = TRUE, xlim = c(50, 100), layout = "JAMA")
```

## Conclusion

This chapter presented a detailed illustration of an empirical study investigating whether a different use of, and access to emotional information expressed and available through an EAT had an effect on the actual use of the EAT itself. $N=48$ participants, then reduced to $N=35$ following exclusion criteria, were randomly assigned to three different interfaces of the EAT -- namely a *Self-Centered*, a *Partner-Oriented*, and a *Mutual-Modeling* interface -- which varied on how socially-oriented each interface was. The main assumption underlying the empirical investigation stated that the more socially oriented interface would yield a greater use of the EAT in terms of emotion expressed and emotional information seeking and processing. This assumption was not corroborated for the number of emotion expressed, since a detectable difference was not observed between the three conditions; and it was only partially corroborated for emotional information seeking and processing, for which participants in the *Partner-Oriented* and *Mutual-Modeling* conditions sought and processed emotional information more than in the *Self-Centered* condition, but no detectable difference was observed between the two more socially-oriented conditions.

Despite the hypotheses of the study being rejected or only partially corroborated, most of the performance-based indicators about the use of the EAT were congruent with the main assumption. These indicators included the number of emotions expressed through the tool, the number of visits and seconds spent looking at the perceiving-monitoring part of the EAT, as well as the transitions between the more socially-oriented parts of the EAT interface. In most of these cases, the *Mutual-Modeling* interface yielded the greater use of the EAT, followed by the *Partner-Oriented*. Congruently with a inter-personal perspective on emotional expression [@parkinsonCurrentEmotionResearch2015; @parkinsonEmotionsAreSocial1996; @rimeEmotionElicitsSocial2009; @vankleefEmotionInfluence2011; @vankleefHowEmotionsRegulate2009; @vankleefInterpersonalDynamicsEmotion2018], these results seem to corroborate the usefulness of an EAT as an affordance to share emotion with a partner and take the partner's emotions into account during a computer-mediated collaborative task. Participants seemed to show a genuine interest in seeking and processing emotional information available about the partner, and knowing that their emotions would be conveyed to the partner did not stop participants to express them. The presence of the partner's emotions on the EAT interface also resulted in a more *dynamic* gaze-path, with more transitions between the part of the interface dedicated to the task, and that dedicated to the monitoring-perceiving function of awareness. This fact seems to corroborate the usefulness of providing real-time emotional awareness, since the emotional information may be truly integrated as instrumental information to the task at hand [@buderGroupAwarenessTools2011; @dourishAwarenessCoordinationShared1992]. A word of caution is nevertheless in order, since the method by which transitions have been computed is not externally validated yet. Whether and when it will, transitions could represent a more adequate measure of integrated and dynamic information seeking and processing compared to the *static* number of visits and seconds spent inside an Area of Interest used in the directional hypotheses of the study. All things considering, thus, a moderate optimism is warranted about the usefulness of an EAT during a computer-mediated collaborative task. Despite the fact that it does not directly provide information about the content space of the task [@janssenCoordinatedComputerSupportedCollaborative2013], sharing emotions could nevertheless sustain the mutual-modeling activity by which learners build and update a holistic representation of their partner in the collaboration [@dillenbourgSymmetryPartnerModelling2016].

At the same time, participants in the *Self-Centered* condition also seemed to harness the presence of the EAT, which is congruent with an intra-personal usefulness in expressing emotions [@liebermanPuttingFeelingsWords2007; @liebermanAffectLabelingAge2019; @torrePuttingFeelingsWords2018]. Even though participants in this condition knew beforehand that they were the exclusive sender and receiver of the emotional information, they still expressed emotions as well as sought and processed their own emotional information available on the EAT. This fact suggests that the presence of an EAT may prompt learners to inquiry about their own emotional state, appraising the situation and seeking for the congruent subjective feeling elicited by the circumstances [@boehnerHowEmotionMade2007; @grandjeanConsciousEmotionalExperience2008; @schererDynamicArchitectureEmotion2009].

### Limitations and Future Development

The present contribution adopted a controlled environment in order to expose every participant to the same stimuli, except for the randomly assigned interface. The use of a simulated partner limited the inter-personal communication that would be normally available in a real collaborative setting. For the purposes of the study, a distinction between cooperation (roughly, doing the same thing with limited interdependence) and collaboration (integrating efforts into a common outcome) was not of primary concern. Nevertheless, this certainly represents a limitation to the generalization of the obtained results to a more articulated communication flow, which could overlap with the emotional information expressed through the EAT. For instance, learners could have used the text area dedicated to their reasoning to inject circumstantial information such as *I don't understand* or *I don't agree with you*, which would have conveyed social, cognitive and emotional information [@derksRoleEmotionComputermediated2008]. A focal question avoided by the present experimental setting is therefore whether participants would still have used the EAT the way they did, were they allowed to convey emotional information through the content space of the joint problem-solving activity. The task at hand, though, can be easily extended to a real collaboration between two participants, and therefore it would be possible to investigate the matter in a way that can be directly related to the results of this contribution. That could be obtained either by a direct replication of the current setting, but using pairs of participants, or by a split design in which part of the participants are randomly assigned to a simulated- or a real-partner. This second option would elicit a better and more reliable comparison, but its interest would be fairly limited to the *validation* of a simulated participant, which was an auxiliary instrument in the study.

A direct replication with real collaboration, on the other hand, would investigate the subject matter more thoroughly, even though the comparison with present results would have to take the difference in time and setting into account. In this regard, a possible solution is to retain a ratio between the duration of the joint-problem solving task and the performance-based measures of the use of the EAT. For example,participants in a dyad that solved the four enigmas in 12 minutes and expressed 18 emotions for one participant and 24 for the second would have a ratio of 1.5 and 2 respectively. Taking into account the temporal dimension would also add another element of interest: investigate whether the ration holds constant across different duration of the joint problem-solving task, or it yields a moderation effect.

Furthermore, the focal question of whether learners would use the EAT having a more thorough channel of communication could also be assessed by allowing learners to display or not the EAT on their screen. This choice may also be instrumental in investigating one of the main assumptions of the thesis, namely the interest for real-time emotional awareness: whether and when participants would decide to display the EAT may convey pivotal evidence about the usefulness of real-time emotional awareness compared, for instance, to a *scripting* strategy in which partners share their emotions at specific intervals outside the task.

### Acknowledgments

The experimental phase of the project has been carried out by Stphanie Perrier as part of her Master thesis [@perrierCollaborationEnvironnementMediatise2017] that Mireille Btrancourt and I co-directed.
