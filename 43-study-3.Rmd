---
bibliography: references.bib
---

# A Data-Driven Assessment of the Emotion Awareness Tool in Different Computer-Mediated Environments {#study-3}

```{r comparison-setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(papaja)
library(see)
library(here)
library(knitr)
library(kableExtra)

Sys.setenv(LANG = "en")

# Load the relevant data and graphs from the study-3 folder
source(here("data", "study-3", "s3-export.R"), local = knitr::knit_global(), encoding = "UTF-8")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

This chapter provides a general assessment of some of the fundamental features of the Emotion Awareness Tool (EAT) implemented in the thesis from a data-driven perspective. To do so, the study will merge data from the usability test [@fritzReinventingWheelEmotional2015], briefly resumed in Section \@ref(dew-ux-test), and the two empirical contributions of Chapter \@ref(study-1) and Chapter \@ref(study-2). The structure of the chapter will nevertheless follow once again the traditional structure of an empirical contribution [@sollaciIntroductionMethodsResults2004] already adopted by the two previous empirical contributions.

## Study Rationale

Even though the use of an EAT is highly dependent on a series of factors such as who is using it, when, and why, one of the main tenet of the thesis is that it is possible to provide a multi-purpose tool, which can be adapted to different situations. Taking advantage from the fact that the EAT has been deployed in three empirical settings presented in this contribution -- the usability test retrived from Fritz (2015) and the two previous empirical chapters -- data collected in each occasion can be integrated to provide a more thorough data analysis.

More specifically, it is possible to distinguish between the synchronous and collaborative (Synch./Collab.) settings of the usability test and the experiment in Chapter \@ref(study-1) on the one hand, and the asynchronous and individual (Asynch./Indiv.) -- or non-collaborative -- settings of the study in Chapter \@ref(study-2) on the other. By this comparison, though, the aim is certainly not to draw inferences from the effect of different computer-mediated environments on the use of an EAT (*e.g.,* students are *happier* working together than in isolation). On the contrary, consistently with the focus on internal validity of the thesis, the objective is rather to assess whether the same EAT can adapt to different settings. One of the central tenet of the thesis is, in fact, that researchers and practitioners interested in endowing computer-mediated learning environments with emotional awareness may do so by adapting a tool, whose central features may be of interest in a wide number of scenarios. At the same time, the EAT is also based upon specific theoretical and technical assumptions, such as the usefulness of implementing an emotion structure into the tool. A comprehensive analysis of data gathered in different settings, but based upon similar theoretical and technical postulates, can therefore represents a first assessment of the intrinsic qualities and shortcomings of the EAT.

## Research Questions

In this regard, the chapter proposes four types of assessment related either to specific features of the EAT or its perception as a whole. Each assessment focuses on a specific research question, presented hereafter.

### Appraisal Dimensions as Meaningful Evaluation of Events

The concept of appraisal is a central tenet of the entire thesis. It is the *glue* that puts together different theoretical assumptions, such as what an emotion *is* [@sanderModelsEmotionAffective2013; @schererWhatAreEmotions2005; @moorsAppraisalTheoriesEmotion2013; @scherer2019; @scherer2022], and pedagogical implications, such as how learners can take full advantage from an EAT in terms of self-reflection and strategic communication of their emotional experience to others [@boehnerHowEmotionMade2007; @cerneaSurveyTechnologiesRise2015; @molinariEmotionFeedbackComputermediated2013; @lavoueEmotionAwarenessTools2020]. Appraisal is technically translated into the interface with the use of sliders, upon which learners can evaluate the situation in a dimensional approach to emotion self-report. In the three empirical settings in which the EAT has been adopted, the *Valence* and *Control/Power* dimensions represented the appraisal criteria, which prompted a cognitive evaluation of the situation by learners. The first research question therefore relates to the use of the sliders as representative of the interest and instrumentality of the underlying appraisal dimensions.

### Lexicalized Emotions as Representative of Learners' Subjective Feelings

Another focal point concerns the symbolic representation of the whole emotional experience that learners can use to extrapolate inter-personal meaning making and/or send a strategic *token* that maximize inference of the *true* emotional episode in others [@grandjeanConsciousEmotionalExperience2008; @fontaineComponentsEmotionalMeaning2013; @ogarkovaFolkEmotionConcepts2013]. In the EAT, this role is conferred to a number of lexicalized emotions pre-compiled in the underlying affective space, which are meant to provide learners with meaningful options to coalesce the emotional experience in a practical and intuitive way. The second research questions therefore assess to what extent the lexicalized emotions proposed in the EATMINT circumplex met this requirement.

### The Computational Model on Trial

The core of the EAT relies on the computational model presented in Chapter \@ref(computational-model), whose fundamental tenet is that the holistic emotional experience can be predicted on the basis of the cognitive evaluation of the situation [@schererHumanEmotionExperiences2013; @fontaineLinearNonlinearRelationships2021; @scherer2018; @gentschEffectsAchievementContexts2017; @schererSemanticStructureEmotion2018]. The EAT harness this causal mechanism to provide a subset of lexicalized emotion that are most likely to represent learners subjective feeling, given the rating on the appraisal dimensions. In all three empirical settings, the EAT provided 3 suggestions as buttons, but also left participants free to provide another emotion term either from the pre-compiled list (but not proposed as button) or outside the list. The third research questions investigates the efficacy of the computational model in providing learners with an *educated guess*, which was accepted as an accurate representation of learners' subjective feeling.

### Perceived Usability Beyond the Concrete Use

Even though the concrete use of an EAT is the primary concern for an awareness tool, it may be influenced by a number of dispositional or situational factors, as illustrated by the abstract model of emotional awareness in Section \@ref(abstract-model-of-ea). In other words, the EAT may have a *potential* that is not fully expressed in the situation at hand. The perceived usability of the tool -- which comprises the efficacy, efficiency and satisfaction in using the EAT [@brookeSUSQuickDirty1996; @lewisItemBenchmarksSystem2018; @tullisMeasuringUserExperience2013] -- may therefore complement performance-based data in the assessment. The fourth research questions thus refers to how the usability of the EAT has been rated across different settings.

## Methods

This third empirical contribution can be considered a form of secondary data analysis [@westonRecommendationsIncreasingTransparency2019] or a small internal meta-analysis of one's own studies [@gohMiniMetaAnalysisYour2016]. It consists in grouping the datasets of the three empirical contributions where the EAT has been deployed: the usability test in Fritz (2015), the experiment of Chapter \@ref(study-1), and the longitudinal study of Chapter \@ref(study-2). Data will be integrated in two datasets, which will be used to address the four research questions.

### Expressed Emotions Dataset

The first dataset consists in all the emotions that have been expressed through the EAT. This dataset comprises $N_{observations} =$ `r s3.dew_combined_emotions |> nrow()` produced by $N_{participants} =$ `r s3.dew_combined_emotions$user |> unique() |> length()`. Table \@ref(tab:s3-emotions-dataset-table) illustrates how observations and participants are divided between the three datasets, with two datasets attributed to the Synch./Collab. setting, and one to the Asynch./Indiv. setting.

(ref:s3-emotions-dataset-caption) Descriptive statistics in the rating of the two appraisal dimensions of the affective space

```{r s3-emotions-dataset-table}
s3.emotions_allocation |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-emotions-dataset-caption)\\label{tab:s3-emotions-dataset-table}",
  caption.short = "Study 3: Datasets of expressed emotions",
  longtable = FALSE,
 ) |>
 kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

### System Usability Scale Dataset

The second composed dataset comprises the rating on the System Usability Scale [@brookeSUSQuickDirty1996], which was administered in the usability test and in the study of Chapter \@ref(study-2), but not in the experiment of Chapter \@ref(study-1). In total, the SUS has been rated by $N =$ 40 participants, 14 in the Synch./Collab. setting of the usability test, and 26 in the study of Chapter \@ref(study-2).

### Analyses

The analyses will be comparative in nature, but without the use of inferential statistics, which are considered outside the scope of a preliminary assessment of the tool.

## Results

The results are presented with respect to the four research questions. A thorough use of graphical representations is adopted to better convey the different facets of the measures at hand [@tukeyExploratoryDataAnalysis1977; @hegartyCognitiveScienceVisualspatial2011; @franconeriScienceVisualData2021].

### Measures About the Use of Appraisal Dimensions

The first theory-driven feature of the EAT under scrutiny are the two sliders, which represent the appraisal dimensions through which the eliciting event is evaluated [@schererAppraisalConsideredProcess2001; @schererDynamicArchitectureEmotion2009; @schererWhatAreEmotions2005]. As a reminder, the EATMINT circumplex adopts the *Valence* and *Control/Power* appraisal dimensions to prompt the evaluation of the situation. In all empirical contributions, the Valence dimension was prompted with the question *Is the situation pleasant?* The *Control/Power* dimension was prompted with the question *Is the situation under your control?* in the Synch./Collab. contributions. For the Asynch./Individ. contribution, on the other hand, the question *Can you modify the situation?* was adopted instead. Both dimensions could be rated from a negative pole labeled *Not at all*, corresponding to a score of -100, to the positive pole labeled *Yes, absolutely*, corresponding to a score of 100. Each slider was sensitive to 1-point variation.

#### Overall Ratings of the Appraisal Dimensions

One of the interesting indications that can be assessed through the ratings on the appraisal dimensions is to what extent participants could make use of the full range of the slider, that is, whether they discriminate the eliciting events as being more or less pleasant, and more or less under their control. In this regard, Table \@ref(tab:s3-appraisal-descriptive-table) reports the number of participants expressing at least one emotion through the tool, the cumulative number of emotions expressed, as well as the overall mean and standard deviation of the two appraisal dimensions.

(ref:s3-appraisal-descriptive-caption) Descriptive statistics in the rating of the two appraisal dimensions of the affective space

```{r s3-appraisal-descriptive-table}
s3.appraisal.descriptive |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-appraisal-descriptive-caption)\\label{tab:s3-appraisal-descriptive-table}",
  caption.short = "Study 3: descriptive of appraisal dimensions",
  longtable = FALSE,
 ) |>
 row_spec(3, bold = T) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

Results show that, for the Valence dimension, the overall mean is almost perfectly neutral for the Asynch./Indiv. setting, whereas it is slightly positive (around 6 points) for the Synch./Collab. setting. In both settings, the rating of the Valence dimension yielded a high standard deviation of around 60 rating points. Data therefore corroborate that participants in both settings took advantage of the full range of the Valence dimension in a very similar way. With respect to the *Control/Power* dimension, the overall mean for the Asynch./Indiv. setting is slightly positive (around 5 points), whereas it is slightly negative for the Synch./Collab. setting (around -4 points). The standard deviations are also high, but more diverging, with a difference of more than 10 rating points (around 50 for Asynch./Indiv. against around 64 for Synch./Collab.). For this second appraisal dimension, thus, data highlight a slight divergences in central tendency, even though it remains close to the neutral point for both settings, and less variance in the rating of the *Control/Power* slider in the Asynch./Indiv. setting.

The descriptive measures are complemented by Figure \@ref(fig:s3-appraisal-density-graph), which shows the density of the two appraisal dimensions for each learning setting. The plots highlight fairly symmetrocal and flat distributions (*i.e.*, Leptokurtic-like shapes) around the neutral point for all combinations, except for the *Control/Power* dimension in the Asnyhc./Indiv. setting, on the top-right plot, which has an higher peek of the distribution (*i.e.*, Platykurtic-like). This higher peek is nevertheless inflated by a single participant who expressed 65 emotions leaving the *Control/Power* dimension on the neutral point. The distributions also denote some *bumps* on the tails, especially in the positive tail of the Asynch./Indiv. and, to a lesser extent, both tails in the Synch./Collab. setting for the *Control/Power* dimension. These *bumps* represent ratings in which participants used the extreme poles of the sliders. There is therefore a certain trend in expressing the more extreme values on the appraisal dimensions.

All things considering, though, participants in both settings seem to take advantage, individually, of the full range of the appraisal dimensions. Being the two dimensions related, though, the analysis must also consider their joint ratings, which is presented next.

(ref:s3-appraisal-density-caption) Density plots of the two appraisal dimensions' ratings for the each setting.

```{r s3-appraisal-density-graph}
#| fig.align="center", 
#| fig.cap="(ref:s3-appraisal-density-caption)",
#| fig.scap = "Study 3: Density plot of the appraisal dimensions",
s3.appraisal_density.graph
```

#### Joint Ratings of Valence and Control/Power

The second element of interest in the use of the appraisal dimensions is whether their use is independent from one another, in which case the dimensions are truly orthogonal, or if there is a sort of *multicollinearity* due to a high correlation between ratings (see Section \@ref(gew-limitations) for a more theoretical discussion on the subject). In other words, does it happen that participants rate a situation as pleasant, but without feeling control over it; or, conversely, a situation as unpleasant, but feeling control over it? Figure \@ref(fig:s3-appraisal-evaluation-graph) shows two Locally Estimated Scatterplot Smoothing (LOESS) functions [@jacobyLoessNonparametricGraphical2000] -- that is, two non-parametric regressions lines that best fit the data at hand -- applied to the points defined by the *Valence* appraisal on the x-axis, and the *Control/Power* appraisal on the y-axis.

(ref:s3-appraisal-evaluation-caption) LOESS functions applied to *Valence* x *Control/Power* appraisals in the two settings.

```{r s3-appraisal-evaluation-graph}
#| fig.align="center", 
#| fig.cap="(ref:s3-appraisal-evaluation-caption)",
#| fig.scap = "Study 3: Scatterplot of appraisal dimensions rating with LOESS function",
#| out.width="80%"
s3.appraisal_evaluation.graph
```

The fitted lines highlight a strong positive correlation between the *Valence* and the *Control/Power* ratings, especially in the Synch./Collab. setting, for which the relationship is almost perfectly rectilinear. The ratings of the two appraisal dimensions tend thus to co-vary, so that *Valence* and *Control/Power* are both negative or both positive. This phenomenon is corroborated if the expressed emotions are divided in three possible combinations: (1) appraisal dimensions share the same sign; (2) appraisal dimensions are of opposite sign; and (3) either or both appraisal dimensions are on the neutral point 0. Table \@ref(tab:s3-appraisal-combination-table) reports the number of participants that expressed at least one emotion with the appraisal combination, as well as the cumulative number and relative proportion of observations.

(ref:s3-appraisal-combination-caption) Emotions expressed with different combinations of appraisal dimensions.

```{r s3-appraisal-combination-table}
s3.appraisal_sign_comparison |>
 select(-Setting) |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-appraisal-combination-caption)\\label{tab:s3-appraisal-combination-table}",
  caption.short = "Study 3: appraisal combinations",
  longtable = FALSE,
 ) |> 
 pack_rows("Asynch./Indiv. (26 participants)", 1, 3) |>
 pack_rows("Synch./Collab. (35 participants)", 4, 6) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

For both settings, the same sign combination was expressed at least one time by a greater number of participants, and proportionally more than the other combinations: almost half of the total (0.49) for the Asynch./Indiv. and almost three quarter of the total (0.72) for the Synch./Collab. setting. The opposite sign combination, on the other hand, accounts only for around one fifth of the total both for the Asynch./Indiv. (0.19) and the Synch./Collab. (0.21) settings. Finally, the neutral point was used in greater proportion in the Asynch./Indiv. setting, with almost one third (0.31) of the total, compared to the Synch./Collab. setting with a proportion of around one in twenty (0.06). As in the case of the density plots described above, this score is inflated by a single participant in the Asynch./Indiv. setting who expressed more than 60 emotions leaving the neutral point on the *Control/Power* dimension.

Individual attitude towards rating both dimensions together should therefore be considered in the assessment. In this regard, it is interesting to compute the individual correlation between the rating of the *Valence* dimension and the *Control/Power* dimension, and then use the average of the correlation ($M_\rho$) as an indicator of whether the two sliders tend to co-vary or not. An $M_\rho \rightarrow 0$ would suggest the two dimensions are orthogonal, whereas $M_\rho \rightarrow \pm 1$ would suggest that the two dimensions are rated exactly in the same way (positive correlation), or that one is rated asymmetrically with respect to the other (negative correlation). Among the $N = 74$ participants that expressed at least 2 emotions (the lower bound to compute an intra-individual correlation), the average correlation observed is of $M_\rho =$ `r s3.appraisal_correlation.overall$cor |> mean(na.rm = TRUE) |> printnum()` ($SD_\rho =$ `r s3.appraisal_correlation.overall$cor |> sd(na.rm = TRUE) |> printnum()`). The correlation is greater in the Asynch./Indiv. setting, with $M_\rho =$ 0.58 ($SD_\rho$ = 0.47) compared to the Synch./Collab. setting, where $M\rho =$ 0.42 ($SD_\rho =$ 0.41). In both settings, thus, there is a highly positive correlation between the two sliders, which tend to be rated symmetrically.

#### Synthesis

All things considering, data suggest that participants take advantage of the full range of each appraisal dimensions individually, but, combined, the two appraisal dimensions are not used as orthogonal. On the contrary, there is strong correlation between the two ratings. This phenomena is consistent in both settings. To assess whether this is problematic, though, it is necessary to check for the subjective feelings that have been expressed with the appraisal ratings. In fact, it could be the case that participants predominantly expressed subjective feelings that are theoretically characterized by either positive *Valence* and positive *Control/Power*, or negative *Valence* and negative *Control/Power*, for that would explain the lack of orthogonality. The link between appraisal dimensions and subjective feelings is illustrated below in the chapter.

### Measures About the Subjective Feelings Expressed

The second assessment concerns the expression of the subjective feeling, that is, the conscious experience of the emotional episode that is usually labeled using natural language words or idioms. [@grandjeanConsciousEmotionalExperience2008; @fontaineComponentsEmotionalMeaning2013; @ogarkovaFolkEmotionConcepts2013]. The assessment primarily aims at determining to what extent the lexicalized emotions included in the underlying affective space met participants' need in terms of representation and differentiation of the conscious experience of the emotion [@barrettKnowingWhatYou2001; @erbasRoleValenceFocus2015]. As a reminder, the EATMINT circumplex proposes 20 lexicalized emotions: `r combine_words(sort(s3.eatmint_circumplex$label_en))`. If these 20 lexicalized emotions meet learners' need in best describing their subjective feeling, participants should have made a spare use of the possibility to express their feelings with natural language words or idioms falling outside this list. In this regard, it is worth comparing whether the lexicalized emotions of the underlying affective space are consistent with participants needs in the two settings. Table \@ref(tab:s3-feelings-cumulative-comparison-table) illustrates the cumulative number of expressed subjective feelings listed or not listed in the underlying affective space.

(ref:s3-feelings-cumulative-comparison-caption) Cumulative number of subjective feelings expressed that were listed or not listed in the underlying affective space

```{r s3-feelings-cumulative-comparison-table}
s3.listed_vs_not_listed |>
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-feelings-cumulative-comparison-caption)\\label{tab:s3-feelings-cumulative-comparison-table}",
  caption.short = "Study 3: listed vs. not listed feelings",
  longtable = FALSE,
 ) |>
 row_spec(3, bold = T)
```

Data show that in both settings, participants privileged the lexicalized emotions included in the EATMINT circumplex with proportions above .80. There is nevertheless a difference between the two conditions of more than ten percentage points, since the proportion of listed options for the Asynch./Indiv. setting is around 0.83 against 0.96 in the Synch./Collab. setting. A difference between the two settings is also reinforced by the number of distinct subjective feelings expressed outside the proposed list. In the Asnych./Non-Coll. setting, participants provided 30 distinct subjective feelings, mostly using single words, compared to 12 distinct subjective feelings in the Synch./Collab. setting. These results suggest that, in the Asynch./Indiv. setting, participants may need a richer *emotional vocabulary* [@erbasRoleValenceFocus2015; @barrettTheoryConstructedEmotion2017; @ogarkovaFolkEmotionConcepts2013] to express their conscious emotional experience, even though the 20 lexicalized emotions proposed by the underlying affective space cover their needs most of the time.

Another aspect worth considering in the assessment of the subjective feelings is whether the relative frequency in expressing each option listed in the circumplex varies across settings. Table \@ref(tab:s3-feelings-frequency-comparison-table) reports the relative frequency of each one of the 20 lexicalized emotions in the EATMINT circumplex for both settings, as well as the absolute difference across settings. The use of the absolute difference highlights the fact that this comparison does not aim at determining whether participants in one setting tend to experience a specific feeling more or less than in the other setting, since the two empirical contributions proposed in the thesis are not fit for this purpose. The aim of the comparison is rather to determine whether the same underlying affective space may adapt to different *needs* in conveying the holistic emotional experience.

(ref:s3-feelings-frequency-comparison-caption) Relative frequencies of the 20 listed subjective feelings and absolute differences between settings

```{r s3-feelings-frequency-comparison-table}
s3.listed_feelings_frequency |>
 select(-Quadrant) |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-feelings-frequency-comparison-caption)\\label{tab:s3-feelings-frequency-comparison-table}",
  caption.short = "Study 3: relative frequency of listed feelings",
  longtable = FALSE,
 ) |> pack_rows(
  index = c(
   "Quadrant I. Positive Valence x Positive Control/Power" = 5,
   "Quadrant II. Positive Valence x Negative Control/Power" = 5,
   "Quadrant III. Negative Valence x Negative Control/Power" = 5,
   "Quadrant IV. Negative Valence x Positive Control/Power" = 5
  )
 ) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

Data illustrate roughly three different combinations. First, there are options with a high relative frequency in one setting and a low relative frequency in the other (*e.g.*, *Amused*, *Relieved*, *Surprised* or *Satisfied*). Second, there are options with high relative frequencies which are consistent across settings (*e.g.*, *Attentive*, *Interested*, *Bored* or *Frustrated*). Finally, there are feelings with low relative frequencies in both settings (*e.g.*, *Envious*, *Disgusted*, *Relaxed*, *Irritated*, *Empathetic*). Results therefore corroborate the assumption that the EATMINT circumplex may adapt to different settings, even though some of its lexicalized emotions are seldom chosen by participants. Whether these feelings should continue to be proposed as choices in the affective space is discussed below.

### Measures About the Computational Model Linking Appraisal Dimensions and Subjective Feelings

The two previous sections highlight that, separately, the appraisal dimensions and the lexicalized emotions composing the affective space seem to adapt to the two different settings. The core of the DEW is nevertheless the theory-driven, computational link between the appraisal dimensions and the subjective feeling. It is therefore pivotal to assess whether the parsimonious computational model that suggests a subset of subjective feelings given a specific evaluation of the eliciting event eases learners' task in conveying the holistic emotional experience. The link between appraisal dimensions and subjective feeling can be derived only for the emotions that are part of the underlying affective space, and therefore the following analysis will filter out subjective feelings not included in the EATMINT circumplex list.

The link between appraisal dimensions and subjective feelings can be assessed mainly in two ways. The first is pragmatic, and pertains to the actual use of the tool with respect to the frequency by which learner's chose one of the options proposed in the subset of buttons on the interface, rather than having to recur to the drop-down menu or typing the response themselves. The second is more theoretically-driven and consists in comparing the underlying affective space -- that is, the one *expected* from the theory -- with the *observed* affective space, which can be computed using the means of *Valence* and *Control/Power* every time a given lexicalized emotion has been chosen by learners.

#### Frequency of Choice of the Proposed Subjective Feelings

The pragmatic assessment consists in computing the frequency by which learner's *accepted* to click on one of the three proposed buttons labeled with a lexicalized emotion, given the evaluation provided through the two sliders representing the appraisal dimensions. In other words, the frequency represent the number of times that learners found one of the suggested options as the *right* representation, or *best* approximation, of their subjective feeling. The frequency can therefore range from 0 -- that is, the learner never found the corresponding subjective feeling in the buttons and had to provide it through the drop-down menu or text input -- to 1, in which case the learner always *accepted* one of the three suggestions provided by the buttons.

This kind of measure, though, can be influenced, among other things, by (1) the sheer number of emotions expressed, with low numbers inflating either the opposite poles or the central tendency; and (2) individual characteristics such as conformity to accept a suggestion or the dexterity in choosing another feeling from the list. For these reasons, the frequency is computed first computed individually for each participant that has expressed at least five emotions, and then averaged over all participants in the same setting -- that is, $N = `r s3.button_vs_other.descriptive[1,2]`$ in the Asynch./Indiv. setting, and $N = `r s3.button_vs_other.descriptive[2,2]`$ in the Synch./Collab. setting. Results are shown in Table \@ref(tab:s3-frequency-button-table).

(ref:s3-frequency-button-caption) Frequency of clicks on one of the suggested subjective feelings

```{r s3-frequency-button-table}
s3.button_vs_other.descriptive |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-frequency-button-caption)\\label{tab:s3-frequency-button-table}",
  caption.short = "Study 3: frequency of clicks on the buttons",
  longtable = FALSE,
 ) |>
 row_spec(3, bold = T) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

Overall, the frequency of clicks on one of the suggested options is $M = `r s3.button_vs_other.descriptive[3,3] |> printnum()`$ ($SD = `r s3.button_vs_other.descriptive[3,4] |> printnum()`$), which means that around 4 out of 5 emotions are expressed using one of the lexicalized emotions suggested through the three buttons on the interface. In the Asnyhc./Indiv. setting, the average frequency is $M = `r s3.button_vs_other.descriptive[1,3] |> printnum()`$ with a standard deviation of $SD = `r s3.button_vs_other.descriptive[1,4] |> printnum()`$. In the Synch./Collab. setting, the average frequency is $M = `r s3.button_vs_other.descriptive[2,3] |> printnum()`$, with a standard deviation of $SD = `r s3.button_vs_other.descriptive[2,4] |> printnum()`$. There is nevertheless a difference of around 0.2 points between the two settings, with participants in the Synch./Collab. setting clicking more often on the buttons. The data, pictured in Figure \@ref(fig:s3-frequency-graph), reveals that most of the participants in the Synch./Collab. setting always *accepted* one of the proposed subjective feelings, whereas in the Asynch./Indiv. setting there is a more heterogeneous disposition. This is consistent with evidence in the previous sections of the chapter indicating the need of a more nuanced emotional expression in the Asynch./Indiv. setting.

(ref:s3-frequency-graph-caption) Frequency of click on one of the three buttons labeled with a subjective feeling. Bars represent 95% confidence intervals.

```{r s3-frequency-graph}
#| fig.align="center", 
#| fig.cap="(ref:s3-frequency-graph-caption)",
#| fig.scap = "Study 3: Frequency of click on the suggested buttons",
#| out.width="60%"
s3.button_vs_other.graph
```

Overall, though, the parsimonious computational model fitted into the EAT seems to adequately connect the appraisals dimensions with the subjective feeling: participants took advantage of this feature on average in four out of five emotions expressed. These results seem also to corroborate the limited number of suggestions proposed by the tool. Three buttons, in fact, seem to provide learners with sufficient options to discriminate their subjective feelings. The sheer frequency, though, does not guarantee that this mechanism *works* in the same way at every level of combination between the appraisal dimensions, for which a more detailed analysis is necessary.

#### Expected Versus Observed Affective Space

With the data collected every time a subjective feeling is expressed, it is possible to compute an observed position of the lexicalized emotion on the circumplex. The observed position is computed in two steps. First, the means of the *Valence* and *Control/Power* dimensions are calculated for every feeling belonging to the underlying circumplex. For example, every time that the subjective feeling *Attentive* has been expressed, the corresponding ratings that the participant has made of the two appraisal dimensions are pooled to determine the means. Once the mean of *Valence* and *Control/Power* are obtained, they are injected into the computational model to retrieve the slope, which will be used to retrieve the *average* angle of feeling on the circumplex. Table \@ref(tab:s3-empirical-feelings-disposition-table) reports the necessary figures to compute the observed slope and compares it with the expected slope, that is the position of the feeling on the EATMINT circumplex. The absolute difference between the two slopes is also provided. The greater the absolute difference, the wider the gap between the *theoretical* position proposed by the underlying affective space and the *empirical* rating made by participants.

(ref:s3-empirical-feelings-disposition-caption) Aggregated means of appraisal ratings for each of the 20 subjective feelings in the EATMINT circumplex, with observed and expected slopes.

```{r s3-empirical-feelings-disposition-table}
s3.empirical_feelings_disposition |>
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-empirical-feelings-disposition-caption)\\label{tab:s3-empirical-feelings-disposition-table}",
  caption.short = "Study 3: observed appraisal ratings in EATMINT circumplex",
  longtable = FALSE,
  col.names = c("Feeling", "N", "Valence", "Contr./Pow.", "Obs. Slope", "Exp. Slope", "|Slope|")
 ) |> 
 pack_rows(
  index = c(
   "Quadrant I. Positive Valence x Positive Control/Power" = 5,
   "Quadrant II. Positive Valence x Negative Control/Power" = 5,
   "Quadrant III. Negative Valence x Negative Control/Power" = 5,
   "Quadrant IV. Negative Valence x Positive Control/Power" = 5
  )
 ) |>
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

The results highlight a wide range of differences between the expected and the observed disposition of each lexicalized emotion, going from almost absolute correspondence for *Bored* (0.22°) to more than a rotation of 90° for *Annoyed* (102.59°). The empirical position of some options is computed using only a few observations, as in the case of *Empathetic* (5), *Envious* (1), or *Disgusted* (5); whereas others show a good approximation with many observations, as it is the case for the aforementioned *Bored* (0.22° with 63 observations), *Amused* (7.20° with 94 observations), *Surprised* (5.72° with 35 observations), or *Stressed* (5.11° with 35 observations). The overall disposition of the observed affective space, though, corroborates the lack of orthogonality, highlighted earlier in the chapter, in the use of the two appraisal dimensions. In fact, the comparison between the graphical representations of the theoretical/expected circumplex in Figure \@ref(fig:s3-theoretical-feelings-graph) and the empirical/observed in Figure \@ref(fig:s3-empirical-feelings-graph) confirms that most of the subjective feelings have been chosen with congruent ratings of *Valence* and *Control/Power*, that is when both appraisal dimensions are either positive or negative. When the two appraisal dimensions are orthogonal, only *Surprised* and *Empathetic* in the bottom-right quadrant, and *Envious* and *Irritated* in the top-right quadrant -- based on only a few observations, though -- have been chosen with the expected combination of the appraisal dimensions. The other lexicalized emotions supposed to appear on the orthogonal combination of appraisals *moved* in another quadrant, depending on the *Valence* rating. That is, feelings from the Positive *Valence* x Negative *Control/Power* quadrant *moved* to the quadrant with both Positive appraisal dimensions; whereas feelings from the Negative *Valence* x Positive *Control/Power* *moved* to the quadrant with both Negative appraisal dimensions.

(ref:s3-theoretical-feelings-graph-caption) The theoretical/expected disposition of the EATMINT circumplex, reported from Figure \@ref(fig:theoretical-feelings-graph) in Chapter \@ref(dew-chapter).

```{r s3-theoretical-feelings-graph}
#| fig.align="center",
#| fig.cap="(ref:s3-theoretical-feelings-graph-caption)",
#| fig.scap = "Study 3: Theoretically-driven (expected) affective space", 
#| fig.height=9, 
#| fig.pos="p",
#| out.width = "100%"
s3.theoretical_feelings_disposition_circumplex.graph
```

(ref:s3-empirical-feelings-graph-caption) The empirical/observed disposition of the EATMINT's circumplex lexicalized emotions according to the actual average rate of participants.

```{r s3-empirical-feelings-graph}
#| fig.align="center", 
#| fig.cap="(ref:s3-empirical-feelings-graph-caption)",
#| fig.scap = "Study 3: Empirically-driven (observed) affective space", 
#| fig.pos="p",
#| fig.height=9, 
#| out.width="100%"
s3.empirical_feelings_disposition_circumplex.graph
```

The phenomenon is consistent in both learning settings, with nevertheless some variations. Figure \@ref(fig:s3-empirical-feelings-comparison-graph) shows the disposition of feelings with respect to the mean *Valence* and *Control/Power*, therefore in a Cartesian plane rather than in a circumplex. The choice of a different format, though, is only dictated by the need to simplify the display of the information, minimizing overlapping; there is thus no change in the underlying computational model. As Figure \@ref(fig:s3-empirical-feelings-comparison-graph) shows, the overall tendency of co-variation in the two appraisal dimensions remains. Nevertheless, in the Synch./Collab. setting, there is more consistency in the bottom-right quadrant, the one characterized by Positive *Valence* x Negative *Control/Power*. Three out of five feelings appears in the expected quadrant, and a fourth one -- *Relieved* -- is close to the edge with the top-right quadrant.

(ref:s3-empirical-feelings-comparison-graph-caption) Comparing the empirical disposition of the two learning settings.

```{r s3-empirical-feelings-comparison-graph}
#| fig.align='center', 
#| fig.cap="(ref:s3-empirical-feelings-comparison-graph-caption)", 
#| fig.scap = "Study 3: Subjective feelings empirically-driven disposition",
#| fig.height=9, 
#| fig.pos="p",
#| out.width="100%"
s3.empirical_space_comparison.graph
```

#### Synthesis

The assessment of the link between the appraisal dimensions and the subjective feeling yielded mixed, but overall promising, results. On the one hand, the overall *accuracy* of the dynamic algorithm was around 0.8, which means that four out of five subjective feelings expressed by participants were included in the three buttons suggested on the interface. The accuracy was lower in the Asynch./Indiv. setting, though, which may suggest the need of a more nuanced expression in this condition.

On the other hand, data clearly confirm a major issue with the *Control/Power* appraisal dimension, which has a high correlation with the *Valence* dimension, a phenomenon already highlighted in the limitations of the Geneva Emotion Wheel in Section \@ref(gew-limitations) and also observed in the usability test of the DEW illustrated in Section \@ref(dew-ux-test). The problem, though, does not seem to be unique to self-report tools. As already mentioned, Scherer and Fontaine [-@schererSemanticStructureEmotion2018] encountered a similar difficulty with the use of the GRID instrument [@fontaineComponentsEmotionalMeaning2013]. As advocated by the authors, this problem requires future studies to find better solution.

### Measures About the Perceived Usability of the Tool

Finally, the EAT will be appraised with respect to its usability, that is the perceived efficiency, effectiveness and satisfaction in using the tool. A usability measure, the System Usability Scale [SUS, @brookeSUSQuickDirty1996], was administered in the empirical contribution of Chapter \@ref(study-2), but not of Chapter \@ref(study-2). The SUS was nevertheless administered in the usability test in Fritz (2015), which used the same configuration and task of the Synch./Collab. setting (but without the experimental conditions). The SUS scores of the usability test ($N = 16$) will therefore be used for the Synch./Collab. setting, alongside the scores obtained in the Asynch./Indiv. of Chapter \@ref(study-2) ($N = 26$).

The scale, using inverse rating for even items, allows to compute an overall score ranging from 0 (very poor perceived usability) to 100 (excellent perceived usability). Results of the SUS have been collected in the last decades in various published and unpublished reports. Thus, there is nowadays the possibility to better assess the overall score of the SUS, as well as of each of its ten items [@bangorDeterminingWhatIndividual2009; @lewisItemBenchmarksSystem2018; @sauroQuantifyingUserExperience2016].

Concerning the overall score of the SUS, Sauro and Lewis [-@sauroQuantifyingUserExperience2016] extrapolated a curved grading scale from 241 industrial usability studies and surveys. According to this scale, the average SUS overall score is $M = 68$. In the meantime, the same authors suggest that "it is becoming a common industrial goal to achieve a SUS of 80" [@lewisItemBenchmarksSystem2018, p. 161] as synonymous of a perceived good experience.

Table \@ref(tab:s3-sus-overall-score-table) shows the SUS score for the two learning settings, as well as the weighted overall mean. With an overall score of $M =$ `r s3.sus_comparison$mean[3] |> printnum()` ($SD =$ `r s3.sus_comparison$sd[3] |> printnum()`), the EAT is perceived somehow halfway between the $M = 68$ empirical benchmark, and the target score of 80. As the table shows, the SUS score is consistent across the two learning settings, with a difference of less than 1 point.

(ref:s3-sus-overall-score-caption) Score to the System Usability Scale [SUS, @brookeSUSQuickDirty1996]

```{r s3-sus-overall-score-table}
s3.sus_comparison |> 
 kable(
  booktabs = TRUE,
  digits = 2,
  caption = "(ref:s3-sus-overall-score-caption)\\label{tab:s3-sus-overall-score-table}",
  caption.short = "Study 3: overall score to the SUS",
  longtable = FALSE,
  col.names = c("Condition", "N", "M", "SD")
 ) |>
 row_spec(3, bold = T) |> 
  kable_styling(latex_options = c("HOLD_position", "repeat_header")) 
```

Furthermore, Lewis and Sauro [-@lewisItemBenchmarksSystem2018] collected data from 166 unpublished industrial usability studies or surveys, each one comprising a mean of the SUS overall score. The 166 means were computed from a total of 11'855 surveys. From these data, the authors retrieved benchmarks for each of the ten items of the SUS to reach the $M = 68$ empirical mean, or the target score of 80. As a reminder, the SUS items are the following:

1.  I think that I would like to use this system frequently
2.  I found the system unnecessarily complex
3.  I thought the system was easy to use
4.  I think that I would need the support of a technical person to be able to use this system
5.  I found the various functions in this system were well integrated
6.  I thought there was too much inconsistency in this system
7.  I would imagine that most people would learn to use this system very quickly
8.  I found the system very cumbersome to use
9.  I felt very confident using the system
10. I needed to learn a lot of things before I could get going with this system

Figure \@ref(fig:s3-sus-items-graph) depicts the score of each of the SUS items across settings. To ease the comparison, even items have already been reversed, so that for each item a higher score equals a higher perceived usability. The horizontal lines represent Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks to reach the target score of 80. The original benchmarks refers to the 1-to-5 rating and have therefore been multiplied by a factor of 1.4 to map to the 1-to-7 scale used in both administrations of the SUS. Also the benchmarks have already been reversed for even items to ease the comparison.

(ref:s3-sus-items-graph-caption) SUS scores on the single items, with the horizontal lines representing Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks to reach the target score of 80, transformed to a 1-to-7 scale. Both items and benchmarks have already been reversed for even items. Bars represent 95% confidence intervals.

```{r s3-sus-items-graph}
#| fig.align='center', 
#| fig.cap="(ref:s3-sus-items-graph-caption)",
#| fig.scap = "Study 3: SUS rating, with benchmarks from Lewis and Sauro (2018)", 
#| fig.pos="H",
#| out.width="100%"
s3.sus_items.graph
```

Data show substantial consistency across learning settings for most of the items, with differences in items SUS1 (frequency), SUS3 (ease of use), and SUS8 (intuition). The first item in particular is the one yielding the bigger discrepancy, with participants in the Synch./Collab. setting reporting a perspective use more frequent than the Asynch./Indiv. setting, which is consistent with the observed use, for instance in terms of number of emotions expressed. All in all, though, the EAT seems to possess an *intrinsic* perceived usability that holds -- for good and for bad -- in both settings.

Comparing the single items to Lewis and Sauro [-@lewisItemBenchmarksSystem2018] benchmarks highlights that half of the items are on-or-above the target, and half are below. The fact that all the odd items are below, and all even items are on-or-above the target may seem peculiar, but is consistent with the pattern of the benchmarks which are more demanding for odd items. Lewis and Sauro [-@lewisItemBenchmarksSystem2018] do not mention anything specific about this pattern, but implicitly exclude it could be determined by the negative formulation of odd items, since previous findings collected by the same authors [@sauroWhenDesigningUsability2011] seem to suggest that there is negligible impact of the negative formulation compared to a positive transformation of the odd items. An effect of the wording of items can nevertheless easily be tested or controlled by using an all-positive version of the SUS, which has already been adopted (*ibid.*).

According to the benchmarks, the EAT performs particularly bad in the frequency of use (SUS1) and in the confidence in the system (SUS9), which are two important dimensions of the scale. The integration of the different parts of the system (SUS5) also does not seem to convince learners, whereas ease of use (SUS3) and quickness of adoption (SUS7) remain below target, but less critically so. On the bright side, the EAT performs very well in learnability (SUS10), and well in consistency (SUS6). Furthermore, simplicity (SUS2), autonomy (SUS4), and intuition (SUS8) are aligned with the target score of 80.

A more contextual comparison of the usability of the EAT can be provided with Feidakis and colleagues [-@feidakisProvidingEmotionAwareness2014], who also used the SUS score to assess the usability of the *emot-control* (see the related works in Section \@ref(ea-in-computer-mediated-environment)). The overall SUS score obtained by the *emot-control* on $N = 29$ is $M = 67.81$ ($SD$ not provided), thus around 6 points lower than the score observed for the EAT at hand. Contrary to the present results, though, learners in Feidakis and colleagues (*ibid.*) reported a higher score on the item about frequency (SUS1).

A preliminary synthesis highlights that the overall perception of the usability of the EAT is fairly good, especially considering the lack of previous experience with this type of device. On the other hand, there are also critical indications, such as the prospected frequency of use or the confidence in the system, which must be carefully considered.

## Discussion

Overall, the analysis performed on the data at hand provided insightful cues upon which assess the EAT. With a dataset of more than one thousand expressed emotions, in particular, the emotion structure injected into the EAT have been put under the lens. The System Usability Scale [@brookeSUSQuickDirty1996] also proved to be insightful, especially taking advantage of the benchmark and the direct comparison of a rating of another EAT [@feidakisProvidingEmotionAwareness2014]. Finally, the two different conditions in which the EAT has been deployed fostered a preliminary assessment of its multi-purpose vocation of the toolbox. The discussion builds upon the four research questions for a thorough assessment.

### Appraising the Appraisals

Data about the use of the appraisal dimensions draw a mixed picture about their efficacy. On the one hand, individually, each slider seem to be evaluated on its full spectrum, suggesting there is an interest in having them both. On the other hand, there is compelling evidence that the two dimensions are evaluated symmetrically, with an overall average correlation within respondents of $M_\rho =$ 0.47, which even reached $M_\rho =$ 0.58 in the Asynch./Indiv. setting. Even if it is not possible to deduce from a correlation which one is subordinate to the other, chances are that the *Valence* dimension has the upper hand, with the *Control/Power* dimension playing the sparring partner role. This would be consistent with other findings in fundamental emotion theory (see \@ref(gew-limitations), but also in applied contexts. For instance, @lavoueEmotionAwarenessTools2020 point out a problem with the *Control* dimension also stemming from a perspective informed by the Control-Value theory of achievement emotions [@pekrunControlValueTheoryAchievement2014; @pekrunControlValueTheoryAchievement2006]. The authors explain:

> Regarding the perceived control of the situations in which students experience emotions, we first identify that they report mainly emotions associated with a low control, with a high percentage of negative emotions. This result is in line with studies that show that emotions associated with a low control provoke mainly negative emotions, such as anxiety and frustration (Pekrun 2006). Second, we also observe that a high number of situations are not associated with a specific level of control (high or low). We deduce that perceived control is an appraisal dimension that is not frequently used by students to explain their emotions, meaning that they may have difficulties in assessing their level of control over learning tasks.\
> --- @lavoueEmotionAwarenessTools2020, p. 282

Except for the part about *negative* emotions being mainly provoked by low *Control* (this does not hold in the Component Process Model where, for instance, *Frustration* is characterized by high *Control/Power*), the reminder of the description seems to perfectly fit the data at hand. The case of the single student expressing more than 60 emotions without touching the *Control/Power* dimension is in this sense revealing.

The problem with the rating of the *Control/Power* dimension was also one of the main reasons to provide researchers and practitioners with the possibility to configure in the toolbox the underlying affective space and the way each appraisal dimensions is prompted on the interface. The scaling of the computational model to an *n*-dimensional structure was also influenced by empirical results, alongside the theoretical considerations illustrated in the presentation of the model in chapter \@ref(computational-model).

Getting rid of the *Control/Power* dimension altogether seems nevertheless excessive. Depending on the theoretical approach adopted, this appraisal/dimension may play a prominent role in emotion elicitation and differentiation [@schererDynamicArchitectureEmotion2009; @scherer2018; @pekrunControlValueTheoryAchievement2014; @fontaineLinearNonlinearRelationships2021]. Rewording of the way the rating is prompted may therefore be a solution, even if in this case it hasn't apparently led to any substantial improvement. A second options consists in training: explaining beforehand to learners/participants what is *really* meant by this dimension, with examples and prototypical vignettes.

A third options is more revolutionary and provoking. Given evidence that *Control/Power* is subordinate to the *Valence* dimension [@schererSemanticStructureEmotion2018], rather than getting rid of the *Control/Power* dimension, one can attempt to bypass the *macro Valence* instead [@erbasRoleValenceFocus2015; @shumanLevelsValence2013]. Not asking respondents to rate whether the situation is pleasant or unpleasant may free their cognitive evaluation of a looming presence. As suggested by @erbasRoleValenceFocus2015, focusing on *Valence* alone seem to impoverish the overall gamut of potential emotional experience. Especially in an overall context in which self-reflection may be of pivotal importance, proposing more *nuanced* appraisals dimensions can also have benefits in widening learners perspective on their emotional experience. This would not only impact the analysis of the causes of how a learner is feeling, but possibly also empower the strategies for emotion regulation. If one is focused exclusively on whether the situation is *positive* or *negative* with respect to *Valence*, then she may be tempted to apply a linear way of action: lean towards the *positive,* full stop. Reflecting on the emotional experience from other points of view may highlight the functional role of emotion in general, not only positively valenced emotions.

To sum up, the potential of the appraisal dimensions seem for the moment quite unexpressed. Whether this is due to contingent factors or to a deeper structural reason remains an open question of pivotal importance for the overall assumption defended by the thesis. Appraisals rest at the core of an instrumental use of emotional awareness in computer-mediated learning environments, and the assessment of their contribution is therefore a necessary condition for a severe test of the EAT. Leveraging on the flexibility in configuring instances of the toolbox, a next step in the assessment could be to experimentally compare two versions of the EAT *powered* by different appraisal dimensions.

### EATMINT's Lexicalized Emotions Are Representative of Learners' Experience

At first blush, the lexicalized emotions proposed by the EATMINT circumplex (see Section \@ref(eatmint-circumplex)) seem to provide learners with adequate options to represent their *true* underlying subjective feeling. Around 90 percent of the expressed emotions adopted one of the lexicalized emotion from the pre-defined list in the affective space.

A distinction between the two settings nevertheless has also emerged, quantified in an decreased accuracy of more than 10 percentage points in the Asynch./Indiv. setting. This difference leans toward the hypothesis that in longitudinal and non-collaborative settings, learners may need a wider and more *nuanced* emotional vocabulary to express how they feel. This may be due to the fact that in such condition, the *object focus* [@pekrunControlValueTheoryAchievement2014; @sanderModelsEmotionAffective2013; @moorsTheoriesEmotionCausation2009] of the emotional experience can be wider compared to a synchronous and collaborative setting, where the domain of events may be more tightly bound to what is happening *hic et nunc*. As stated by Pekrun in the Control-Value theory [@pekrunControlValueTheoryAchievement2014], retrospective and prospective emotions can also play a prominent role in emotion related to learning. Learners may project more stable characteristics of themselves, bestowing to the emotional experience a more complex and overarching meaning with respect to the immediacy and volatility of synchronous exchanges [@rimePartageSocialEmotions2005; @rimeEmotionElicitsSocial2009]. These considerations are nevertheless at the stage of speculation given the limitations of the sample at hand.

A more pragmatic discussion may relate to the number of lexicalized emotions to propose in the pre-defined list. On the one hand, there are contributions highlighting a restricted domain of emotion term that seem to account for the most frequent emotions experienced in computer-mediated learning environments [@molinariEMORELOutilReporting2016; @reisAffectiveStatesCSCL2015; @dmelloDynamicsAffectiveStates2012]. On the other, emotion differentiation play an important role in representing the nuances of how a person feels, to herself or to others [@erbasRoleValenceFocus2015; @grandjeanConsciousEmotionalExperience2008; @torrePuttingFeelingsWords2018]. The data at hand strike a balance between the two: it is evident that there are lexicalized emotions that haven been chose consistently more often than others, but at the same time each option has been chosen at least by some participants. *Amused*, *Attentive*, *Interested*, *Confused* and *Bored* were among the most frequently chosen ones, whereas *Envious*, *Disgusted*, *Empathetic*, *Annoyed*, and *Relaxed* the least frequent ones. There were also differences between settings, with some lexicalized emotions being chosen more often in one setting than the other, like *Amused*, *Relieved*, *Confused*, *Surprised*, and *Satisfied*. This seems to confirm that the context has an important role in determining whether there are lexicalized emotion who are more or less *prototypical* according to the situation at hand [@gentschEffectsAchievementContexts2017].

It is therefore impossible to figure out whether there are a number and kind of lexicalized emotions that can fit any situation, even though the EATMINT circumplex seems to adapt at least to the settings at hand. When a number of closed options do not have a particular importance -- for instance when there is some form of intelligence that must respond to a limited domain of inputs, see Section \@ref(affect-aware-system) -- it is argued here that it would be better to provide learners with a sufficient number of lexicalized emotions to choose from. This may once again broaden their perspective on emotion differentiation and empower their ability to reflect on their own emotional experience, as well as increasing the chances to convey a strategic signal to others [@schererComponentialEmotionTheory2007]. Since all the contributions adopted a list of 20 lexicalized emotions, it would be easy to confirm that this is a sufficient number. But obviously this is a superficial assessment. Once again, if this information is crucial, it may be more accurately determined empirically, comparing for instance affective spaces that vary on the number of pre-defined options.

A last point concerning the EATMINT circumplex may be cited with respect to the *Zeitgeist*. The affective space use a list of adjectives rather than nouns for the lexicalized emotions. In French, this has the shortcoming of providing only adjective with the male declination at the end. To the best of my knowledge, there is no compelling evidence that the distinction between an adjective and a noun can influence the subjective feeling. If this is the case, then it would be more appropriate to switch the EATMINT circumplex on a noun-based format, which would be more inclusive. If avoiding the noun format is of essence, a more neutral form such as *intéressant* (interesting), rather than *intéressé-e* (interested) could be adopted instead.

To sum up, the number and kind of predefined lexicalized emotion may play a prominent role in sustaining learners in expressing how they feel. The options provided by the EATMINT circumplex seem to fit this purpose, adapting to different participants and environments, even if differences between the Asynch./Indiv. and Synch./Collab. settings could be observed. Given that the same circumplex has been used throughout the contributions, there is nevertheless no counter-factual evidence to better assess this claim. In this sense, a very severe test would be to adopt a *pure* constructivist approach [@barrettHowEmotionsAre2018; @barrettTheoryConstructedEmotion2017; @barrettSolvingEmotionParadox2006] and force learners to type in an emotion term, without providing any predefined option. It would be interesting to compare how many lexicalized emotions of the EATMINT circumplex would spontaneously be chosen as representative of learners subjective feelings.

### The Computational Model Is a Good Enough Heuristics Most of the Time

The assessment of the parsimonious computational model presented in Chapter \@ref(computational-model) provided mixed evidence about its interest and accuracy. The interest seems warranted, since participants found a representative subjective feeling in the three options proposed as buttons around 3 out of 4 times, with a proportion nearing 4 out of 5 in the Synch./Collab. setting. Considering that the system proposed only 3 out of 20 lexicalized emotion, the performance seems to corroborate that is indeed possible to predict the emotional experience of a person based on how she has appraised the situation [@schererHumanEmotionExperiences2013; @fontaineLinearNonlinearRelationships2021; @scherer2018; @gentschEffectsAchievementContexts2017; @schererSemanticStructureEmotion2018].

The efficacy of the computational model nevertheless depends from the interplay between the accuracy of the underlying affective space on the one hand [@gilliozMappingEmotionTerms2016; @fontaineLinearNonlinearRelationships2021; @schererWhatDeterminesFeeling2006; @schererGRIDMeetsWheel2013], and the respondents' *adequate* interpretation of the appraisal criteria on the other hand [@schererComponentialEmotionTheory2007; @scherer2021].

With the data at disposal it is evident that the *observed* affective space, retrieved from participants' actual rating of the appraisal dimensions when a given subjective feeling was expressed, does not overlap with the theoretically-driven disposition in the EATMINT circumplex. Given that the disposition of the lexicalized emotions in the EATMINT circumplex has not been validated, the possibility of a problem with the underlying affective space cannot be formally ruled out. Nonetheless, it is safe to assume that the problem stems, at least principally, from the evaluation of the appraisal dimensions, as already discussed above. When the *Valence* and *Control/Power* are rated symmetrically, it is inevitable that all subjective feelings align in the upper-right and bottom-left quadrants of a two-dimensional space.

Given its computational parsimony and the benefit in reducing the number of concurrent discrete emotions on the interface, the computational model can be at least retained as a good enough heuristics from a usability stand point. It allows respondents to perform a first skim, which is sufficient most of the time, but does not bind the choice to the pre-defined options. In this regard, the use of an alternative way to input a subjective feeling outside the proposed options is not only a good practice in emotion self-report [@mortillaroEmotionsMethodsAssessment2015; @schererWhatAreEmotions2005], but becomes essential to the possibility to express a strategic signal [@schererComponentialEmotionTheory2007].

To sum up, the computational model remains a promising feature of the EAT as long as it does not have the pretension to accurately predict the emotional experience of respondents every time and in every situation. For a more severe assessment of the interest and accuracy of the computational model, though, a direct comparison with an alternative mechanism should be implemented. In this regard, the toolbox proposes a random algorithm that can be applied to any underlying affective space. A direct comparison between the computational model and a random sub-setting may help to quantify more specifically the benefit in endowing the EAT with a probabilistic link between appraisal dimensions and subjective feelings.

### Usability Is Not Bad -- But It Is Not Enough

The last assessment takes a more subjective perspective and, rather then the concrete use of the EAT, focuses on the perception of its usability. The System Usability Scale [@brookeSUSQuickDirty1996] was administered in the usability test and at the end of the longitudinal design of Chapter \@ref(study-2). The overall rating of the tool was of $M =$ 73.5, that is, halfway between the benchmarks provided by @lewisItemBenchmarksSystem2018: the overall average of 68 points and the target of 80 points for good usability. In a more direct comparison with the rating of the *emot-control* tool in @feidakisProvidingEmotionAwareness2014, for which an average of 67.8 was observed, the EAT therefore received a rating of around 6 points higher. The rating on the SUS can therefore be retained as promising, even if some dimensions (frequency of use especially) scored far from the target benchmark to reach 80 points.

More than the score in itself, though, the overall assessment of the perception of the tool should be ameliorated by taking into account a wider perspective. In fact, the SUS is a very reliable and valid scale, adopted in different contexts. Nevertheless, as the name implies, it focuses on usability alone, whereas there is growing consensus in trying to assess the user experience more globally [@lallemandMethodesDesignUX2017; @lawAttitudesUserExperience2014; @tullisMeasuringUserExperience2013]. @tullisMeasuringUserExperience2013 draws in this sense a useful distinction:

> Usability is usually considered the ability of the user to use the thing to carry out a task successfully, whereas user experience takes a broader view, looking at the individual's entire interaction with the thing, as well as the thoughts, feelings, and perceptions that result from that interaction.\
> --- @tullisMeasuringUserExperience2013, p. 5

Given the importance that is attributed to the affective experience by the EAT itself, it would be coherent to follow through with a more holistic assessment, integrating usability measures with scales such as the AttrakDiff [@hassenzahlAttrakDiffFragebogenZur2003], the UEQ [@laugwitzConstructionEvaluationUser2008], or the meCUE [@mingeMeCUEQuestionnaireModular2017]. These scales provide a more thorough assessment of a technological device, considering for instance also the look-and-feel of the tool, which is known to not only impact the perception of the tool, but also its use [@lallemandMethodesDesignUX2017; @tullisMeasuringUserExperience2013; @vermeerenUserExperienceEvaluation2010].

In the meantime, usability and user experience would cover only the perspective of the individual learner that interact with the tool. A more thorough assessment of the perception of the EAT should therefore also be guided by a more comprehensive theoretical framework in human-computer interaction. One potential candidate is the widely adopted Technology Acceptance Model [TAM; @venkateshUserAcceptanceInformation2003; @davisPerceivedUsefulnessPerceived1989; @davisCriticalAssessmentPotential1996; @venkateshTheoreticalExtensionTechnology2000]. The main assumptions of the model state that the actual use of a technological device is determined in particular by two intervening factors: perceived usefulness and perceived ease of use. The TAM may therefore be integrated alongside a more specific scale such as the tentative Emotion Awareness Usefulness (EAU) scale introduced in Chapter \@ref(study-2), which targets specifically the use of an EAT.

Another advantage of introducing a more overarching framework in human-computer interaction concerns the perspective of practitioners who may be interested in endowing their courses with an EAT. A meta-analysis conducted by @schererTechnologyAcceptanceModel2019[^1] illustrates how teacher's acceptance of a technology is an integral part in determining to what extent it can be fruitfully adopted by students. For instance, the RULER approach [@brackettRULERTheoryDrivenSystemic2019; @brackettEnhancingAcademicPerformance2012; @hoffmannTeachingEmotionRegulation2020], of which the Mood Meter application is an integral part, proposes training for its use in the classroom.

To sum up, the assessment of the perception about the use of the EAT has so far focused on the usability perspective, which was pertinent during the first phases in the interaction design process. Even though the response has been promising so far, as the iterations progress, the assessment should widen and take into consideration a more holistic perception of the EAT. One way to move forward is by integrating user experience scales, and also an overarching theoretical framework such as the TAM, which may also implicate the perception of practitioners interested in endowing their classes with the use of an EAT.

## Conclusion

This chapter proposed an overall data-driven assessment of the use and perception of the EAT, as well as a comparison between the use in an asynchronous and individual environment, versus a synchronous and collaborative one. The primary purpose of the study was to determine to what extent the EAT meets learners' need, in particular by evaluating whether learners' take full advantage of the emotional structure implemented into the tool in expressing their emotional states. Results of the overall and comparative assessment provide mixed evidence.

On the one hand, the tool and the underlying EATMINT affective space seem to adapt fairly well to different settings. For instance, the 20 lexicalized emotions proposed in the circumplex seem to be sufficient to convey most of learners' emotional experiences. Furthermore, the algorithm linking the two appraisal dimensions with the proposed subjective feelings provided a good heuristic, consistent across different settings. These indicators suggest that the EAT possesses a sort of *intrinsic* value, which may be adaptable to different contexts. This does not mean, though, that the EAT will be perceived as useful regardless of other determinants, such as the task at hand or the overall instructional design. For instance, the comparison suggests that learners' may need a more nuanced emotional expression in an asynchronous and individual setting.

On the other hand, results also corroborate and extend some problems already emerged both theoretically and in applied contexts, such as the lack of orthogonality of the two appraisal dimensions *Valence* and *Control/Power*. This kind of issues, though, does not seem limited to technical elements of the EAT, but rather extend to more fundamental aspects of emotional awareness, which will be considered more thoroughly in the general discussion in Part IV of the thesis. This chapter concludes with the limitations and the contribution of this study in particular.

### Limitations

The study proposed a mixture between a secondary data analysis [@westonRecommendationsIncreasingTransparency2019] and a small internal meta-analysis of one's own studies [@gohMiniMetaAnalysisYour2016]. By staying in between the two, the risk is that neither of the qualities of both methods has been truly exploited. The sample size at hand remain in fact relatively small, and the research questions were not clearly stated beforehand, but emerged as long as data were collected or analyzed for the two empirical contributions of the thesis.

Even the sample size of more than one thousand expressed emotions, which seems huge, must be put into perspective. In a provocative comparison, it may be stated that with two ratings between -100 and 100, as well as 20 lexicalized emotions as possible outcome (not taking into account custom responses), there is a total of 808'020 possible combinations between the three measures that composes an observation in the three datasets at hand. A greater number of emotion expressed would also provide the possibility to fit more complex models on the data at hand [@mcelreathStatisticalRethinkingBayesian2020; @rodgersEpistemologyMathematicalStatistical2010; @guestHowComputationalModeling2021; @marsellaComputationalModelsEmotion2010].

### Study's Overall Contribution

In spite of the limitations, the study provided a first attempt in comparing measures obtained through the same toolbox, but with different instances of the EAT. Even though the instances in this specific case were not very different, this is still a first example of the kind of analyses that would be possible when data is produced and shared in a common format [@levensteinDataSharingCaring2018; @gilmorePracticalSolutionsSharing2018].

From a date-driven perspective, the study explored topics that have both practical and theoretical relevance. For instance, the analyses allowed to investigate the rating on appraisal dimensions, revealing a non-independence between the *Valence* and the *Control/Power* appraisals also encountered in fundamental and applied research [@lavoueEmotionAwarenessTools2020; @erbasRoleValenceFocus2015; @schererSemanticStructureEmotion2018; @shumanLevelsValence2013]. Through the inner functioning of the EAT, it was also possible to compare a theoretically driven and an *empirically rated* affective spaces. In the case of lack of resources for a more thorough validation of an affective space -- such as using the GRID instrument [@schererGRIDMeetsWheel2013; @schererCoreGRIDMiniGRIDDevelopment2013] -- this method may represent a viable approximation to validate or, at least, assess the accuracy of an affective space. In this regard, from an Open Science perspective, it may be considered to provide some common analyses that can be performed through data exported from the toolbox in an automated process. A good candidate for this role would be a package in the R programming language environment.

[^1]: I took the liberty to add to Ronny Scherer last name an \* at the end, so that the distinction from Klaus R. Scherer will not entail -- following the orthodoxy of APA rules -- the full name stated in the many contributions where Klaus R. Scherer is involved. I hope the authors, if they will ever read the manuscript, would not mind.
